performance of bert and roberta models on the easy and hard subsets is reported in table 4 . these models perform well on both subsets , with the exception of the hard subset , where training on b - copa improves performance by 2 . 5 points . on the hard subset , the models perform better than all the previous models except bert - large - ft .
shown in table 1 , bert and roberta achieve considerable improvements on copa ( see table 1 ) .
2 shows the five tokens with highest coverage . for example , a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21 . 2 % of copa training instances . its productivity of 57 . 5 % expresses that it appears in correct alternatives 7 . 9 % more often than expected by random chance . this suggests that a model could rely on such unbalanced distributions of tokens to predict answers based only on alternatives .
human evaluation shows that the mirrored instances are comparable in difficulty to the original ones ( see table 3 ) .
then compare bert and roberta with previous models on the easy and hard subsets . we observe that both models perform similarly on the hard subset , with the exception of sasaki et al . ( 2017 ) . however , bert ' s model performs slightly better on the hard subset , indicating that it has more training data to learn the task .
relatively high accuracies of bert - large and roberta - large show that these pretrained models are already well - equipped to perform this task " out - of - the - box " . however , the performance remains relatively lower on the hard subset , indicating that training on b - copa encourages bert to rely less on superficial cues .
observe that bert trained on balanced copa is less sensitive to a few highly productive superficial cues as shown in table 7 . these cues are shown in bold . however , for cues with lower productivity , the picture is less clear , in case of roberta , there are no noticeable trends in the change .
the proposed cnnlstmour - neg - ant as an alternative to the lstm - w / o neg . baseline for sentiment analysis and classification . the proposed classifier improves the sentiment baseline with f1 scores improving from 0 . 72 to 0 . 78 for positive sentiment and from 1 . 87 to 2 . 87 for negative sentiment .
results in table 7 show that the method performs comparable to stateof - the - art bilstm model from ( fancellu et al . , 2016 ) on gold negation cues for scope prediction .
statistics are shown in table 3 . the average number of tokens per tweet is 22 . 3 , per sentence is 13 . 6 and average scope length is 2 . 9 .
the f - score of false negation from a 0 . 61 baseline to 0 . 68 on a test set containing 47 false and 557 actual negation cues . the proposed system improves f - score by 3 points .
results are shown in table 5 . we report the average value of diversity and appropriateness , it is clear from table 5 that data augmentation improves the model ' s performance by a significant margin . in addition , the percentage of responses that are acceptable to both groups drops significantly .
results are shown in table 4 . after applying our data augmentation , both the action and slot diversity are improved consistently , hdsa has the worse performance , we observe that hdsa is more representative of the multi - decoder generation than damd , and therefore requires more data to coalesce . the single - action baselines are broken down in terms of performance , with different classifiers contributing differently depending on the data distribution . with respect to action diversity , we see that damd and hdsa have the best performance on both datasets .
present the results of 3 models that use domain - adaptive belief span modeling . our model outperforms all the baseline models except seq2seq models ( upadhyay et al . ( 2018 ) in terms of inform and success rates , we observe that applying our data augmentation to system action forms improves the combined score by 3 . 8 points in bleu score ( mehri et al . , 2018 ) .
3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and qa - srl . the models are evaluated using exact match ( em ) and f1 measures , as the results show , incorporating coverage improves the model ' s performance in the in - domain evaluations as well as the out - of - domain evaluation in qasrl .
2 shows the performance for both systems for in - domain ( the multinli development set ) as well as out - of - domain evaluations on snli , glockner , and sick datasets . the results show that coverage information considerably improves the generalization of these models across various nli datasets . the resulting cross - dataset improvements are larger than those on the snli and glockner datasets .
4 shows the kl - divergence between different dialog policy and the human dialog kl ( πturns | | pturns ) , where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the user simulator , and pturns for the real human - human dialog .
performance of each approach that interacts with the agenda - based user simulator is shown in table 3 . gdpl obtains the best performance on the agenda turns , while aldm gets the worst performance . gdpl even gets worse performance than gdpl - sess , gdpl is comparable with human in terms of inform and match rate , aldm even gets better match rate acer and ppo obtain the best match rate as well . though aldm obtains a slight improvement in dialog turns , its performance is still inferior than ppo , aldm and gdpl .
agent the performance of all the methods is shown in table 5 . aldm obtains the best performance when matching vhus turns with acer and ppo . ppo is slightly worse than acer , but still achieves the highest success rate .
6 presents the results of human evaluation . gdpl outperforms three baselines significantly in all aspects ( sign test , p - value < 0 . 01 ) except for the quality compared with acer . among all the baselines , gdpl obtains the most preference against ppo . it is clear from table 6 that gdpl performs better than both aldm and ppo in the quality comparison .
7 provides a quantitative evaluation on each metric by showing the distribution of the return r = t γtrt according to each metric . it can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets the full score of the corresponding metric , and negative otherwise .
present precision scores for the word analogy tests in table vii . our proposed method improves the performance by 10 % over the original embeddings . however , it has the worse performance . we observe that our proposed method obtains comparable performance with glove and word2sense .
apply the test on five participants . results tabulated at table v shows that our proposed method significantly improves interpretability by increasing the average true answer percentage from ∼ 28 % for baseline to ∼ 71 % for our method .
performing slightly worse than the original embeddings , we observe that the proposed algorithm outperforms all the alternatives except word2vec baseline on average . it is clear that it performs better on some of the test sets , however it is slightly worse on others . categories from roget ' s to word2sense are broken down in terms of performance on these tests . we observe that it is more likely to perform better on the glove test set than it is on the original . this indicates that the semantic information injected into the algorithm by the additional cost term is significant enough to result in a measurable improvement .
investigate the effect of the additional cost term on the performance improvement in the semantic analogy test in table viii . we empirically found that for all the questions that we considered , our proposed algorithm outperforms the original embeddings . however , for the cases where only one concept word was considered , the results are slightly worse than those proposed by word2vec . this is mostly due to the high number of concept word counts in the dataset ( e . g . , 8783 vs . 8783 ) ,
accuracies are presented in table ix . the proposed method outperforms the original embeddings and performs on par with the sov . however , it has the advantage of training on a larger corpus , which results in significantly less training data . this result along with the intrinsic evaluations show that the proposed imparting method can significantly improve interpretability without a drop in performance .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the performance of our model on event coreference . our joint model outperforms all the base lines with a gap of 3 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results reconfirm that pre - clustering of documents to topics is beneficial , improving upon the kcp performance by 4 . 5 points , though still performing substantially worse than our joint model . to test the contribution of joint modeling , we benchmark our model against three baselines , namely , cluster + kcp , kenyon - dean et al . ( 2018 ) and cv cybulska and vossen ( 2015a ) . the results show that joint modeling improves the joint model ' s performance by 3 . 6 points .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results of our joint model on event coreference . our joint model outperforms all the base lines with a gap of 3 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points .
show the precision numbers for some particular recalls as well as the auc in table 1 , where pcnn + att is applied to train the model . these precision numbers show that our model has good recall ability .
show the precision numbers for some particular recalls as well as the auc in table 2 , where pcnn + att ( 1 ) refers to train sentences with two entities and one relation label . we observe that our model exhibits the best performances .
experimental results on wikidata dataset are summarized in table 3 . the results of " - word - att " row refers to the results without word - level attention . according to the table , the drop of precision demonstrates that the word level attention is quite useful .
shown in table 4 , the training time increases with the growth of d .
5 shows the comparison of 1 - 5 iterations . we find that the performance reach the best when iteration is set to 3 .
3 presents the rouge scores of our system ( neuraltd + learnedrewards ) and multiple stateof - the - art systems . our learned reward is optimised towards high correlation with human judgement ( upadhyay et al . , 2018b , c ; kryscinski et al . 2018a ; chen and bansal , 2018b ; zhong and wang , 2018c ; xu and chen , 2018a ) .
table 4 , we find that all metrics we consider have low correlation with the human judgement . more importantly , their g - pre and g - rec scores are all below . 50 , which means that more than half of the good summaries identified by the metrics are actually not good , and more than 50 %
3 shows the quality of different reward learning models . as table 3 shows , all reward learning methods use the best feature - rich clustering algorithm . mlp with bert as en ( 2018 ) coder has the best overall performance . specifically , bert + mlp + pref significantly outperforms ( p < 0 . 05 ) all the other models that do not use bert + mlp ,
4 presents the human evaluation results . summaries generated by neuraltd receives significantly higher human evaluation scores than those by refresh ( p = 0 . 0088 , double - tailed ttest ) and extabsrl ( p ( cid : 28 ) 0 . 01 ) . also , the average human rating for refresh is significantly higher ( p : 28 ) ,
5 compares the rouge scores of using different rewards to train the extractor in extabsrl ( the abstractor is pre - trained , and is applied to rephrase the extracted sentences ) . again , using the learned reward helps the rl - based system generate summaries with significantly higher human ratings .
results are shown in table 2 . we observe that the training set size and the number of negative responses are the most important factors in model performance . the model performs best when trained with hinge loss instead of cross - entropy loss , indicating the importance of the loss function . table 2 shows the results of an ablation study we performed to identify the most interesting components of the model architecture and the mostchallenging ones to design . we observed that a 4 layer lstm performs similarly to either a 2 layer or a 3 layer sru .
performance of our model compared to an lstm encoder or an sru encoder on a single cpu core . sru exhibits a significant speedup in inference time compared to traditional encoders ( by a factor of 4 . 1x in our experiments ) , srus also exhibits a considerable speedup to encode a context in the current state of the art . retrieving the best candidate once the context is encoded takes a negligible amount of time compared with the time to encode the context . table 8 shows the scalability of using a dual encoder architecture . the sru achieves a speedup of up to 4 . 7x compared to the speed up of an existing encoder , since the sru is more than 4x faster at inference time , we can also see a noticeable drop in performance between inference time and production cost .
performance of our model according to these auc metrics can be seen in table 3 . the high auc indicates that our model can easily distinguish between the true response and negative responses . furthermore , the auc @ p numbers show that the model is able to distinguish between negative responses and positive responses .
4 shows rn @ k on the test set for different values of n and k when using a random table 4 shows that recall drops significantly as n grows , meaning that the r10 @ k evaluation performed by prior work may significantly overstate performance .
results in table 5 show that the clustering and frequency whitelists perform comparably to each other when the true response is added . however , bleu performs slightly worse than the baseline on both frequency and clustering datasets , indicating that there is a need to improve the recall scores for both datasets .
6 compares r @ 1 and coverage for the frequency and clustering whitelists . while the clustering dataset has higher recall , the frequency whitelist has higher coverage .
results of the human evaluation are in table 7 . our proposed system works well , selecting acceptable ( i . e . good or great ) responses about 80 % of the time , and selecting great responses more than 80 % . the size and type of whitelist seem to have little effect on performance , indicating that all the whitelists contain responses appropriate to a variety of conversational contexts .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , the cues are markedly gender - neutral , improving the bias metric by 2 % in the standard task formulation and to parity in the gold - two - mention case .
note particularly the large difference in performance between genders , both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , lee et al . ( 2017 ) and parallelism produce remarkably similar output : of the 2000 example pairs in the development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , the cues are markedly gender - neutral , improving the bias metric by 9 % in the standard task formulation and to parity in the gold - two - mention case . further , cues yield remarkably similar baselines to the baselines ( e . g . name a vs . name b ) on only 325 examples . token distance and topical entity are only weak improvements above random - single for goldtwomention .
istent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific heads and that these heads are in the deep layers of the network ( e . g . l3h7 , l4h7 ) . these heads are roughly balanced , with the exception of those that are not .
find that the instances of coreference that transformersingle can handle is substantially
investigate the effects of the multi - factor count ( m ) in our final model on the test datasets in table 3 . we observe that for the nyt10 dataset , m = { 1 , 2 , 3 } gives good performance with m = 1 achieving the highest f1 score . on the nyt11 dataset , it gives the best performance .
present the results of our model on the relation extraction task on the two datasets in table 3 . our model outperforms the previous stateof - the - art models on both datasets in terms of f1 score . on the nyt10 dataset , it achieves 3 . 3 higher f1 scores compared to the previous best state - of - the - model ea on the nyt11 dataset . similarly , the model improves the precision scores of both datasets with the help of bilstm - cnn trained on the same dataset ( table 3 ) . our model achieves the best results with a f1score of 0 . 541 on the three datasets compared to 0 . 6 on the four datasets . we observe that our model exhibits the best performance with the best recall scores . on both datasets , our model performs better than the best state of the art model ea on average .
4 shows the ablation results on the nyt11 dataset . when we add multi - factor attention to the baseline bilstm - cnn model without the dependency distance factor in the attention mechanism , we get 0 . 5 f1 score improvement ( a2 − a3 ) . adding dependency weight factor with a window size of 5 improves the score marginally ( a3 − a4 ) . replacing the attention normalizing function with softmax operation improves f1 by 3 . 5 points ( a4 − a6 ) . adding the dependency factor with max - pool operation improves the f1 scores by 4 . 5pp ( a6 − a7 ) . to mimic the effect of concatenated attention , we concatenate the features extracted by each attention layer . rather than concatenating them , we can apply maxmax operation across the multiple attention scores to compute the final attention scores . this helps the model to perform better in the multitasking task .
3 shows the performance of our models compared to other widely used visual detectors . we observe that the zsgnet model exhibits the best performance in terms of " animals " and " people " while " vehicles " have the worst performance . on the " clothing " dataset , we observe that it exhibits a lot more performance than the " bodyparts " model . this is mostly due to the high accuracy of the object detectors ( animals , bodyparts , vehicles )
2 compares zsgnet with prior approaches on flickr30k entities and referit . we use " det " and " cls " to denote models using pascal voc detection weights and imagenet [ 10 , 41 ] classification weights . networks marked with " * " fine - tune their object detector pretrained on pascalvoc on fixed entities . however , such information is not available in referit dataset which explains ∼ 80 % of the performance loss on fixed and unigramized objects .
4 shows the performance of our zsgnet model compared to qrg on the four unseen splits we observe that the accuracy obtained by qrg is significantly higher than the vg - split - 0 baseline , indicating that the model performs better on the unseen splits . moreover , the accuracy remains the same across all clusters , with the exception of those in the " split - 0 " set .
show the performance of our model with different loss functions using the base model of zsgnet on the validation set of referit in table 6 . note that using softmax loss by itself places us higher than the previous methods . further using binary cross entropy loss and focal loss give a significant ( 7 % ) performance boost which is expected in a single shot framework . finally , image resizing gives another 4 % increase .
ert outperforms no - reg and l2 on news , ted , l2 and news datasets . table 3 shows the performance of all fine - tuning schemes on news and ted datasets . ewc outperforms l2 news and newsmax on both datasets , but on newsmax , it is still better than l2 . ewc reduces forgetting on two previous tasks while further improving on the target domain .
es - en , the health and bio tasks overlap , but catastrophic forgetting still occurs under noreg ( table 3 ) . regularization reduces forgetting and allows further improvements on bio over noregon fine - tuning . ewc outperforms l2 and l2 in both domains .
5 shows uniform ensembling with different decoding schemes trained only on one domain : es - en bio , health , news , and it . identity - bi models perform better than uniform or unadapted models , leading to better interpretability for all domains . uniform models perform worse than uniform models , but bi and is both outperform uniform models in all domains except health and news . bi ' s adaptive decoding improves over uniform uniform decoding , with the exception of health and science .
table 6 we apply the best adaptive decoding scheme , bi + is , to models fine - tuned with ewc . ewc models perform well over multiple domains , so the improvement over uniform ensembling is less striking than for unadapted models . nevertheless adaptive decoding improves over the oracle model in most cases .
7 shows the bleu for test data concatenated across domains . bi + is performs similarly to the approach described in freitag and al - onaizan ( 2016 ) with ewc as the test domain , and with no test domain known as the ewc . uniform ensembling gives a 2 . 4 % boulu gain over the oracle model chosen with single domain modeling . with the exception of ewc , we can choose whether to use uniform or unadapted models , since both ewc and no - reg models perform similarly to single - domain models .
results in table 8 show that when only using original utterances with ellipsis , precision is relatively high while recall is low .
performance using different selection methods . table 6 shows that empirically adding logits from two models after classifiers performs the best .
table 6 , the best performance on map metric is achieved by our approach , which verifies the effectiveness of map modeling . we observe that our approach exceeds traditional neural models like cnn , lstm and ntnlstm by a noticeable margin .
table 2 , we can see that our approach exceeds the strong baselines of eur - lex , and achieves competitive results on rcv1 , prec , and eur - sparse datasets .
evident from table 1 , there is a significant imbalance in the distribution of training instances that are suggestions and non - suggestions , 2https : / / www . uservoice . com / for sub task a , the organizers shared a training and a validation dataset whose label distribution ( suggestion or a nonsuggestion ) is presented in table 1 .
3 shows the performances of all the models that we trained on the provided training dataset . the ulmfit model achieved the best results with a f1 - score of 0 . 861 on the training dataset and a f2 - score ( test ) of 1 . 7 .
4 shows the performance of the top 5 models for sub task a of semeval 2019 task 9 . our team ranked 10th out of 34 participants .
iii shows the wers on the simulated and real test sets when aas is trained with different training data . with the simulated dataset as the training data , fsegan ( 29 . 6 % ) does not generalize well compared to aas ( 25 . 2 % ) in terms of wer . when aas was trained with real and simulated datasets , it achieves the best wer ( 24 . 7 % ) on the real test set . in chime - 4 , aas performs better than aas in both scenarios .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . it is clear from the table that acoustic supervision ( 15 . 6 % ) and multi - task learning ( 14 . 4 % ) are the better performing methods for minimizing dce .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer ( e . g . , > 90 % ) , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . the same tendency is observed for acoustic supervision ( 31 . 1 % ) and multi - task learning ( 29 . 1 % ) .
cerning transfer learning experiments ( rq1 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . our approach shows a slight improvement over the performance of source - based model on two of the four scenarios , with the exception of the f - measure metric .
evaluating the approaches laid out in section iv , we consider three real - world datasets described in table ii . originally , all the raw messages for the datasets described by table ii were unlabeled , in that their urgency status was unknown . since macedonia is a small but information - dense country , we labeled all messages in macedonia as urgent or non - urgent ( hence , there is no unlabeled message in macedonia per table ii ) , and nepal as containing 205 messages . table ii shows that nepal is roughly balanced , while kerala is imbalanced . we used 205 messages per dataset , which indicates that there is a need to design more complicated datasets .
cerning transfer learning experiments ( rq1 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . our approach shows a slight improvement over the local baseline on f - measure metric , but still performing slightly worse than the local baseline .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) .
4 shows the synchronic performance of our system in the setup when tn and rn are tested on the locations from the same year ( including peaceful ones ) . at the same time , we observe exactly the same trends ( table 4 ) . for peaceful locations , our system performs better than the previous stateof - the - art systems on both datasets .
replication experiment in table 2 , we replicate the experiments from ( kutuzov et al . , 2017 ) on both sets . it follows their evaluation scheme , where only the presence of the correct armed group name in the k nearest neighbours of the ˆi mattered , and only conflict areas were present in the yearly test sets . essentially , it measures the recall @ k , without penalizing the models for yielding incorrect answers along with the correct ones , and never asking questions having no correct answer at all ( e . g . , peaceful locations ) . the performance is very similar across the two sets ,
3 shows the diachronic performance of our system in the setup when the matrix tn and rn are applied to the year n + 1 . for both gigaword and now datasets ( and the corresponding embeddings ) , using the cosine - based threshold decreases recall , but increases precision ( f1 ) .
4 : the ablation study on the woz2 . 0 dataset with the joint goal accuracy on the test set . the effectiveness of our hierarchical attention design is proved by an accuracy drop of 1 . 69 % after removing residual connections and the hierarchical stack of our attention modules .
3 shows the joint goal accuracy of the dst models on the woz2 . 0 test set and the multiwoz test set . we also include the inference time complexity ( itc ) for each model as a metric for scalability table 3 compares our model with the previous state - of - the - art on both datasets . for woz 2 . 0 , we maintain performance at the level of the state - ofthe - art , with a marginal drop of 0 . 3 % compared with previous work . considering the fact that woz is a relatively small dataset , this small difference does not represent a significant big big performance drop . on the other hand , for the multiwoz dataset , our model achieves an overall o ( n ) of 42 . 12 % , which marginally outperforms the previous work by 3 % .
5 : the ablation study on the multiwoz dataset with the joint domain accuracy ( jd acc . ) , joint domain - slot accuracy ( jds acc . ) and joint goal accuracy ( mg acc . ) on the test set . from table 5 , we can further calculate that given the correct slot prediction , comer has 87 . 42 % chance to make the correct value prediction . however , the accuracy is still lower than the jds acc . and mg acc . ( 87 . 42 % ) . we can also see that post - processing has a significant impact on model evaluation performance : the jd acc . has an absolute boost of 5 . 48 % when we switch from the slot prediction to the nested tuple prediction .
4 presents the results of domain transfer . our model ( ours ) obtains significant gains in accuracy over the baselines across all three target domains . it closely matches the performance of oracle with only 0 . 40 % absolute difference . we observe that all rationale - augmented methods ( ra - svm , ra - trans and ours ) outperform their rationale - free counterparts on average .
3 presents the results of aspect transfer . our model ( ours ) obtains substantial gains in accuracy over the baselines across all three target aspects . it closely matches the performance of oracle with only 0 . 40 % absolute difference . we observe that all rationale - augmented methods ( ra - svm , ra - trans and ours ) outperform their rationale - free counterparts on average . this confirms the value of human rationales in the low - resource settings . specifically , we observe that the rationale - neutral aspects of the beer review dataset outperform the rationale aspects of both source and target . this corroborates our intuition that the target aspects of beer review are more appealing to the rationales of the user . this validates our hypothesis that rationales and attention are different .
5 presents the results of an ablation study of our model in the setting of domain transfer . as expected , both the language modeling objective ( ours ) and the wasserstein distance contribute similarly to the task , with l [ l ] having the better interpretability .
compare our proposed method against the baseline catseq model on three datasets : krapivin , kp20k , and gan ( yuan et al . , 2018 ) . the results are summarized in table 5 . our proposed model performs better than the baseline on three of the four datasets , with f1 scores improving from 0 . 25 to 0 . 37 on all three datasets .
2 presents the results of our models in terms of α - ndcg @ 5 ( clarke et al . , 2018 ) . our model obtains the best performance on three out of the four datasets . the difference is most prevalent in kp20k , where our gan model ( at 0 . 85 ) outperforms all the other models except for the one that we included in the analysis .
simplify our dataset , we have decided to focus our work on job positions – which , we believe , are an interesting window into the nature of gender bias – , and were able to obtain a comprehensive list of professional occupations from the bureau of labor statistics ' detailed occupations table , from the united states department of labor . the values inside , however , had to be expanded since each line contained multiple occupations and sometimes very specific ones . fortunately , this table also provided a percentage of women participation in the jobs shown , which we filtered because they were too generic ( " computer occupations , all other " , and others ) .
we have found is that google translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender - neutral pronouns , indicating that the profession is indeed more useful for women than it is for men . this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes , such as life and physical sciences , architecture , computer science , engineering , mathematics and mathematics . table 6 summarizes these data ,
results in table 2 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 5 times as frequently as white aligned tweets . the classifier trained on the davidson et al . ( 2017 ) data is significantly less likely to classify blackaligned tweets as hate speech , although the disparity is narrower . for the founta et al . , 2018 classifier , we see that black aligned tweets are more frequently classified as offensive , although those in the white aligned corpus are slightly less frequently considered to be hate speech . for golbeck and ( 2018 ) we see a substantial racial disparity , with black - aligned tweets classified as hate - speech at 1 . 3 times the rate of white aligned ones . we see similar results for golbeck et al . : we see that for black aligned tweet , the rates of harassment are much lower than for white - aligned ones . in the same experiment , we find that blackaligned tweet are classified as harassment at a higher rate , at 2 . 1 times the rates for white aligned tweet . golbeck & hovy , 2017 , see the results of experiment 1 . we find that for both groups of tweets the rates are significantly lower than in experiment 1 , although we see higher rates . for davidson and al . , we see an absolute drop of 1 . 15 points compared to the previous experiment , while the rates remain relatively high .
performance of these models on the 20 % held - out validation data is reported in table 2 . we observe varying performance across classifiers , with some performing much better out - of - sample than others . in particular , we see that hate speech and harassment are particularly difficult to detect . since we are primarily interested in within classifier , between corpora performance , any variation between classifiers should not impact our results .
results in table 2 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 5 times as frequently classified as hate speech . the classifier trained on the davidson et al . ( 2017 ) data is significantly less likely to classify blackaligned tweets as offensive , although the disparity is narrower . golbeck et al . , 2018 , classifies black - based tweets as harassment at a higher rate for both groups , with the higher rate of those classified as offensive . for the founta and golbeck ( 2018 ) classifier , we see that between 0 . 005 and 1 . 7 times the rate of white aligned tweets , both for blackaligned and whitealigned tweets , is slightly higher than in the previous experiment , although we see a slight improvement .
results in table 2 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 5 times as frequently as white aligned tweets . the classifier trained on the davidson et al . ( 2017 ) data is significantly less likely to classify blackaligned tweets as hate speech , although the disparity is narrower . for the founta et al . , ( 2018 ) classifier we see that there is no significant difference in the estimated rates at which black aligned tweets are predicted to belong to this class . on the other hand , we see substantial racial disparities , with black - aligned tweets predicted to be hate speech at 1 . 7 times the rate of white aligned ones . we see that for the waseema and hove ( 2016 ) , the rates are significantly lower than those of white - aligned tweeters . golbeck et al . :
can be seen in table 2 , sparsemax and tvmax achieve better results when compared with softmax , indicating that the use of selective attention leads to better captions . moreover , when using selective attention , the accuracy is higher than softmax and similar on flickr30k . selective attention mechanisms like sparsemax , especially when using mscoco and bleu4 metrics , reduce repetition , and improve the accuracy of the captions generated .
performing slightly worse than sparsemax under automatic metrics , tvmax outperforms sparsemax and softmax in caption human evaluation and the attention relevance human evaluation , reported in table 2 . the superior score on attention relevance shows that tvmax is better at selecting the relevant features and its output is more interpretable . moreover , the superior caption evaluation results demonstrate that the ability to select compact regions induces the generation of better captions .
can be seen in the results presented in table 3 , sparsemax outperforms softmax and tvmax in all aspects except for the number of frames in the decoder . moreover , when using bounding box features , the accuracy is superior than softmax , indicating that the model is more suitable for the task at hand . we can see that selecting the features that the best suited for each decoder is beneficial , improving the accuracy and the accuracy of the results . this corroborates our intuition that selecting only the features of the highest quality image leads to a better answering capability . this validates our hypothesis that tvmax is a better complement to softmax in the self - attention layers of the image , as shown in fig . 3 . 6 . additionally , we can also see that combining the bounding boxes of the two decoder features improves the accuracy , since sparsemax is more accurate in answering the questions in question .
iv presents the system ' s performance on each error generation algorithm . we included only p @ 1 and p @ 10 to show trend on all languages . " random character " and " character bigrams " includes data for edit distance 1 and 2 whereas " characters swap " consists of data from edit distance 2 .
3 shows the performance of algorithms for edit distance 2 compared to [ bold ] trie , sda and directed acyclic word graphs ( dawgs ) . on the other hand , we see a performance gap of 2 . 57 % on average compared to 3 . 55 % on the bold dataset .
best performances for each language is reported in table ii . we present precision @ k9 for k ∈ 1 , 3 , 5 , 10 and mean reciprocal rank ( mrr ) . the system performs well on synthetic dataset with a minimum of 80 % p @ 1 and 98 % mrr . it is clear from the table ii that most of the languages used for this analysis have poor performance on synthetic datasets .
system is able to do each sub - step in real - time . all the sentences used for this analysis had exactly one error according to our system . detection time is the average weighted time weighted over number of tokens , ranking time is weighted over length of suggestions generated , and ranking time performs similarly weighted over suggestions generated .
performance of most popular spell checkers is reported in table vi . since most of these tools only work on word - error level , we used only unigram probabilities for ranking . we used only p @ 1 , p @ 2 and p @ 10 to compare against the best performing systems .
shown in table ii , most of the words for each language were detected as known but still there was a minor percentage of words that were detected .
, table 9 presents the results of models trained on tweets from one domain to the other . we observe that predictive performance is relatively consistent across all domains with two exceptions ( ' food & beverage ' consistently shows lower performance , while ' other ' achieves higher performance ) with the exception of apparel .
total , 1 , 232 tweets ( 62 . 4 % ) are complaints and 739 are not complaints ( 37 . 4 % ) . the statistics for each category is in table 3 .
unigrams and part - of - speech features specific of complaints and non - complaints are presented in table 4 . all correlations shown in these tables are statistically significant at p < . 01 , with simes correction for multiple comparisons . unigrams ( error , issue , working , fix ) are the most distinctive features for a tweet not being labeled as a complaint . in addition , there are a number of other distinctive features that are distinctive of complaints ( not , no , won ' t ) that are not complaints ( good , win , posemo , affect , assent ) . several distinctive patterns are present in the unigram patterns of complaints , not , complaints , and win ( voca ) . on the other hand , these patterns are less distinctive for tweets not being complaints . we see a distinctive pattern emerging around pronoun usage : pronouns followed by nouns are frequently used to describe complaints , but are not used in this analysis . complaints use more possessive pronouns indicating that the complaint is addressed to the responsible party ( e . g . , the complainer , the exclamation marks , the administrator ) . across all clusters , there is one distinctive pattern that is most distinctive about complaints : the presence of verbs in the past tense ( a ) indicates that the user is describing personal experiences . this pattern appears to be more prevalent in complaints about customer service , as many complaints are formulated as complaints to service providers .
top features for the liwc categories and word2vec topics are presented in table 4 . top liwc category features include : negate ( negate ) . general topics such as requiring assistance or customer support ( he , she , it , him , you , shehe , male , female ) and function ( prpemo ) . all the words used to express complaints are accompanied by a label or a label indicating that the complaint is addressed to a third party ( e . g . , a non - complainer , a party or a party ) . several groups of words are used as part of the complaint label : requiring assistance , customer support , customer assistance ( in the retail domain ) , and customer support .
models trained on stanford data are shown in table 4 . the best results are obtained using the volkova & bachrach model ( sentiment – v & b ) which achieves 60 f1 and 55 . 2 auc scores respectively compared to the previous best state - of - the - art model ( nrc ) . complaint specific features are predictive of complaints , but are significantly worse than sentiment , reaching an f1 of up to 77 . 5 and auc of 0 . 866 . further , the best performance is obtained using language trained on the stanford dataset ( v & b ) . the combination of syntactic and semantic features further improves predictive performance . syntactic features such as the liwc dictionaries perform best , but do not improve predictive performance significantly over syntactic features . we observe that neural network approaches are more effective than syntactic or semantic features , improving predictive performance by a large margin . they complement the syntactic patterns of speech tags with a high f1 score of 77 . 2 and 0 . 864 auc . they also outperform the sentiment - neutral approach by a margin of 2 . 6 points compared to stanford .
results in table 7 show that the domain adaptation approach further boosts f1 by 1 point to 79 ( t - test , p < 0 . 5 ) and roc auc by 0 . 012 .
8 shows the model performance in macro - averaged f1 using the best performing feature set . we base our model on tweets from each domain , adding out - of - domain data as inputs . the apparel domain is qualitatively very different from the others as a large number of complaints are about returns or the company not stocking items , hence leading to different features being important for prediction . domain adaptation is beneficial for all domains , with the exception of transport . we show the results in table 8 , which shows the performance of the apparel domain in each domain .
ert achieved a final accuracy of 91 . 20 % , now marginally comparable to ulmfit ' s full performance . gpt - 2 , on the other hand , finetuned to an accuracy of 96 . 28 % , a full 4 . 42 % improvement over the performance of ulm fit . finetuning time is an average of 4 . 48 hours per epoch , compared to the previous state of the art model . multitasking time is a reasonable amount of time for a model to train on a single network , with a finetuning speed of up to 4 . 53 hours .
table 5 , it can be seen that generative pretraining via language modeling does account for a considerable amount of performance , constituting 44 . 32 % of the overall performance ( a boost of 42 . 67 % in accuracy ) in the multitasking setup and constituting 43 . 93 % in the standard finetuning setup , a boost of 39 . 97 % in performance .
shown in table 6 , reducing the number of attention heads severely decreases multitasking performance . using only one attention head , thereby attending to only one context position at once , degrades the performance to less than the performance of 10 heads using the standard finetuning scheme . this shows that more attention heads is important to boosting performance to state - of - the - art results .
6 shows the ablation study results on paragraph selection loss loss lpara and entity prediction loss lentity . as shown in the table , using paragraph selection can further improve the joint f1 by 0 . 31 points ,
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distractor and fullwiki set , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distract and full wiki setting , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
3 shows the performance of paragraph selection on the dev set of hotpotqa . in dfgn , paragraphs are selected based on a threshold to maintain high recall ( 98 . 28 % ) , leading to a low precision ( 60 . 53 % ) . compared to both threshold - based and pure paragraph selection , our proposed paragraph selection process is more accurate , achieving 94 . 53 % precision and 94 . 59 % recall ( 95 . 53 % ) compared to the threshold of 98 . 53 % .
shown in table 5 , the use of ps graph improves the joint f1 score over the plain roberta model by 3 . 36 points . by further adding entity nodes , the joint f1 increases by 2 . 59 points . these results show that dfgn ( bert - base ) is better than eps ( ours ) on the dev set .
performance of hgn for different reasoning types is shown in table 7 . our hgn outperforms dfgn and eps , indicating that the performance gain comes from a better model design .
4 shows the bleu scores of dual2seq models taking gold or automatic amrs as inputs . the improvement from automatic amr to gold amr ( + 0 . 5 blei ) is significant , which shows that the translation quality of our model can be further improved with an increase of amr parsing accuracy .
3 shows the test bleu , ter and meteor scores of all systems trained on the smallscale news commentary v11 subset or the largescale full set . dual2seq is consistently better than both opennmt - tf and transformer - tf in both settings . the dual 2seq model is more accurate in the small - scale settings than in the large - scale setting . when trained on smallscale settings , the model performs slightly worse than the seq2seq baseline under all three scenarios except for the nc - v11 subset . this indicates that the reliance on syntactic patterns leads to a more accurate model .
most representative models are elmo , gpt , bert and its variants , and xlnet . next , we give a brief overview of these models and summarize their performance on the selected benchmark tasks . table 2 quantitatively compares these models against each other . most of the models perform similarly on the benchmark tasks , with the exception of bert ( which performs on par with google translate and word2vec ) , while others perform slightly better on the smaller benchmarks .
2 shows that coreference propagation ( corefprop ) improves named entity recognition performance across all three domains . the largest gains are on the computer science research abstracts of scierc and genia .
3 presents the performance of our model on the entity and relation extraction tasks . our framework establishes a new state - of - the - art on all three high - level tasks , with the exception of event argument identification . relative error reductions range from 0 . 2 - 27 . 9 % over previous state of the art models .
3 shows that coreference propagation ( relprop ) improves relation extraction performance on scierc and wlpc . relation propagation improves bert ' s f1 scores by 3 . 5 points over pretrained bert models .
7 compares the performance of bert and scibert models on the scierc and genia datasets . the results are summarized in table 7 . the best bert model performs better on scierc , while the genia model performs slightly worse .
6 shows that both variations of our bert model benefit from wider context windows . our model achieves the best performance with a 3sentence window across all relation and event extraction tasks .
table 1 we report the best and average precision @ 1 scores and the average number of iterations among 10 experiments , for different language translations . our model improves the results in translation . we find that the noise - aware model is more stable and therefore requires fewer iterations to converge . the accuracy improvements are small but consistent , and we note that we consider them as a lower - bound on the actual improvements as the current test set comes from the same distribution of the training set , and also contains similarly noisy pairs .
performance of the baselines on the instructional and interdomain evaluations is reported in table 3 . the baselines trained on the instr - dt dataset are significantly better than the existing baselines , the two - stage approach by wang et al . ( 2017 ) achieves the best performance with an intra - and interdomain evaluation ( 83 . 88 % vs . 82 . 88 % ) . the hierarchical right / left branching baselines dominate the sentiment - neutral discourse , leading to an overall improvement of 2 . 36 % over the previous state - of - the - art model ( hernault et al . , 2010 ) . the rst - dt outperforms the other approaches with an absolute improvement of 3 . 36 % . the inter - domain evaluation is further investigated in table 4 .
4 shows the evaluation results . our dkrn agent outperforms all the other agents with a large margin .
3 shows the turn - level evaluation results . our approach dkrn outperforms all state - of - the - art methods in terms of all metrics on both datasets with two tasks .
5 shows the evaluation results . our dkrn agent outperforms all the other agents with a large margin .
6 shows the results of the second study . our agent outperforms all the comparison agents with a large margin .
istic output on a resnet - 152we show encouraging relative improvements for future research in this direction . we empirically found that self - attention was the most efficient in the 3rd stage . it is clear from table 1 that it is beneficial to continue researching into visual modulation
experimental results shown in table 1 show that a resnet - 152 is able to distinguish between self - attention and self - monitoring . we notice small improvements relative to the baseline showing that self - awareness is important for improving the model ' s performance . we notice that the improvement is modest but significant : we see that it is possible to improve the model by adding more self - aware modules to the resnet in the future .
experimental results of all models are shown in table 2 . first , all models appear to be comparable in terms of performance . han models outperform all the base models except mead . mead has been shown to perform well in previous studies ( luo et al . , 2016 ) and it appears to handle redundancy removal exceptionally well . we observe that the redundancy removal step is beneficial for both systems , improving the rouge scores and the f - score by 3 . 2 points ( svm - svm , mead ) . mead performs similarly to mead except for mead , where it performs better in redundancy removal . it is perceptible that there is a difference in performance between mead and mead because redundancy removal is more efficient , hence less effort is required to reproduce the results . we suspect that there are not enough data to pretrain mead models , hence why there is no need to re - evaluate these models . we observed that redundancy removal was beneficial in mead ' s case ( table 2 ) . it reduces the noise of thread removal and sentence prediction . it helps the system to learn effective thread vectors and to learn the thread classification task . it improves the recall scores of thread prediction and thread prediction . this suggests that future work may need to consider redundancy removal strategies .
3 : joint goal accuracy on dstc2 and woz 2 . 0 test set vs . various approaches reported in the literature . table 3 compares the effectiveness of each approach . statenet ps outperforms statenet , and statenet performs best among all 3 models . we also compare it with the approach described in peyrard et al . ( 2017 ) and rastogi et al . , ( 2017 ) . the model developed in the previous literature is delexicalised and semantic , improving the joint goal accuracy by 3 . 6 points .
2 : joint goal accuracy on dstc2 and woz 2 . 0 of statenet psi using different pre - trained models based on different single slot . the fact that the food initialization has the best performance verifies our selection of the slot with the worst performance for pre - training . we can see from table 2 that our food initialization is more accurate than the slot initialization with the best performing pricerange .
5 shows the performance of multi30k model in asymmetric mode . ame outperforms fme and fme , confirming the importance of word embeddings adaptation .
show the results for english and german captions . for english captions , we see 21 . 28 % improvement on average compared to mono due to more training data and multilingual text encoder . ame performs better than fme model on both symmetric and asymmetric modes , which shows the advantage of finetuned word embeddings .
german descriptions are presented in table 9 . the results are 11 . 03 % better on average compared to mono in german descriptions . we find that ame performs better than fme in both languages ,
achieve 10 . 66 % improvement on average compared to kiros et al . ( 2014 ) in symmetric mode . we show that adapting the word embedding for the task at hand , boosts the general performance , since ame performs better than fme model in both symmetric and asymmetric modes .
5 shows the results for english and german captions . ame reaches 6 . 25 % and 3 . 66 % better results on average compared to monolingual model , respectively , compared to symmetric and asymmetric modes .
4 shows the results for italian and german , compared to english , both for the original and the debiased embeddings . in both languages , the difference between the averages of the two sets is much lower . in italian , we get a reduction of 91 . 67 % of the gap with respect to english . in german , the reduction is 100 . 67 % . in both cases , it is much more difficult to find a difference of this magnitude between the average of the 2 sets in the two languages than it is to find one that is completely different .
2 shows the results for italian and german , compared to english . as expected , the average ranking of samegender pairs is significantly lower than that of different - gender pairs , both for german and italian , while the difference between the sets in italian is much smaller . table 2 compares the results of the original and the debiased embeddings for german , italian , and turkish .
6 shows the results for italian and german , compared to the original embeddings . in both cases , the results are slightly better than the original ones , in both cases .
results in table 7 show that precision on the embeddings for german and italian is indeed high , both for the debiased and unbalanced sets , i . e . that there is no difference in the effect of debiasing . also , the accuracy remains the same for both languages ,
3 shows the ari and silhouette scores of all methods trained on the same dataset ( od - d2v and od - w2v ) . we benchmark against the following baselines : wmd ( which relies on word2vec embeddings ) , doc2vec and tf - idf . the results are summarized in table 3 . the ari scores of both methods are reported in the table . the wmd baseline ( barely providing a performance improvement over the baseline ) is statistically significant ( paired t - test , p < 0 . 01 ) with respect to baselines at significance level 0 . 005 . opinion distance od significantly outperforms the baseline on all metrics except ari . in the exceptional case of " hydroelectric dams " dataset , we observe that the clustering technique performs best on the wmd dataset , with the exception of the rand index .
3 shows the ari and sil scores of methods trained on the seanad abolition and doc2vec datasets ( od - parse ) . od significantly outperforms the baseline methods , with the exception of tf - idf . od achieves low ari , sil and od - parse scores , respectively , on the " video games " and " pornography " datasets ( barely providing a performance improvement over random clustering , i . e . , a zero ari score ) . the most striking thing about od is that it relies on syntactic or semantic cues derived from " seanad " or " dkr " embeddings instead of " doc2vec " . this suggests that od relies on superficial cues , such as the word " democracy " . we observe that the clustering patterns used by od are more interpretable than those by wmd , sent2vec , or wmd . these methods perform slightly better than wmd on the ' video games ' dataset , but are slightly less effective than od .
completeness , here we also compare against unigram or n - gram based classifiers the classification performance of the baselines is reported in table 4 . svm with unigrams and bigrams achieves the best performance on the " video games " and " pornography " datasets , while svm has the worst performance . we observe that the classification performance based on baselines svm and lsa is significantly better than any combination of features excluding the word " democracy " .
ributhe results of different variants are shown in table 1 . od significantly outperforms od - parse : we observe that compared to od , od is much more accurate . on the three datasets , od achieves an average weighted f1 score of 0 . 54 , 0 . 56 and 0 . 41 respectively compared to the scores of zero , - 0 . 01 , and zero . 01 by od . table 1 shows that od is significantly better than jensen - shannon divergence on all variants of the dataset .
performance of our model on the testing and development set is shown in table 3 . the difference in accuracy between development and testing set is minimal , however we see significant difference in macro - f score due to different class balance in these sets . we observe that the branch - lstm model predicts commenting , the majority class well .
denying instances get misclassified as commenting ( see table 5 ) ,
1 shows the absa datasets from the restaurants domain for english , spanish , french , dutch , russian , turkish , turkish and turkish . from left to right each row displays the number of tokens , number of targets and the type of training and test set .
3 provides detailed results on the opinion target extraction ( ote ) task . we show the results of the best models using only one type of clustering feature , namely , the best brown , clark and word2vec models , respectively . we observe that the best clustering features are the best on both datasets , with the best performing on the 2015 and 2016 datasets .
5 shows the performance of our system across all languages . our system outperforms the best stateof - the - art systems across all three languages .
errors in our system are caused by false negatives [ fn ] , as it can be seen in table 7 .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
greek analogy test set contains 39 , 174 questions divided into semantic and syntactic analogy questions . semantic questions are divided into 15 categories and include 13 , 650 questions in total . they are mostly language specific , with a total of 260 , 524 questions in the semantic category .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
4 shows the similarity of our model to other models . we see that gr def model has the highest correlation with human ratings of similarity .
epm " generalizes best , and in out - ofdomain evaluations , it considerably outperforms ensemble coreference , i . e . , the ensemble model of e2e - coref , in terms of conll score , pt score , and lemma score .
pos and named entity tags have the least and the pairwise features have the most significant effect . pos and - pairwise have the biggest impact , but pairwise feature - values have the smallest impact .
orporating epm feature - values improves the three points . performance by about it achieves onpar performance with that of " g & l " .
observe that incorporating all the linguistic features bridges the gap between the performance of " top - pairs " and " ranking " . it also improves the conll performance by 3 . 8 points , which shows the competitiveness of incorporating more linguistic features .
observe that the impact on generalization is also not notable , i . e . the conll score improves only by 0 . 3pp over " ranking " .
orporating epm feature - values improves the three points . performance by about it achieves onpar performance with that of jim while jim achieves a slight improvement .
first analyze the coverage of the vsms in question with respect to lexica , see table 4 . for brevity we only report coverage on w2 contexts lemmatization allows more targets to exceed the sgns frequency threshold , which results in consistently better coverage . pos - disambiguation , in turn , fragments the vocabulary and fragments the lexica . it is clear from the table 4 that the shared vocabulary contributes significantly to the coverage .
1 summarizes the performance of the vsms in question on similarity benchmarks . lemmatized targets generally perform better , with the boost being more pronounced on simverb . adding pos information benefits the model performance , lemmatization targets show a significant drop in performance compared to the original embeddings . using morph - fitting ( mfit - a ) improves the vsm performance by a noticeable margin . the type - based vsms generally perform similarly on simlex , simlex and simlex datasets , however , the difference is less pronounced for simlex due to the larger size of the training set . morph - fitting consists of two stages : the attract ( a ) stage brings word forms closer together , while the repel ( r ) stage sets the derivational repel further apart .
5 provides exact scores for reference . we use w2 contexts for reference : type - pos and lemma - based type - based targets . the results are summarized in table 5 . for type - aligned targets , we see significantly better f scores ( p ≤ . 005 ) than on lemmatized targets ( p ≈ . 005 ) . on the other hand , the difference is less pronounced for vn , which is more likely to rely on dep - based contexts . note that pos disambiguation requires a significant amount of training data to perform in the vn context , which results in significantly better performance than using dep based contexts .
embeddings derived from wiki - pubmed - pmc outperform glove ( mikolov et al . , 2013a ) in the extraction of most relation types . the models using boc feature outperform the methods using bow and asm features . for example , the svm with boc features performs better than the other three baselines on average ( table 1 ) with an absolute improvement of 3 . 6 % on average .
applying the co - occurrence baseline ( ρ = 0 ) improves the relation extraction performance for all approaches except boc ( wiki - pubmed - pmc ) and toxicity .
3 compares multi - news to other mds news datasets used in experiments below . we choose to compare mult - news with duc data from 2003 and 2004 and tac 2011 data , which are typically used in multi - document settings . additionally , we compare to sds datasets , as this has been recently used in work which adapts sds to mds ( lebanoff et al . , 2018 ) . the total number of examples in multinews is two orders of magnitude larger than other news datasets , meaning the average number of words in the concatenated inputs is less than the sds dataset used in mds experiments .
4 shows the percentage of n - grams in the gold summaries which do not appear in the input documents as a measure of how abstractive our summaries are in table 4 . multi - news is comparable to the abstractiveness of sds datasets , while sds is comparable . grusky et al . ( 2018 ) additionally define three measures of the extractive nature of a dataset , which we use here for a comparison .
model outperforms pg - mmr when trained and tested . the transformer performs best in terms of r - 1 while hi - map outperforms it on r - 2 and r - su . also , we notice a drop in performance between pg - original ( which takes the pre - trained pg - news and applies mmr on top of it ) and pg - brnn ( which applies mmr to the original and the mmr on the latter ) .
shown in table 5 , as the required derivation step increases , the prkgc + ns model suffers from predicting answer entities and generating correct nlds . this indicates that the challenge of rc - qede is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer .
evaluation results shown in table 2 indicate that the annotated nlds are of high quality ( reachability ) , and each nld is properly derived from supporting documents ( derivability ) . we found that more than 90 % of 294 ( out of 900 ) 3 - step nlds has missing steps to derive a statement .
shown in table 4 , the prkgc models learned to reason over more than simple shortest paths . as shown in fig . 4 , having the shortest shortest path indicates the use of supervised nlds as supervision ( i . e . using ld during training ) . supervising path attentions are important for rc - qede to improve the model ' s interpretability and generalization ability . it is clear from table 4 that these models are interested in improving the interpretability of question answering . they find the shortest path to be the most interesting , and the most difficult to solve .
shown in table 7 , the prkgc models achieve a comparable performance to other sophisticated neural models .
results are presented in table 3 . we observe that the ' traditional ' lstm layout outsperformed the ' alternating ' one we chose for our submission . apart of the flipped cv , we observe that both the flipped and un flipped results indicate that the model performs better in the singlemodel scenario . finally , we notice that the ablation results are slightly worse than our original model ( no - overlaps , dropout , founta et al . , 2017 ) due to the size of the data set and the training set size . further , the results are less consistent when we compare the flipped results of the original and flipped ones , indicating that the models are performing well in the final scenario .
system ' s official score was 60 . 9 % ( micro - f1 ) af therefore , we report both the official score ( from our second submission ) and the result of re - scoring these runs after replacing these 10 files with the ones from our first submission . the results are presented in table 1 .
can be seen in table 2 , both the official score ( from our second submission ) and the result of re - scoring these 10 files after replacing them with the ones from our first submission . the results are presented in tables 1 and 2 .
table 2 , we compare relis with state - of - the - art neural models trained on the duc datasets ( icsi , tcsum , srsum ) and deeptd datasets ( srsum ) . relis performs better than deeptd , but it has the advantage of training on a larger corpus . relis outperforms deeptd in terms of r & r , and is comparable with deeptd on average .
2 compares the quality of our ^ ( cid : 27 ) u x with other widely used rewards for input - specific rl ( see x4 ) . ^ ( cus ) ux has significantly higher correlation to the ground - truth ranking compared with all other approaches , confirming that our proposed l2r method yields a superior reward oracle .
orporating polarity features improved the results for task b . several models outperform the baseline in all aspects except for micro f1 .
task a , all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings .
task b , all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings .
performed an ablation study on a single model having obtained 69 . 23 % accuracy on the validation set . results are summarized in table 2 . we can observe that emoji significantly increased the performance for our model , with an absolute boost of 3 . 71 % . we observed that emoji also contributed significantly to the model ' s performance , with the exception of the elmo layer providing a significant boost of performance . finally , we tried using sgd with different learning rates and a stepwise learning rate schedule as described by conneau et al . ( 2017 ) , but found out that doing this did not improve the performance .
3 presents the classification report . it can be seen that anger and surprise are the most difficult class to predict , followed by surprise , whereas joy , fear , and disgust are the better performing ones .
4 shows the overall effect hashtags have on classification performance . tweets containing emoji seem to be easier for the model to classify than those without . hashtags also have a positive effect , however it is less significant .
5 shows the effect specific emoji have on classification performance . it is clear some emoji strongly contribute to improving prediction quality . the most interesting ones are mask , rage , and cry , which significantly increase accuracy . further , contrary to intuition , the sob emoji contributes less than cry , despite representing a stronger emotion . finally , not all emoji are beneficial for this task . when removing sweat smile and confused accuracy increased ,
results on winograd and winocoref datasets are shown in table 7 . the best performing system is knowcomb . it improves by over 20 % over a state - of - art general coreference system , on average . it also improves by 15 % over rahman and ng ( 2012 ) . these results show significant performance improvement by using predicate schemas knowledge on hard coreference problems .
results on standard ace and ontonotes datasets are shown in table 8 . our knowcomb system achieves the same level of performance as does the state - of - art general coreference system we base it on . as hard coreference problems are rare in standard coreference datasets , we do not have significant performance improvement . however , these results show that our additional predicate schemas do not harm the predictions for regular mentions .
ailed analysis to study the coverage of our predicate schemas knowledge , we label the instances in winograd ( which requires type 1 / type 2 schema knowledge , respectively ) . the distribution of the instances is shown in table 9 . the type of coverage that we cover is the coverage required by our propel schema knowledge . the percentage of instances that have type 1 or type 2 knowledge is also shown in the table .
also provide an ablation study on winocoref dataset in table 10 . these results use the best performing knowcomb system . they show that both type 1 and type 2 schema knowledge have high precision on category 1 and category 2 datainstances , respectively , compared to that on full data . type 1 schema knowledge has similiar performance on both data , but the results show that it is harder to solve instances in category 2 than those in category 1 . this indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition .
5 shows the numerical results obtained during the experiments . in general terms , the results displayed in table 5 show that the rejection method can reduce the error of the output predictions when applying a pre - trained black - box classification system to a new domain . table 5 : accuracy obtained by training an standalone classifier , applying the api and the proposed wrapper for each domain
