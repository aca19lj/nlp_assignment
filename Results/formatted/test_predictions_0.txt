results are shown in table 4 . training data b - copa ( 50 % ) reduces training error on the easy and hard subsets , but does not improve performance on the hard subset , both when training with 50 % of the training data , and when matching the training size of the original copa ( 50 % ) . note that training on these subsets is strictly limited to the size of training instances , as these are typically referred to as training instances of roberta content .
studies show that bert and roberta achieve considerable improvements on copa ( see table 1 ) .
2 shows the five tokens with highest coverage . for example , a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21 . 2 % of copa training instances . its productivity of 57 . 5 % expresses that it appears in correct alternatives 7 . 7 % more often than expected by random chance . this suggests that a model could rely on such unbalanced distributions of tokens to predict answers based only on alternatives without understanding the task .
human evaluation shows that our mirrored instances are comparable in difficulty to the original ones ( see table 3 ) .
then compare bert and roberta with previous models on the easy and hard subsets . 7 as table 4 shows , previous models perform similarly on both subsets , with the exception of sasaki et al . ( 2017 ) . 8 overall both bert ( 76 . 5 % ) and rberta ( 71 . 7 % ) considerably outperform the best previous model . however , bert ' s improvement over previous models is less striking on the hard subset , where training data is unsupervised , and the difference between accuracy on this subset is less pronounced . this suggests that bert relies less on superficial cues than previous models .
relatively high accuracies of bert - large and roberta - large show that these pretrained models are already well - equipped to perform this task " out - of - the - box " . moreover , training on b - copa improves performance on the hard subset as well as the easy subset as shown in table 4 .
observe that bert trained on balanced copa is less sensitive to a few highly productive superficial cues than bert - trained on original copa . note the decrease in the sensitivity of cues from 0 . 7 to 0 . 9 . these cues are shown in table 7 . however , for cues with lower productivity , the picture is less clear , in case of roberta , there is no noticeable difference .
, the proposed cnn - lstmour - neg - ant improves upon the simple cnn - w / o neg . baseline with f1 scores improving from 0 . 72 to 0 . 78 for positive sentiment and from 1 . 66 to 2 . 66 for negative sentiment . the proposed cnnlstmsour - neg - ant has the best f1 score and is 1 . 86 better than the simple svm with 0 . 70 and 0 . 83 .
results in table 7 show that the method is comparable to stateof - the - art bilstm model from ( fancellu et al . , 2016 ) on gold negation cues for scope prediction .
statistics are shown in table 3 . the average number of tokens per tweet is 22 . 3 , per sentence is 13 . 6 and average scope length is 2 . 9 .
4 shows the f - score of false negation from a 0 . 61 baseline to 0 . 68 on a test set containing 47 false and 557 actual negation cues .
results are shown in table 5 . we report the average value of diversity and appropriateness , and the percentage of responses scored as average . with data augmentation , our damd model obtains a significant improvement in diversity score and achieves the best average appropriateneness score as well . however , the slightly increased invalid response percentage we observe that hdsa model significantly outperforms hdsa in the diversity score , and is comparable to damd in the appropriatenization score as a baseline .
results are shown in table 4 . after applying our data augmentation , both the action and slot diversity are improved consistently , hdsa has the worse performance and is more difficult to solve than our proposed domain - aware multi - decoder network . we also observe that the number of action and the average number of data diversity are relatively low , we notice that the data diversity is relatively high across all classifiers , especially in the 5 - action generation group , from the above table , we observe that data diversity has a generally positive effect on the performance of the classifier , however , it is less beneficial for the slot diversity .
3 shows that after applying our domain - adaptive delexcalization and domain - aware belief span modeling , the task completion ability of seq2seq models becomes better , and the combined score of our model improves . the damd model significantly outperforms other models with different system action forms in terms of inform and success rates , while seq2seq model achieves a limited improvement on combined score , it is unable to solve the system action problems in a single task . we conjecture that if a model has access to ground truth system action , the model may be able to improve its task performance in the longer term .
3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and qa - srl . the models are evaluated using exact match ( em ) and f1 measures , as the results show , incorporating coverage improves the model ' s performance in the in - domain evaluation as well as in the out - ofdomain evaluation .
2 shows the performance for both systems for in - domain ( the multinli development set ) as well as out - of - domain evaluations on snli , glockner , and sick datasets . the results show that coverage information considerably improves the generalization of both examined models across various nli datasets . in particular , the improvements on the snli and glockner datasets are larger than those on the sick dataset .
4 shows gdpl has the smallest kl - divergence to the human on the number of dialog turns over the baselines , which implies that gdpl behaves more like the human .
performance of each approach that interacts with the agenda - based user simulator is shown in table 3 . acer and ppo achieve extremely high performance in the task success on account of the substantial improvement in inform f1 and match rate over the baselines . surprisingly , gdpl even outperforms human in completing the task , and its average dialog turns are close to those of humans , though gdpl is inferior in terms of match rate . it is perceptible that gdpl has better performance than ppo and aldm due to the small size of the dialog turns and the large variation in task success . though aldm obtains the upper - body advantage over ppo , it does not achieve the higher performance of gdpl - discr . ablation test results are also perceptible in table 3 ,
agent performance is shown in table 5 . all the methods cause a significant drop in performance when interacting with vhus . aldm even gets worse performance than acer and ppo . in comparison , gdpl is comparable in terms of match rate and even achieves higher task success . gdpl even outperforms acer , ppo and aldm .
6 presents the results of human evaluation . gdpl outperforms three baselines significantly in all aspects ( sign test , p - value < 0 . 01 ) except for the quality compared with acer . among all the baselines , gdpl obtains the most preference against ppo .
7 shows the distribution of the return r = t γtrt according to each metric . it can be observed that the learned reward function has a significant impact on the performance of the dialog , as it gets a full score on each metric , and a negative one on the other metric .
present precision scores for the word analogy tests in table vii . our proposed method outperforms the original glove embeddings in semantic analogy test set and in overall results . however , it has the advantage of training in syntactic and semantic analogy tests . therefore , we proposed to improve the results for the proposed method by increasing the precision scores to 73 . 53 % and total score to 71 . 15 % . however , this result is still slightly better than the original proposed method . this is mostly due to the smaller size of the dataset and the syntactic similarity of the two proposed methods .
apply the test on five participants . results tabulated at table v shows that our proposed method significantly improves the interpretability by increasing the average true answer percentage from ∼ 28 % for baseline to ∼ 71 % for our method .
correlation scores for 13 different similarity test sets and their averages are reported in table vi . we observe that , let alone a reduction in performance , the scores obtained by word2vec are significantly better than the original embeddings . according to the results , the average score of word2sense is 0 . 005 , 0 . 669 and 0 . 869 respectively compared to the original score of 0 . 866 . word1sense performs slightly better on some of the test sets than it does on others . it is clear from the results that the similarity test set by roget et al . ( 2017 ) is a result of the high correlation between similarity values and the performance of the proposed algorithm . it should also be noted that the scores computed by spine is unacceptably low on almost all tests indicating that it has achieved its interpretability performance on a larger corpus .
investigate the effect of the additional cost term on the performance improvement in the semantic analogy test in table viii . in particular , we present results for the cases where i ) all questions in the dataset are considered , ii ) only the questions that contains at least one concept word are considered . iii ) for the cases of " all questions " that consist entirely of concept words , we observe that for all three scenarios , our proposed algorithm results in a performance improvement comparable to that of the original embeddings . however , the greatest performance increase is seen for the last scenario , which underscores the extent to which the semantic features captured by the proposed algorithm can be improved with a reasonable selection of the lexical resource from which the concept words were derived .
accuracies are presented in table ix . the proposed method outperforms the original embeddings and performs on par with the sov . however , it has the advantage of training on a larger corpus . this result along with the intrinsic evaluations show that the proposed imparting method can significantly improve interpretability without a drop in performance .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results on event coreference . our joint model outperforms all the base lines with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results reconfirm that the joint model , when combined with effective topic clustering , achieves the best f1 score .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results of cluster + kcp again . our joint model outperforms all the base models with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results reconfirm that pre - clustering of documents to topics is beneficial , improving upon the kcp performance by 3 . 6 points , though still performing substantially worse than our joint model .
also show the precision numbers for some particular recalls as well as the auc in table 1 , where our model generally leads to better precision .
also show the precision numbers for some particular recalls as well as the auc in table 2 , where pcnn + att ( 1 ) refers to train sentences with two entities and one relation label , pcnn + . the table 2 shows that our model exhibits the best precision numbers .
experimental results on wikidata dataset are summarized in table 3 . the results of " - word - att " row refers to the results without word - level attention . according to the table , the drop of precision demonstrates that the word level attention is quite useful .
the table 4 depicts , the training time increases with the growth of d .
5 shows the comparison of 1 - 5 iterations . we find that the performance reach the best when iteration is set to 3 .
3 presents the rouge scores of our system ( neuraltd + learnedrewards ) and multiple stateof - the - art systems . the summaries generated by our system outperform all the previous systems except for the one by chen et al . ( 2018 ) which is still broken down in terms of performance . our system learns better than any previous system we have tried ( yuan et al . , 2018b , c , d , e , n , p , c ) .
table 1 , we find that all metrics we consider have low correlation with the human judgement . more importantly , their g - pre and g - rec scores are all below . 50 , which means that more than half of the good summaries identified by the metrics are actually not good , and more than 50 %
2 shows the quality of different reward learning models . we first consider the feature - rich reward learning method proposed by peyrard and gurevych ( see § 2 ) . mlp with bert as en ( 2018 ) coder has the best overall performance . specifically , bert + mlp + pref significantly outperforms ( p < 0 . 001 ) all the other models that do not use bert - mlp ,
4 presents the human evaluation results . summaries generated by neuraltd receives significantly higher human evaluation scores than those by refresh ( p = 0 . 0088 , double - tailed ttest ) and extabsrl ( p ( cid : 28 ) 0 . 01 ) . also , the average human rating for refresh is significantly higher ( p : 28 ) , which indicates that the use of extractive summaries is beneficial for human evaluation .
5 compares the rouge scores of using different rewards to train the extractor in extabsrl ( the abstractor is pre - trained , and is applied to rephrase the extracted sentences ) . again , using the learned reward helps the rl - based system generate summaries with significantly higher human ratings .
results are shown in table 9 . as table 9 shows , the training set size and the number of negative responses for each positive response are the most important factors in model performance . the model performs significantly worse when trained with hinge loss instead of cross - entropy loss , indicating the importance of the loss function . we observed no advantage to using a hierarchical encoder , finally , we see that a 2 layer lstm performs similarly to a 4 layer sru with a similar number of parameters , but with a slightly larger variation of the function .
model makes use of a fast recurrent network implementation ( lei et al . , 2016 ) and multiheaded attention ( hochreiter and schmidhuber , 1997 ) and achieves a significant speedup in inference time compared to using an lstm encoder . retrieving the best candidate once the context is encoded takes a negligible amount of data compared to the time to encode the context . using an sru encoder with a similar number of parameters can further speed up inference time to a negligible extent .
performance of our model according to these auc metrics can be seen in table 3 . the high auc indicates that our model can easily distinguish between the true response and negative responses . furthermore , the auc @ p numbers show that the model is able to distinguish between negative responses and positive responses .
4 shows rn @ k on the test set for different values of n and k when using a random table 4 shows that recall drops significantly as n grows , meaning that the r10 @ k evaluation performed by prior work may significantly overstate model performance in production settings .
results in table 5 show that the clustering and frequency whitelists perform comparably to each other when the true response is added . however , in the more realistic second case , when recall is only computed on examples with a response already in the whitelist , performance on this metric drops significantly . in the recall case , recall results are slightly higher than those in the random whitelist . recall that recall is strictly limited to responses that are contained within a whitelist ; recall is limited to instances that are contiguous with a whitelist , as this is the case in the recall set .
6 shows r @ 1 and coverage for the frequency and clustering whitelists . while recall @ 1 shows lower coverage , it does have higher recall and higher recall .
results of the human evaluation are in table 7 . our proposed system works well , selecting acceptable or great responses about 80 % of the time and selecting great responses more than 50 % . interestingly , the size and type of whitelist seem to have little effect on performance , indicating that the whitelists contain a variety of conversational and semantic information .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 6 ) . in fact , the cues are comparable in terms of bias metric , with the exception of token distance . token distance and topical entity are only weak improvements over random , further , cues are markedly gender - neutral , improving performance by 2 % in the standard task formulation and to parity in the gold - two - mention case . transformer - multi is stronger than transformers - single and tokendistance .
note particularly the large difference in performance between genders , both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , lee et al . ( 2017 ) and parallelism produce remarkably similar output : of the 2000 example pairs in the development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 7 ) . token distance and topical entity are only weak improvements over random , further , the cues are markedly gender - neutral , improving the bias metric by 2 % in the standard task formulation and to parity in the gold - two - mention case .
istent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific heads and that these heads are in the deep layers of the network ( e . g . l3h7 ) .
find that the instances of coreference that transformersingle can handle is substantially
investigate the effects of the multi - factor count ( m ) in our final model on the test datasets in table 3 . we observe that for the nyt10 dataset m = { 1 , 2 , 3 } gives good performance with m = 1 achieving the highest f1 score . on the nyt11 dataset , m = 4 gives the best performance . these experiments show that the number of factors giving the best performances may vary depending on the underlying dataset .
present the results of our final model on the relation extraction task on the two datasets in table 2 . our model outperforms the previous stateof - the - art models on both datasets in terms of f1 score . on the nyt10 dataset , it achieves 4 . 3 % higher f1 scores compared to the previous best state - of - the - art model ea . similarly , it performs 2 . 4 % higher on the nyt11 dataset compared to our previous best model pcnn .
shown in table 4 , when we add multi - factor attention to the baseline bilstm - cnn model without the dependency distance - based weight factor in the attention mechanism , we get 0 . 8 % f1 score improvement ( a1 − a2 ) . adding the dependency weight factor with the window size of 5 improves the model performance marginally ( a2 − a3 ) . replacing the attention normalizing function with softmax operation also improves the f1 scores marginally ( a3 − a4 ) . however , to match the full attention vector , we have to resubmit the attention vector to a maximum of 10 . in this easier setup , we can apply max - pooling operation across the multiple attention scores to compute the final attention scores . this helps the model to regain its recall scores . we can further reduce the number of attention scores by 1 . 5 %
flickr30k we also note entity - wise performance with the exception of " people " and " vehicles " in table 3 . as these models use object detectors pretrained on pascal - voc , they have somewhat higher performance on classes that are common to both flickr29k and pascal30k . however , on the classes like " clothing " , " bodyparts " , and " animals " our model zsgnet shows much better performance .
2 compares zsgnet with prior works on flickr30k entities and referit . we use " det " and " cls " to denote models using pascal voc detection weights and imagenet [ 10 , 41 ] classification weights . networks marked with " * " fine - tune their object detector pretrained on pascalvoc on the fixed entities of flickr31k . however , such information is not available in referit dataset which explains ∼ 5 % increase in performance of zsgnet over other methods .
shown in table 5 , zsgnet shows 4 − 8 % higher performance than qrg even though the latter has seen more data we observe that the accuracy obtained on flickr - split - 0 , 1 , 2 are higher than the vg - split - 1 , 3 and 4ub . however , the accuracy remains the same across all clusters indicating that the model performs well across the five clusters .
6 shows the performance of our model with different loss functions using the base model of referit in the ablation study . note that using softmax loss by itself places us higher than the previous methods . further using binary cross entropy loss and focal loss give a significant performance boost which is expected in a single shot framework . finally , image resizing gives another 4 % increase .
4 shows the results of fine - tuning on ted , no - reg and news datasets . the ewc task outperforms no - reg , l2 and l2 on news and ted , but is much more complicated . training on it data alone results in over - fitting , with a 17 . 5 bleu improvement over the baseline news model . ewc reduces forgetting on two previous tasks while further improving on the target domain .
es - en , the health and bio tasks overlap , but catastrophic forgetting still occurs under noreg ( table 3 ) . regularization reduces forgetting and allows further improvements on bio over noregon fine - tuning . we find ewc outperforms no - reg and l2 in both health and bio .
5 shows improvements on data without domain labelling using unadapted models trained only on one domain uniform ensembling under - performs all oracle models except es - en bio and health 35 . 9 % and 35 . 5 % over the baseline , respectively . identity - bi is particularly effective for data with multiple domains , with a gap of 10 . 5 co - workers ' bias test set and further data augmentation for each domain . bi and is both individually outperform the oracle for all but is - news , with adaptive decoding , we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain . we highlight this in table 5 by reporting results with the ensembles of tables 5 and 6 over concatenated test sets , to mimic the realistic scenario of unlabelled test data . we additionally include the uniform uniform ensemble in our final report ,
table 6 , we apply the best adaptive decoding scheme , bi + is , to models fine - tuned with ewc . ewc models perform well over multiple domains , so the improvement over uniform ensembling is less striking than for unadapted models . nevertheless adaptive decoding improves over both uniform and oracle models . we do not need to assume whether a uniform ensemble might perform better for a single domain .
no - reg ensembling outperforms unadapted models , since fine - tuning gives better in - domain performance . bi + is decoding with single - domain trained models gives a 0 . 8 / 3 . 4 bleu gain over the approach described in freitag and al - onaizan , and a 2 . 4 / 10 . 4 overall overall improvement over no - reg models . bi + is with ewc models improves over both the naive uniform approach and the uniform uniform approach .
results in table 8 show that when only using original utterances with ellipsis , precision is relatively high while recall is low .
can be seen in table 6 that empirically adding logits from two models after classifiers performs the best .
table 6 , the best performance on map metric is achieved by our approach , which verifies the effectiveness of our model . we also observe that our approach exceeds traditional neural models like cnn , lstm and ntnlstm by a noticeable margin .
table 2 shows the results for rcv1 and its variants compared to other widely used approaches . these results appear to indicate that our approach has superior generalization ability on datasets with fewer training examples than other approaches .
evident from table 1 , there is a significant imbalance in the distribution of training instances that are suggestions and non - suggestions , 2https : / / www . uservoice . com / for sub task a , the organizers shared a training and a validation dataset whose label distribution ( suggestion or a nonsuggestion ) is presented in table 1 .
3 shows the performances of all the models that we trained on the provided training dataset . the ulmfit model achieved the best results with a f1 - score of 0 . 861 on the training dataset and 0 . 701 f1 ( test ) on the test dataset .
4 shows the performance of the top 5 models for sub task a of semeval 2019 task 9 . our team ranked 10th out of 34 participants .
iii shows the wers on the simulated and real test sets when aas is trained with different training data . fsegan ( 29 . 6 % ) does not generalize well compared to aas ( 25 . 2 % ) in terms of wer . with the simulated dataset as the training data , aas shows severe overfitting since the size of training data is small . when aas was trained with simulated dataset , it achieves the best result ( 24 . 7 % ) on the real test set . however , when trained with real datasets , it does not reach the best results ( 24 of7 % ) as expected .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . similarly , acoustic supervision ( 15 . 6 % ) and multi - task learning ( 14 . 4 % ) show lower wer compared to no enhancement , because less noisy speech leads to higher dce .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . further , acoustic supervision ( 27 . 7 % ) and multi - task learning ( 26 . 1 % ) show lower wer compared to no enhancement ( 29 . 1 % ) . when acoustic supervision is used as a baseline , the dce and wer rates are less than minimizing dce .
cerning transfer learning experiments ( rq1 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . our approach shows a slight improvement over the best performance by 2 - 2 . 7 % on the f - measure metric , which shows the diminishing returns from mixing source and target labeled training data . further improving performance by high margins
evaluating the approaches laid out in section iv , we consider three real - world datasets described in table ii . originally , all the raw messages for these datasets were unlabeled , in that their urgency status was unknown . since the macedonia dataset only contains 205 messages , and is a small but information - dense dataset , we labeled all messages in macedonia as urgent or non - urgent ( hence , there is no urgent or nurgent message in macedonia per table ii ) . since macedonia is small and sparsely populated , we used unlabeled messages as the base for our experiments . table ii shows that nepal is roughly balanced , while kerala is imbalanced . we used 205 messages per dataset . we set a time range of 08 / 17 / 2018 - 08 / 22 / 2018 .
iv illustrate the results for rq1 on the nepal and kerala datasets . concerning transfer learning experiments ( rq1 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . our approach shows a slight improvement over the best performance by 2 - 2 . 7 % on the f - measure metric , which shows the diminishing returns from mixing source and target labeled training data . further improving performance by high margins
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning supervision urgency detection on nepal as source , and f - measure on macedonia as target . our approach shows a slight drop in performance from the previous state - of - the - art model , but still achieves a high f - score .
in low - supervision settings . concerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning supervision urgency detection on a single dataset ( kerala , kerala , macedonia ) . further improving performance by high margins
in low - supervision settings . concerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning supervision urgency detection on nepal as well as kerala as source . our approach shows a slight improvement on the f - measure metric , which shows the diminishing returns from mixing source and target labeled training data . further improving performance by high margins
4 shows the synchronic performance of our model in the settings when tn and rn are tested on the locations from the same year ( including peaceful ones ) . in this easier setup , we observed exactly the same trends ( table 4 ) . in both cases , the model performed better than the previous stateof - the - art .
replication experiment in table 2 , we replicate the experiments from ( kutuzov et al . , 2017 ) on both sets . it follows their evaluation scheme , where only the presence of the correct armed group name in the k nearest neighbours of the ˆi mattered , and only conflict areas were present in the yearly test sets . essentially , it measures the recall @ k , without penalizing the models for yielding incorrect answers , and never asking questions that were already contained in the test set ( e . g . , peaceful locations ) . the performance is very similar ,
3 shows the diachronic performance of our system in the setup when the matrix tn and the threshold rn are applied to the year n + 1 . for both gigaword and now datasets ( and the corresponding embeddings ) , using the cosine - based threshold decreases recall and increases precision ( differences are statistically significant with t - test , p < 0 . 001 ) . in both cases , the algorithm tn performs better than the threshold without sacrificing too many correct answers .
4 : the ablation study on the woz2 . 0 dataset with the joint goal accuracy on the test set . the effectiveness of our hierarchical attention design is proved by an accuracy drop of 1 . 69 % after removing residual connections and the hierarchical stack of our attention modules .
3 : the joint goal accuracy of our model on the woz2 . 0 test set and the multiwoz test set . we also include the inference time complexity ( itc ) for each model as a metric for scalability . for example , in the recent work on woz 2 . 0 dataset , we have managed to achieve an accuracy of 48 . 42 % , which is slightly higher than the previous state - of - the - art .
5 : the ablation study on the multiwoz dataset with the joint domain accuracy ( jd acc . ) and joint goal accuracy ( jds acc . ) on the test set . from table 5 , we can further calculate that given the correct slot prediction , comer has 87 . 42 % chance to make the correct value prediction . we can also calculate that the accuracy obtained by slot prediction with the correct domain prediction ( 95 . 52 % ) is 58 . 43 % and the jds acc . is 48 . 43 % . we also can calculate that once the slot prediction has been done , the accuracy remains the same .
4 presents the results of domain transfer . we use the three aspects of the beer review data together as our source tasks while use the four aspects of hotel review data as the target . our model ( ours ) shows marked performance improvement . the error reduction over the best baseline is 15 . 08 % .
3 summarizes the results of aspect transfer on the beer review dataset . our model ( ours ) obtains substantial gains in accuracy over the baselines across all three target aspects . it closely matches the performance of oracle with only 0 . 5 % absolute difference .
5 presents the results of an ablation study of our model in the setting of domain transfer . as this table indicates , both the language modeling objective and the wasserstein distance contribute similarly to the task . in both cases , the results are markedly better than those in the original study .
compare our proposed model against the baseline catseq - rl model in terms of f1 scores as explained in ( yuan et al . , 2018 ) . as can be seen , the results are broken down in table 1 . for example , in most cases our model performs slightly worse than the baseline on three out of the four datasets except for on krapivin where it obtains the best f1 @ m of 0 . 37 and 0 . 38 . on the other hand , for the four extractive datasets , the performance remains the same across all three datasets suggesting that gan models are more effective in generation of keyword answers .
also evaluated the models in terms of α - ndcg @ 5 ( clarke et al . , 2018 ) . the results are summarized in table 2 . our model obtains the best performance on three out of the four datasets . the difference is most prevalent in the kp20k dataset , where the gan model ( at 0 . 85 ) is nearly 5 % better than the other two baseline models .
simplify our dataset , we have decided to focus our work on job positions – which , we believe , are an interesting window into the nature of gender bias – and , consequently , were able to obtain a comprehensive list of professional occupations from the bureau of labor statistics ' detailed occupations table , from the united states department of labor . the values inside , however , had to be expanded since each line contained multiple occupations and sometimes very specific ones . fortunately , this table also provided a percentage of women participation in the jobs shown , for those that had more than 50 thousand workers .
we have found is that google translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender - neutral pronouns , in general . furthermore , this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes , such as life and physical sciences , architecture and engineering , computer science and mathematics , among others . we have found that these fields are particularly difficult to interpret , with the exception of computer science , engineering and mathematics .
results of experiment 1 are shown in table 2 . we observe substantial racial disparities in the performance of classifiers . for example , waseem and hovy ( 2016 ) classifies black - aligned tweets as hate speech at 1 . 15 times the rate of white aligned tweets , compared to the previous experiment , in which blackaligned tweets are classified as harassment at 0 . 1 and 1 . 254 times the rates of white - aligned ones . for davidson et al . ( 2017 ) classifiers , we see that black aligned tweets are the most frequently considered to be hate speech , but are slightly less frequently classified as containing offensive language . we see that for both groups , there is a significant racial disparity , with blackaligned tweet classified as sexism at 2 . 1 times their rate of those in the white aligned corpus . similarly , for the founta et al . , 2018 classifier we see a significant increase in the rates at which tweets are clas sified as racist , 1 . 3 times as frequently as in experiment 1 .
results of experiment 1 are shown in table 1 . we observe that in all but one of the comparisons we see that hate speech and harassment are extremely difficult to detect . in fact , we see varying performance across classifiers , with some performing much worse out - of - sample than others . for example , in the classifier r . & s . ( 2016 ) , there is a significant drop in precision between classifiers ( from precision 0 . 56 to 0 . 87 ) and f1 scores of 0 . 95 . in the other comparisons , we find that in both cases , the classifiers are much more likely to flag offensive or sexist speech as well as instances of hate speech . in particular , this group of tweets is particularly sensitive to the word " n * gga " and " b * tch " compared to those in classifier w . & h . , 2016 . these tweets are considered to be instances of sexism and harassment , but are not classified into a classifier .
results of experiment 1 are shown in table 1 . we observe substantial racial disparities in the performance of classifiers . for waseem and hovy ( 2016 ) we see that there are statistically significant ( p < 0 . 001 ) differences in the estimated rates at which black - aligned tweets are classified as hate speech . the classifier trained on the davidson et al . ( 2017 ) data is 1 . 7 times more likely to classify blackaligned tweets as harassment at a higher rate than in experiment 1 , although the disparity is narrower . we see that for both groups we see a substantial racial disparity . for the founta et al . , 2018 classifier , there is a 1 . 993 % increase in the rate at which tweets are predicted to belong to the blackaligned corpus compared to those in the white - aligned corpus . similarly , for the waseema et al . ( 2018 ) classifier we see an increase of 1 . 1 % in rates at the time of classifier compared to experiment 1 .
results of experiment 1 are shown in table 2 . we observe substantial racial disparities in the performance of classifiers . for example , waseem and hovy ( 2016 ) classifies black - aligned tweets as harassment at 1 . 5 times the rate of white aligned tweets , a higher rate than in experiment 1 . similarly , for the founta et al . ( 2017 ) classifier , we see that blackaligned tweets are 1 . 4 times as frequently classified as hate speech at 2 . 3 times the rates of those in the white - aligned corpus . for davidson et al . , we see a substantial racial disparity , with blackaligned and whitealigned tweets classified as harassment 1 . 1 times as often as white aligned ones . we see similar results in experiment 2 , where for black aligned tweets the classifiers trained on the tweets are more likely to classify them as offensive . in experiment 1 , we observe a racial disparity in the estimated rates at which tweets are clas sified as containing sexism and racism , 1 . 7 times as many as those in experiment 4 .
can be seen in table 1 , sparsemax and tvmax achieve better results overall when compared with softmax , indicating that the use of selective attention leads to better captions . selective attention mechanisms like sparsemax , especially tvmax , have higher precision on mscoco and flickr30k datasets , and the latter has higher correlation with the human judgement . moreover , when using selective attention , the results are slightly worse than sparsemax but still comparable to that of softmax .
performing slightly worse than sparsemax , tvmax outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation , reported in table 2 . the superior score on attention relevance shows that tvmax is better at selecting the relevant features and its output is more interpretable . additionally , the superior caption evaluation results demonstrate that the ability to select compact regions induces the generation of better captions . the results are reported in tables 2 and 3 . the results of the second study are presented in the supplementary material . table 2 : human evaluation results on mscoco .
can be seen in the results presented in table 3 the models using tvmax in the output attention layer outperform the model using softmax and sparsemax . moreover , the results are slightly superior when sparsemax is used in the self - attention layers of the decoder , and when using bounding box features , the accuracy is superior than that of softmax . in fact , when using sparsemax in the final attention layer , it is easier to achieve the highest accuracy than softmax , indicating that the model is more suitable for the task at hand . this corroborates our intuition that selecting the features of contiguous regions of the image leads to a better answering capability . we can also see that combining sparsemax and tvmax achieves the best results , as the sparsemax model outperforms the other models using the same number of features . furthermore , when selecting bounding boxes , sparsemax outperforms softmax because it is more compact . this validates our hypothesis that combining features of the two attention mechanisms improves the accuracy .
iv presents the system ' s performance on each error generation algorithm . we included only p @ 1 and p @ 10 to show trend on all languages . " random character " and " character bigrams " includes data for edit distance 1 and 2 whereas " characters swap " consists of data that are used in edit distance 2 . all the system data used in this analysis had one error according to our system .
considered four approaches — trie data structure , burkhard - keller tree ( bk tree ) , directed acyclic word graphs ( dawgs ) and symmetric delete algorithm ( sda ) 6 . in table i , we represent the performance of algorithms for edit distance 2 without adding results for bk trees because its performance was in range of couple of seconds .
best performances for each language is reported in table ii . we present precision @ k9 for k ∈ 1 , 3 , 5 , 10 and mean reciprocal rank ( mrr ) . we report p @ 1 , p @ 3 and p @ 10 to show trend on synthetic dataset . we also include p @ k10 for turkish , danish , dutch and russian . we included the best performance @ k1 , p @ 10 and mrr for dutch .
system is able to do each sub - step in real - time . all the sentences used for this analysis had exactly one error according to our system . detection time is the average time weighted over number of tokens , ranking time is weighted over length of suggestions generated , and ranking time , weighted over misspelling character length , and length of suggestion generated . we do not include the weighted average number of misspelled character length in our system , as this is a weighted over - fitting of sample sentences .
results in table vi show that our system outperforms both the systems in terms of p @ 1 and p @ 10 by a large margin .
shown in table vii , most of the words for each language were detected as known but still there was a minor percentage of words which were known to be known as errors .
, table 9 presents the results of models trained on tweets from one domain and tested on all tweets from other domains , with the exception of tweets from the other domains . we observe that predictive performance is relatively consistent across all domains with two exceptions ( ' food & beverage ' consistently shows lower performance , while ' other ' achieves higher performance ) when trained on all domains .
total , 1 , 232 tweets ( 62 . 4 % ) are complaints and 739 are not complaints ( 37 . 6 % ) . the statistics for each category is in table 3 .
unigrams and part - of - speech features specific of complaints and non - complaints are presented in table 4 . all correlations shown in these tables are statistically significant ( p < . 01 , r = 0 . 005 ) with respect to complaints and not complaints . the presence of words in unigram clusters , especially those referring to questions or errors , is one of the most distinctive features of complaint and not complaint . a large number of words are used to describe negative sentiment or emotions . complaints are not usually accompanied by exclamation marks . on the other hand , a large percentage of tweets are not complaints , indicating that the user is describing personal experiences . question marks and other words are distinctive of complaints , as many complaints are formulated as questions to the responsible party ( e . g . , why is this not working ? , when will this be corrected ? ) and when will my response be generated ?
top features for the liwc categories and word2vec topics are presented in table 5 . complaints tend to not contain personal pronouns ( he , she , him , you , shewc , male , female ) , as the focus on expressing the complaint is on the self and the party the complaint was addressed to and not other third parties . general topics typical of complaint tweets include requiring assistance or customer support . several groups of words are used to express complaints , including those about orders or deliveries ( in the retail domain ) , as well as parts of tech products ( in tech ) .
the best results are obtained with the volkova & bachrach model ( sentiment – v & b ) which achieves 60 f1 and 55 auc 0 . 654 . complaint specific features are predictive of complaints , however , predictive accuracy is low , reaching an f1 of less than 0 . 005 and a auc of 0 . 866 . the best results obtained by combining all the features is obtained using the best predictive accuracy . syntactic part - ofspeech features alone obtain higher predictive accuracy than any sentiment feature alone . however , the best performance obtained by using the feature - rich model is obtained with a f1 - of - 60 f1 score of more than 2 . 5 .
results in table 7 show that the domain adaptation approach further boosts f1 by 1 point to 79 ( t - test , p < 0 . 5 ) and roc auc by 0 . 012 . however , simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in f1 .
8 shows the model performance in macro - averaged f1 using the best performing feature set . results show that , in all but one case , adding out - of - domain data helps predictive performance . the apparel domain is qualitatively very different from the others as a large number of complaints are about returns or the company not stocking items , hence leading to different features being important for prediction . overall , predictive performance is high across all domains , with the exception of transport . domain adaptation is beneficial , lowering performance on a single domain compared to the others .
ert achieved a final accuracy of 91 . 20 % , a full 4 . 42 % improvement over the performance of ulmfit . gpt - 2 , on the other hand , finetuned to an accuracy of 96 . 28 % and a final performance of 0 . 2609 . rersults for this experiment are outlined in table 4 .
table 5 , it can be seen that generative pretraining via language modeling does account for a considerable amount of performance , constituting 44 . 32 % of the overall performance ( a boost of 42 . 67 % in accuracy ) in the multitasking setup and constituting 43 . 93 % in the standard finetuning setup .
shown in table 6 , reducing the number of attention heads severely decreases multitasking performance . using only one attention head , thereby attending to only one context position at once , degrades the performance to less than the performance of 10 heads using the standard finetuning scheme . this shows that more attention heads is important to boosting performance to state - of - the - art results .
6 shows the ablation study results on paragraph selection loss loss lpara and entity prediction loss lentity . as shown in the table , using paragraph selection can further improve the joint f1 by 0 . 31 points to 0 . 38 points ,
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distract from the empty set , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distract and full wiki , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
3 shows the performance of paragraph selection on the dev set of hotpotqa . in dfgn , paragraphs are selected based on a threshold to maintain high recall ( 98 . 28 % ) , leading to a low precision ( 60 . 53 % ) . compared to both threshold - based and pure topn - based paragraph selection , our two - step paragraph selection process is more accurate , achieving 94 . 53 % precision .
5 shows the results with different pre - trained language models on the dev set of distractor . using bert whole word masking improves the joint f1 score .
7 shows the results of hgn experiments for different reasoning types . compared to dfgn , our model achieves a lower precision .
4 shows the bleu scores of our dual2seq model taking gold or automatic amrs as inputs . the improvement from automatic amr to gold amr is significant , which shows that the translation quality of our model can be further improved with an increase of amr parsing accuracy .
3 shows the test bleu , ter and meteor scores of all systems trained on the smallscale news commentary v11 subset of the multi - news dataset . dual2seq is consistently worse than the other systems under all three metrics , seq2seq - linamr is less effective than both opennmt - tf and transformer - tf . the difference between the two systems is less pronounced under the small - scale setting of the nc - v11 subset than under the large - scale set of the other two . when trained on small - sample settings , our model performs slightly better than the others in both settings . we notice that the gap between the error rates under the largescale setting is larger than that under the full set of meteor , as shown in table 3 .
most representative models are elmo , gpt , bert and its variants , and xlnet . next , we give a brief overview of these models and summarize their performance on the selected benchmark tasks . table 2 quantitatively compares these models on various benchmarks . smaller tweaks to various aspects of the model have resulted in hundreds of entries on leaderboards ( e . g . , those linked to in section 4 . 3 . 3 and table 2 ) leading to a significant improvement in performance for all models .
2 shows that coreference propagation ( corefprop ) improves named entity recognition performance across all three domains . the largest gains are on the computer science research abstracts of scierc .
1 shows the results of models trained on the entity , relation and event extraction tasks . our framework establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except event argument identification . relative error reductions range from 0 . 2 - 27 . 9 % over previous state of the art models . we observe that , when trained on entity and relation extraction tasks , precision is relatively high , but error reduction is low .
3 shows that relation propagation ( relprop ) improves relation extraction performance over pretrained bert , but does not improve fine - tuned bert . it is less effective on scierc , and not comparable with finetune .
7 compares bert with the best model configurations . scibert significantly boosts performance for scientific datasets including scierc and genia .
6 shows that both variations of our bert model benefit from wider context windows . our model achieves the best performance with a 3sentence window across all relation and event extraction tasks .
table 1 we report the best and average precision @ 1 scores and the average number of iterations among 10 experiments , for different language translations . our model improves the results in the translation tasks . the average precision of our model is slightly better than the previous best model . the noise - aware model is more stable and therefore requires fewer iterations to converge . the accuracy improvements are small but consistent , and we note that we consider them as a lower - bound on the actual improvements as the current test set comes from the same distribution of the training set .
further analyze our findings with respect to baselines and existing discourse parsers . the first set of results in table 3 shows that the hierarchical right / left branching baselines dominate the completely right and left branching ones . however , their performance is still significantly worse than any discourse parser we have tried before . the second set shows the performance of both baselines when trained and tested on the same dataset ( intra - and interdomain ) . the rst - dt dataset is arguably the most sophisticated in terms of semantic parsing , with an average score of 82 . 88 % on the instructional and interdomain datasets .
4 shows the results of a second study . our agent dkrn outperforms all the agents with a large margin . the results of the second study are shown in table 4 . the agents trained on our cwc dataset are slightly less appealing than those trained on kernel . however , the difference is less pronounced for our agent .
3 shows the turn - level evaluation results . our approach dkrn outperforms all state - of - the - art methods in terms of all metrics on both datasets with two tasks .
5 shows the results of the second study . our dkrn agent outperforms all the other agents with a large margin .
6 shows the results of the second study . our agent outperforms the comparison agents with a large margin .
istic input a cnn augmented with self - attentionwe show encouraging relative improvements for future research in this direction . we managed to show small improvements with a resnet - 34 . though the improvement is slim , it is encouraging to continue researching into visual input in future research . it is clear from table 1 that there is a need to consider visual input .
istic output is shown in table 1 . we managed to show improvements with a resnet - 152 . though the improvement is slim , it is encouraging to continue researching into visual modulation
experimental results of all models are shown in table 1 . first , han models outperform all supervised and unsupervised baselines except mead . mead performs particularly well in terms of rouge and precision . it is clear from the results that there is a need to improve the precision scores for future work . we conjecture that there are not enough data to reproduce the results , but there is one exception : mead does not outperform mead in precision removal . we observe that the redundancy removal step is crucial for the han model to achieve outstanding results . it helps improve precision scores by 2 . 5 points in the final analysis . when redundancy removal was applied to mead , it produces only marginal improvement . this suggests that future work may need to consider more sophisticated redundancy removal strategies . we suspect that redundancy removal is a more efficient means of redundancy removal , but we do not see a significant difference in results .
3 : joint goal accuracy on dstc2 and woz 2 . 0 test set vs . various approaches reported in the literature . mrkˇsić et al . ( 2017 ) and rastogi ( 2018 ) show that the db model significantly outperforms the previous stateof - the - art models in both domains .
2 shows the joint goal accuracy on dstc2 and woz 2 . 0 of statenet psi using different pre - trained models based on different single slot . the fact that the food initialization has the best performance verifies our selection of the slot with the worst performance for pre - training .
5 shows the performance of our model on multi30k dataset in asymmetric mode . ame outperforms the fme model in terms of training data , confirming the importance of word embeddings adaptation .
show the results for english and german captions . we show that adapting the word embedding for the task at hand , boosts the general performance , since the model performs better in symmetric and asymmetric modes compared to monolingual modes . we achieve 10 . 66 % improvement on average compared to kiros et al . ( 2014 ) .
show the results for english and german captions . the results are 11 . 05 % better on average compared to ( gella et al . , 2017 ) in symmetric and asymmetric modes . ame also achieves competitive or better results than fme model in both languages , as shown in the table .
achieve 10 . 42 % improvement on average compared to kiros et al . ( 2014 ) in symmetric and asymmetric modes . we show that adapting the word embedding for the task at hand , boosts the general performance , since ame performs better in asymmetric mode than fme model . we also show that ame model can improve the results for english and german captions .
ame reaches 10 . 70 % and 3 . 66 % better results on average compared to monolingual model in symmetric and asymmetric modes , respectively , compared to symmetric modes .
4 shows the results for italian and german , compared to english , both for the original and the debiased embeddings . in both cases , the difference between the averages of the two sets with same gender is much lower . in italian , we get a reduction of 91 . 67 % of the gap with respect to english . in german , the reduction of 100 % is much larger .
2 shows the results for italian and german , compared to english . as expected , the average ranking of samegender pairs is significantly lower than that of different - gender pairs , both for german and italian , while the difference between the sets in italian is much smaller . table 2 shows that the difference in ranking between the original and the debiased embeddings is much larger for both languages . as we expect , in both languages , the same - gender ranking drops significantly for both sets .
6 shows the results for italian and german for both datasets , compared to the original embeddings . in both cases , the results are slightly better than the original ones .
results reported in table 7 show that precision on bdi indeed increases as a result of the reduced effect of grammatical gender on the embeddings for german and italian , i . e . that the embedding spaces can be aligned better with the debiased embedding space .
3 shows the ari and silhouette coefficients of the opinion distance measures for each opinion distance . we evaluate our distance measures in the unsupervised setting , specifically , evaluating the clustering quality of the baseline distance measures . we benchmark against the following baselines : wmd ( which relies on word2vec embeddings ) , doc2vec and tf - idf . the results are shown in table 3 . the average distance between the od - d2v and od - w2v baseline scores are statistically significant ( paired t - test ) with respect to baselines at significance level 0 . 005 . opinion distance measures generally perform better than the competition on both od and w2v datasets . however , their performance is slightly worse than those of the other baselines . this is mostly due to the small size of the data set . we observe that the distance measures vary depending on the underlying data distribution . for example , wmd has the worst clustering performance , with the average distance being close to zero .
opinions : we see that od significantly outperforms the baselines of tf - idf , sent2vec and doc2vec in fact , od achieves the best ari and silhouette coefficient scores of close to zero on the " video games " and " pornography " datasets ( barely providing a performance improvement over random clustering , i . e . , a zero ari score ) . from the " seanad abolition " dataset , we see that wmd performs particularly bad compared to wmd and wmd . we observe that the text - similarity based baselines such as wmd achieves the highest ari scores and the silhouettes coefficient of zero . 01 and 0 . 005 respectively compared to the other baselines . we also observe that , in the " vegas " datasets , the clustering performance of the od is slightly better than the other two baselines , but not close to the best performing ones .
completeness , here we also compare against unigram or n - gram based classifiers the classification performance of the baselines is reported in table 1 . svm with od and bigrams achieves the best performance on the " seanad abolition " dataset . on the " pornography " dataset , there are scores of 0 . 86 , 0 . 87 and 0 . 83 respectively compared to the scores of 1 . 05 , 2 . 05 and 0 . , 6 . 05 by od and 5 . 55 by lsa . sentiment is slightly better than lsa , but it is slightly worse than od and baseline . we observe that the classification performance is more of a result of the clustering of baselines than any other classifier .
ributhe results of different variants are shown in table 5 . we observe that compared to od - parse , od is much more accurate . on the three datasets , od achieves an average weighted f1 score of 0 . 54 , 0 . 56 and 0 . 41 respectively compared to 0 . 07 by the scores of - 0 . 01 by od . also , compared to " video games " and " pornography " data , od performs slightly better than jensen - shannon divergence .
results on the testing and development sets are shown in table 3 . the difference between accuracy and f score is minimal , however we see significant difference in macro - f score due to different class balance in these sets .
denying instances get misclassified as commenting ( see table 5 ) ,
1 shows the absa datasets from the restaurants domain for english , spanish , french , dutch , russian and turkish . from left to right each row displays the number of tokens , number of targets and the total number of multiword targets for each training and test set .
3 provides detailed results on the opinion target extraction ( ote ) task for english . we show in bold the best results obtained using only one type of clustering feature , namely , the best brown , clark and word2vec models . moreover , we also show the best performances obtained using the best combination of all the best clustering features .
6 shows that our system outperforms the best previous approaches across the five languages .
errors in our system are caused by false negatives [ fn ] , as can be seen in table 7 .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
greek analogy test set contains 39 , 174 questions divided into semantic and syntactic analogy questions . semantic questions are divided into 15 categories and include 13 , 650 questions in total . syntactic questions include 25 , 524 questions , of which 15 , 524 are in semantic category . they include 260 questions we show the full greek analogy dataset in table 1 .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
to pearson correlation , gr def model had the highest correlation with human ratings of similarity , followed by p - value of 3 . 8e - 33 .
epm " generalizes best , and in out - ofdomain evaluations , it considerably outperforms the ensemble model of e2e - coref .
pos and named entity tags have the least and the pairwise features have the most significant effect . however , the results are still significantly worse than those of " + pairwise " and " pairwise " .
performance of the " + epm " model compared to previous state - of - the - art models on the conll test set is presented in table 4 . epm feature - values result in significantly better performance than those of " g & l " . however , the performance gap is still considerably larger than that of " jim " .
observe that incorporating all the linguistic features bridges the gap between the performance of " top - pairs " and " ranking " . it also improves the conll score by 3 points .
observe that the impact on generalization is not notable , i . e . the conll score improves only by 0 . 5pp over " top - pairs " .
performance of the " + epm " model compared to previous state - of - the - art models on the conll test set is presented in table 4 . epm feature - values result in significantly better performance than those of jim while the number of epm features is considerably less than jim .
first analyze the coverage of the vsms in question with respect to the lexica at hand , see table 4 . for brevity we only report coverage on w2 contexts lemmatization allows more targets to exceed the sgns frequency threshold , which results in consistently better coverage . pos - disambiguation fragments the vocabulary , as shown in the table 4 . wn - n shows low coverage containing many low - frequency members .
1 shows the performance of the vsms in question . lemmatized targets generally perform better , with the boost being more pronounced for simverb and simlex verbs . adding pos information benefits the comparison performance , lemma - based targets show a considerable performance drop compared to the comparison of the two datasets . using dep - based vsms proves beneficial for both datasets we provide the morph - fitting scores ( muli ´ c et al . , 2017b ) as a state - of - the - art reference . morph - fitting scores are similar to those of jimm , but are slightly less effective , in that the comparison is more concentrated on the word " sim " compared to " verb " . morph - fitting consists of two stages : one stage brings word forms of the same word closer in the vsm , while the other stage sets the derivational antonyms further apart . syntactic repel ( mra ) uses word forms similar to nouns , but is more similar in terms of performance . lemma grouping is similar to the similarity of the mra stage , but requires more training time .
3 provides exact scores for reference . lemma disambiguation puts the type and type targets at higher significance for vn than for lemmatized targets . for reference , we use w2 contexts and w3 contexts . for window - based targets , we type - disambiguated targets significantly outperform the vn and wn - v contexts . note the low f scores for windowbased targets as the effect is less pronounced for lemma targets . note that the effect of pos on vn is not statistically significant ( p ≤ . 005 ) but it does not significantly impact the f scores of vn in lemma contexts .
embeddings derived from wiki - pubmed - pmc outperform glove - based models in the extraction of most relation types ( table 1 ) . the feed - forward ann models outperform models using boc as well as asm features , but do not exceed the upper boundary of boc feature , in which again , the performance gains are not statistically significant . the feedforward ann displays significant over - fitting across all relation types , with the exception of the relation types described in table 1 .
semantic cooccurrence baseline outperforms other approaches that allow boundary expansion . as the results of applying the co - occurrence test ( ρ = 0 ) shows , the semantic relations in this data are strongly concentrated within a sentence boundary , with an f1 of 1 . 0 . the machine learning approaches based on boc lexical features effectively complement the deficiency of cross - sentence relation extraction .
3 compares multi - news to duc data from 2003 and 2004 and tac 2011 data , which are typically used in multi - document settings . we choose to compare the mds datasets with those from 2003 , 2004 and 2004 . the total number of examples in multi - mds dataset is two orders of magnitude larger than previous mds news data . the number of words in the concatenated inputs is shorter than in previous works , but still comparable to the sds dataset in terms of number of inputs . the average number of input words is two . 88 , but larger than in other works . we also compare mds with duc datasets , as this is a larger dataset than mds data , and smaller than cnndm data , as shown in table 4 .
4 shows the percentage of n - grams in the gold summaries which do not appear in the input documents as a measure of how abstractive our summaries are in table 4 . as the table shows , the smaller mds datasets tend to be more abstractive , but multi - news is comparable to the abstractiveness of sds datasets . grusky et al . ( 2018 ) additionally define three measures of the extractive nature of a dataset , which we use here for a comparison .
model outperforms pg - mmr when trained and tested on the multi - news dataset . the transformer performs best in terms of r - 1 while hi - map outperforms it on r - 2 and r - su .
shown in table 5 , the prkgc + ns model suffers from predicting answer entities and generating correct nlds . this indicates that the challenge of rc - qede is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer .
evaluation results shown in table 2 show that the annotated nlds are of high quality ( reachability ) , and each nld is properly derived from supporting documents ( derivability ) . crowdworkers found that 45 . 3 % of 294 ( out of 900 ) 3 - step nld steps is missing from three of the 34 . 5 % reported in the report .
shown in table 4 , the prkgc models learned to reason over more than simple shortest paths . it also learned to find the shortest paths with the highest score , which indicates the use of rc - qede as supervision . however , when paths with more than 900 nlds are found to be difficult to find , path attentions are less thorough than those with fewer than 900 .
shown in table 7 , the prkgc models achieve a comparable performance to other sophisticated neural models .
results are presented in table 5 . the results of the ablation study seem to indicate that the lstm - 400 has a relatively high false positive rate compared to the other two models , apart of the flipped results of lstms - 800 and the handsdropout , the results are unacceptably bad for singlemodel models ( e . g . no - overlaps ) when ensembled . this is mostly due to the small size of the data set ( low some ablated models that perform poorly in the singlemodel scenario are able to regain a lot of accuracy .
system ' s official score was 60 . 9 % ( micro - f1 ) af therefore , we report both the official score ( from our second submission ) and the result of re - scoring our third submission after replacing these 10 files with the ones from our first submission .
, we report both the official score ( from our second submission ) and the result of re - scoring the second submission after replacing these 10 files with the ones from our first submission . the results are presented in tables 1 and 2 .
table 2 compares the quality of non - rl - based summarisation systems with those using icsi and priorsum . the results are summarized in table 2 . priorsum and tcsum significantly outperform the other rl - based systems in terms of sentence quality . however , the performance gap between the two is narrower than that of the other two systems .
2 compares the quality of our ^ ( cid : 27 ) u x with other widely used rewards for input - specific rl ( see x4 ) . for input - based rl , we compare against all other methods , confirming that our proposed l2r method yields a superior reward oracle . the x4r method has significantly higher correlation to the ground truth , with the difference being less pronounced for non - rl than for rl .
furthermore trained models on the polarity features of task b as mentioned before . results are summarized in table 6 . sif ( de - en ) significantly outperforms ( 0 . 7 % ) all the other models except for those using sif embeddings . in task b , sif also outperforms the baseline in terms of both f1 and f2 scores .
results of the models that trained on task a are shown in table 4 . the models trained on the stacked learner beat the baseline significantly even when using only plain averaged word embeddings .
task b , the models trained on the stacked learner beat the baseline significantly even when using only plain averaged word embeddings . adding the additional features improved the results for all models except for those using sif embeddings .
performed an ablation study on a single model having obtained 69 . 23 % accuracy on the validation set . results are summarized in table 2 . we can observe that the architectural choice that had the greatest impact on our model was the elmo layer , providing a 3 . 71 % boost in performance as compared to using glove pre - trained word embeddings . finally , we can see that emoji also contributed significantly to the model ' s performance as described in concat pooling ( hochreiter and schmidhuber , 1997 ) and word2vec ( keller et al . , 1997 ) achieving an overall 69 . 43 % performance improvement over the previous state of the art model . using the 50 - dimensional lstm hidden as the final layer of the model was beneficial , but did not improve performance as expected .
3 shows the corresponding classification report . in general , we confirm what klinger et al . ( 2018 ) report : anger was the most difficult class to predict , followed by surprise , whereas joy , fear , and disgust are the better performing ones .
4 shows the overall effect of hashtags and emoji on classification performance . tweets containing emoji seem to be easier for the model to classify than those without . hashtags also have a positive effect , however it is less significant .
5 shows the effect specific emoji have on classification performance . it is clear some emoji strongly contribute to improving prediction quality . the most interesting ones are mask , rage , and cry , which significantly increase accuracy . further , contrary to intuition , the sob emoji contributes less than cry , despite representing a stronger emotion . finally , not all emoji are beneficial for this task .
results on winograd and winocoref datasets are shown in table 7 . the best performing system is knowcomb . it improves by over 20 % over a state - of - art general coreference system on winongrad and also outperforms rahman and ng ( 2012 ) by a margin of 3 . 5 % . on the other hand , it improves by 15 % . these results show significant performance improvement by using predicate schemas knowledge on hard coreference problems . note that these systems are trained on the wino coref dataset . the results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features .
results on standard ace and ontonotes datasets are shown in table 8 . our knowcomb system achieves the same level of performance as does the state - of - art general coreference system we base it on . as hard coreference problems are rare in standard coreference datasets , we do not have significant performance improvement . however , these results show that our additional predicate schemas do not harm the predictions .
ailed analysis to study the coverage of our predicate schemas knowledge , we label the instances in winograd ( which also applies to winocoref ) with the type 1 / type 2 schema knowledge required . the distribution of the instances is shown in table 9 . the size and type of the coverage are shown in the table . the coverage is small but significant , with a gap of 2 . 5 % between the size and the number of instances . this is reflected in the type 1 and cat2 schema knowledge that are required for coverage . the type of coverage that is required is cat1 / cat2 and cat3 knowledge . all other instances are put into cat3 . this distribution shows that the size of the pool of instances can be further expanded with the expansion of the schema knowledge
also provide an ablation study on the winocoref dataset in table 10 . these results use the best performing knowcomb system . they show that both type 1 and type 2 schema knowledge havehigher precision on category 1 and category 2 datainstances , respectively , compared to the full set of knowledge in full data . type 1 , type 2 and type 3 knowledge have similiar performance on full data , but the results show that it is harder to solve instances in category 2 than those in category 1 . also , the performance drop between full data and full data indicates that there is a need to design more complicated knowledge schemas .
1 shows the numerical results obtained during the experiments for the four combinations tested . in general terms , the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre - trained black - box classification system to a new domain . table 1 : accuracy obtained by training an standalone classifier , applying the api and the proposed wrapper for each domain
