performance of these models compared to previous models on the hard and hard subsets is shown in table 4 . training data on b - copa shows that the reliance on superficial cues bridges the gap between the performance of both models when training on the original copa ( 50 % vs . 50 % ) and when matching the training size of the previous model ( 50 % ) .
studies show that bert and roberta achieve considerable improvements on copa ( see table 1 ) .
2 shows the five tokens with highest coverage . for example , a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21 . 2 % of copa training instances . its productivity of 57 . 5 % expresses that it appears in in correct alternatives 7 . 4 % more often than expected by random chance . this suggests that a model could rely on such high coverage to predict answers .
human evaluation shows that our mirrored instances are comparable in difficulty to the original ones ( see table 3 ) .
3 compares bert and roberta with previous models on the easy and hard subsets . as table 3 shows , previous models perform similarly on both subsets , with the exception of sasaki et al . ( 2017 ) . however , bert ' s improvements over previous models are only statistically significant on the hard subset , as these models perform on a larger corpus . this indicates that bert relies on superficial cues to improve performance on the low - supervision subset . this suggests that previous models may have underestimated the impact of superficial cues on performance .
relatively high accuracies of bert - large and roberta - large show that these pretrained models are already well - equipped to perform this task " out - of - the - box " . moreover , these models perform comparably to the pre - trained models on both the easy and hard subsets .
observe that bert trained on balanced copa is less sensitive to a few highly productive low - frequency cues . specifically , it is trained on cues that are less productive for the user . these cues are shown in table 7 . however , for cues with lower productivity , these cues are less effective .
are summarized in table 2 . the first set of summaries from our proposed network features is summarized in terms of prefixing . the proposed cnn - lstmour - neg - ant improves upon the simple cnn - w / o neg . baseline with positive sentiment and f1 scores improving from 0 . 72 to 0 . 78 for positive sentiment . from the second set , we further refine our model with prefixing for additional features .
results in table 7 show that the method is comparable to stateof - the - art bilstm from ( fancellu et al . , 2016 ) on gold negation cues for scope prediction . report the f - score for both in - scope and out - ofscope tokens .
statistics are shown in table 3 . the average number of tokens per tweet is 22 . 3 , per sentence is 13 . 6 and average scope length is 2 . 9 .
the f - score of false negation from a 0 . 61 baseline to 0 . 68 on a test set containing 47 false and 557 actual negation cues .
results are shown in table 5 . we report the average value of diversity and appropriateness , and the percentage of invalid responses . with data augmentation , our model obtains a significant improvement in diversity score and achieves the best average appropriatz score as well . however , the slightly increased invalid response percentage we also observe that our damd model outperforms hdsa in both diversity and invalid response scores . finally , we also observe the average percentage of responses that are acceptable for both datasets .
results are shown in table 1 . after applying our data augmentation , our proposed domain - aware multi - decoder network ( amd ) outperforms both the action and slot diversity , hdsa has the worse performance , as shown in the second group of results , the difference between the action / slot diversity is minimal , however , we do see a slight improvement from the above table , we compare the performance of the proposed domain augmentation network across the five action and slot diversity .
are shown in table 2 . the first group shows that after applying our domain - adaptive delexcalization and domain - aware belief span modeling , the task completion ability of seq2seq models becomes better . as shown in the second group , once the system action forms are taken into account , the model performs better than the other two models . with respect to inform and success rates , we can further see that if a model has access to ground truth system action , then it should be able to regain its task success .
3 shows the impact of coverage for improving generalization across these two datasets . the models are evaluated using exact match ( em ) and f1 measures , as the results show , incorporating coverage improves the model ' s performance in the in - domain evaluation as well as the out - of - domain evaluations in qasrl . with the help of additional coverage , the model improves its performance in both the evaluation and the evaluation results in the qa - srl .
3 shows the performance for both systems for in - domain ( the multinli development set ) as well as out - of - domain evaluations on snli , glockner , and sick datasets . coverage information further improves the generalization results on both datasets . the resulting cross - dataset improvements are larger than those on the sick dataset .
4 shows that gdpl has the smallest kl - divergence to the human on the number of dialog turns over the baselines , where πturns denotes the discrete distribution of gdpl , and pturns denote the fraction of kl turns for the real human .
performance of each approach that interacts with the agenda - based user simulator is shown in table 4 . gdpl achieves extremely high performance on account of the substantial improvement in inform f1 and match rate over the baselines . surprisingly , gdpl even outperforms human in completing the task , and its average dialog turns are close to those of humans , though gdpl is inferior in terms of match rate . acer and ppo obtain high performance as well , though aldm obtains a slight improvement in dialog turns , its performance is still inferior than gdpl - sess , it is perceptible that gdpl has better performance on the task success and is comparable to humans on the average dialog turns , gdpl also outperforms gdpl on the account of its lower dialog turns and its higher rate of dialog turns . though gdpl performs slightly better than ppo , its task success is still comparable to that of humans on average . ablation tests are performed using ppo and aldm - discr
agent the performance of different agents on the neural user simulator is shown in table 5 . all the methods cause a significant drop in performance when interacting with vhus . aldm even gets worse performance than acer and ppo . in comparison , gdpl is still comparable with acer , ppo and aldm , obtains better match rate , and even achieves higher task success .
6 presents the results of human evaluation . gdpl outperforms three baselines significantly in all aspects ( sign test , p - value < 0 . 01 ) except for the quality compared with acer . among all the baselines , gdpl obtains the most preference against ppo .
7 provides a quantitative evaluation on the learned rewards by showing the distribution of the return r = t γtrt according to each metric . it can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric , and negative otherwise .
present precision scores for the word analogy tests in table vii . we compare our proposed method with the original embeddings from word2vec . the results are presented in tables vii and viii . our proposed method outperforms the original ones in both semantic and syntactic analogy tests . in particular , it has the best performance in the semantic analogy test set .
replication study results are presented in table v . our proposed method improves the interpretability by increasing the average true answer percentage from ∼ 28 % for baseline to ∼ 71 % for our method .
correlation scores for 13 different similarity test sets and their averages are reported in table vi . we observe that , let alone a reduction in performance , the scores obtained by spine are significantly better than those obtained by word2vec . these test results are summarized in table vii . they are statistically significant on almost all of the test sets , except for those that are trained on the original embeddings ( e . g . , word2sense , spine ) . these scores are obtained using the best performing algorithm available on the test set ( oiwe - ipg ) .
results in table viii show that for the semantic analogy test , we present results for the cases where i ) all questions in the dataset are considered and ii ) only the questions that contains at least one concept word are considered . we observe that for all three scenarios , our proposed algorithm results in an improvement in precision scores . however , the greatest performance is obtained when we consider the number of instances in which the concept word embeddings are considered , and iii ) the percentage of instances that consist entirely of concept words is considered .
accuracies are presented in table ix . the proposed method outperforms the original embeddings and performs on par with our proposed method . pretrained word2sense outperforms our method , however it has the advantage of training on a larger corpus . this result along with the intrinsic evaluations show that the proposed imparting method can significantly improve interpretability without a drop in performance .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results on event coreference . our joint model outperforms all the base lines with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results of cluster + kcp again indicate that pre - clustering of documents to topics is beneficial , improving upon the kcp performance by 9 . 5 points , though still performing substantially worse than our joint model . to test the contribution of joint modeling , we compare our joint models to their disjoint counterparts . we observe that the joint model performs better on both event and entity coreference tests . we observe significant performance gap between joint model performance ( 71 . 5 % vs . 69 . 5 % ) on both events , indicating that joint modeling is beneficial .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results on event coreference . our joint model outperforms all the base lines with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results of cluster + kcp again indicate that pre - clustering of documents to topics is beneficial , improving upon the kcp performance by 3 . 5 points , though still performing substantially worse than our joint model . our model achieves state - of - the - art results , outperforming all the baselines except for the one that had the largest impact on the joint model performance . we observe that our model achieves the best results with an f1 score of 43 . 5 . on the other hand , our model performs slightly worse than its joint model due to the high overlap of documents , to test our model ' s predictive performance , we compare it to other methods of leveraging word clusters .
also show the precision numbers for some particular recalls as well as the auc in table 1 , where our model generally leads to better precision .
show the precision numbers for some particular recalls as well as the auc in table 2 , where pcnn + att ( 1 ) refers to train sentences with two entities and one relation label , pcnn - att ( m ) . the results are summarized in tables 2 . our model exhibits the best performances .
experimental results on wikidata dataset are summarized in table 3 . the results of " - word - att " row refers to the results without word - level attention .
the table 4 depicts depicts the training time of d = 0 . 1 and d = 16 .
4 shows the comparison of 1 - 5 iterations . we find that the performance reach the best when iteration is set to 3 .
3 presents the rouge scores of our system ( neuraltd + learnedrewards ) and multiple stateof - the - art systems . the summaries generated by our systems are consistently better than those by other systems , such as google translate and word2vec ( 2018 ) .
table 1 , we find that all metrics we consider have low correlation with the human judgement . more importantly , their g - pre and g - rec scores are all below . 50 , which means that more than half of the good summaries identified by the metrics are actually not good , and more than 50 %
3 shows the quality of different reward learning models . as a baseline , we also consider the feature - rich reward learning method proposed by peyrard and gurevych ( see § 2 ) . mlp with bert as en ( 2018 ) coder has the best overall performance . specifically , bert + mlp + pref significantly outperforms ( p < 0 . 05 ) all the other models that do not use bert - mlp .
4 presents the human evaluation results . summaries generated by neuraltd receives significantly higher human evaluation scores than those by refresh ( p = 0 . 0088 , double - tailed ttest ) and extabsrl ( p ( cid : 28 ) 0 . 01 ) . also , the average human rating for refresh is significantly higher ( p cid : 27 ) than that of refresh ,
5 compares the rouge scores of using different rewards to train the extractor in extabsrl ( the abstractor is pre - trained , and is applied to rephrase the extracted sentences ) . it is clear from table 5 that using the learned reward helps the rl - based system generate summaries with significantly higher human ratings . the summaries generated using our learned reward also have higher human rating .
results are shown in table 9 . as table 9 shows , the training set size and the number of negative responses for each positive response are the most important factors in model performance . the model performs significantly worse when trained with hinge loss instead of cross - entropy loss , indicating the importance of the loss function . finally , we see that a 2 layer lstm performs similarly to a 4 layer sru with the same number of parameters , but with the exception of the hinge loss . table 9 summarizes the results of an ablation study we performed to identify the most interesting components of our model architecture and the most difficult ones to detect .
performance of our model using an sru encoder and an lstm encoder is shown in table 8 . retrieving the context from a context takes a negligible amount of time compared to encodering an encoder with a similar number of parameters . sru also exhibits a relatively fast inference speedup compared to using an unlabelled context such as a text encoder ( e . g . , a single context encoder ) , since the sru is more than 4x faster at inference time , it is clear from table 8 that using a dual encoder architecture benefits from a larger corpus of data , and is comparable to using a single encoder encoder . table 8 also highlights the scalability of using a multiheaded attention layer ( i . e . , a high - level attention layer ) to encode a context . as expected , the speedup of inference time between selecting the context and the time to encode the context is small but significant we notice that using only one encoder during inference time is a significant part of the model ' s performance , i . e . the inference time increases with the number of context instances ,
performance of our model according to these auc metrics can be seen in table 3 . the model easily distinguish between the true response and negative responses . furthermore , the auc @ p numbers show that our model has a high true positive rate even under the difficult requirement of a low false positive rate .
4 shows rn @ k on the test set for different values of n and k when using a random table 4 shows that recall drops significantly as n grows , meaning that the recall drops substantially as the training set grows .
results in table 5 show that the three types of whitelists perform comparably to each other when the true response is added . however , in the more realistic second case , when the response is only computed on examples with a response already in the whitelist , the performance drops significantly . we notice that the recall results in the second case are slightly worse than those in the frequency and clustering datasets .
6 shows r @ 1 and coverage for the frequency and clustering whitelists . while the clustering dataset has higher recall , it has higher coverage .
results are in table 7 . our proposed system works well , selecting acceptable and great responses about 80 % of the time . interestingly , the size and type of whitelist seem to have little effect on performance , indicating that all the whitelists contain responses appropriate to a variety of conversational contexts . however , the percentage of responses that are good or great is less than 50 % .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . token distance and topical entity produce remarkably similar output : of the 2000 example pairs in the development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples . further , the cues are markedly gender - neutral , improving the bias metric by 9 % in the standard task formulation and to parity in the gold - two - mention case .
note particularly the large difference in performance between genders , both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , lee et al . ( 2017 ) and parallelism produce remarkably similar output : of the 2000 example pairs in the development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . token distance and topical entity produce remarkably similar output : of the 2000 example pairs in the development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples . further , the cues are markedly gender - neutral , improving the bias metric by 9 % in the standard task formulation .
istent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific heads and that these heads are in the deep layers of the network ( e . g . l3h7 ) .
find that the instances of coreference that transformersingle can handle is substantially
investigate the effects of the multi - factor count ( m ) in our final model on the test datasets in table 3 . we observe that for the nyt10 dataset , m = { 1 , 2 , 3 } gives good performance with m = 1 giving good performance . for the nyt11 dataset , we observe m = 4 giving good performances . on the other hand , on the nyt12 dataset , m = 4 gives bad performance .
present the results of our final model on the relation extraction task on the two datasets in table 3 . our model outperforms the previous stateof - the - art models on both datasets in terms of f1 score . on the nyt10 dataset , it achieves 3 . 8 % higher f1 scores compared to the previous best stateofthe - the – art models ea and pcnn . similarly , it improves the precision scores on the nyt11 dataset by 3 . 6 % compared to previous best models . we also observe that our model improves the recall scores of both the datasets in the final model as well .
investigate the effects of the multi - factor attention ( a1 − a2 ) on the bilstm - cnn model performance in prec . ( a2 ) . adding the dependency weight factor with the window size of 5 improves the results , however , it does not improve the f1 score significantly . replacing the attention normalizing with softmax operation also hurts the model performance marginally . we observe that the max - pooled attention used by the a2 model performs similarly to the softmax approach used by a3 .
flickr30k we also compare against [ 7 , 41 ] models using pascal voc ( " animals " , " people " and " vehicles " ) . as these models use object detectors pretrained on pascalvoc , they have somewhat higher performance on classes like classes like " clothing " , " bodyparts " , and " animals " . however , on the class of " vehicle " our model zsgnet shows much higher performance .
2 compares zsgnet with prior works on flickr30k entities and referit . we use " det " and " cls " to denote models using pascal voc detection weights and imagenet [ 10 , 41 ] classification weights . networks marked with " * " fine - tune their object detector pretrained on pascalvoc to obtain a better interpretability score . however , such information is not available in referit dataset which explains ∼ 9 % increase in performance over previous works .
4 shows the performance of our zsgnet model compared to qrg on the four unseen splits across the four splits , we observe that , let alone a reduction in performance , the accuracy is higher than qrg even though the latter has seen more data we observe that the accuracy obtained on flickr - split - 0 , 7 are higher than those obtained on the other two splits though the accuracy remains the same , we see that our model is comparable across the balanced and unbalanced sets
show the performance of our model with different loss functions using the base model of zsgnet on the validation set of referit in table 6 . note that using softmax loss by itself places us higher than the previous methods . further using binary cross entropy loss and focal loss give a significant performance boost which is expected in a single shot framework . finally , image resizing gives another 4 % increase .
4 shows the performance of all models on ted . fine - tuning rapidly forgets previous tasks . next , we give a brief overview of all the models on the target domain . next , we give brief summaries on it and no - reg topics . the it task is very similar : training on it data alone results in over - fitting , with different features contributing differently to the model ' s performance . regularization reduces forgetting on two of the three tasks . further , fine - tune on it improves performance on the four tasks .
es - en , the health and bio tasks overlap , but catastrophic forgetting still occurs under noreg ( table 4 ) . regularization reduces forgetting and allows further improvements on bio over noregon , l2 and ewc .
table 6 compares our approach with unadapted models trained only on one domain uniform ensembling under - performs all oracle models except es - en bio , especially on it . identity - bi strongly improves over both uniform ensembled models and is - news , with adaptive decoding , we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain . we highlight this in table 6 by reporting results with the ensembles of tables 6 and 7 over concatenated test sets .
table 6 shows improvements over unadapted models over ensembling and fine - tuned models for all domains except es - en bio . uniform models perform well over multiple domains , so we do not need to assume whether or not a model might perform better for some potentially unknown domain . bi + is with ewc models improves accuracy over both ensembled and under - of - domain models .
no - reg ensembling outperforms unadapted or unlabeled models bi + is decoding with single - domain trained models trained only on one domain with adaptive decoding , we achieve over - fitting across all three domains , with a bleu gain of 0 . 9 / 0 . 18 over the best previous state - of - the - art model . ewc - adapted models perform well over both singledomain and multidomain models ,
results in table 8 show that when only using original utterances with ellipsis , precision is relatively high while recall is low .
can see from table 4 that empirically adding logits from two models after classifiers performs the best .
table 6 , we can see that our approach exceeds traditional neural models like cnn , lstm and ntnlstm by a noticeable margin . on the other hand , our approach outperforms all the other models on map metric , which shows the effectiveness of our approach .
table 6 shows that our capsule - based approach outperforms the strong baselines on three of the four datasets we base our model on .
evident from table 1 , there is a significant imbalance in the distribution of training instances that are suggestions and non - suggestions , these instances are presented in table 1 .
3 shows the performances of all the models that we trained on the provided training dataset . the ulmfifit model achieved the best results with a f1 - score of 0 . 861 on the training dataset and a tfidf vectorizer - svm - f1 - test on the test dataset .
4 shows the performance of the top 5 models for sub task a of semeval 2019 task 9 . our team ranked 10th out of 34 participants .
iii shows the wers on the simulated and real test sets when aas is trained with different training data . with the simulated dataset as the training data , fsegan ( 24 . 7 % ) shows lower wer than aas ( 25 . 2 % ) on the real test set . when aas was trained with simulated , real and real datasets , aas shows severe overfitting since the size of training data is small . in the real - time setting aas achieves the best result ( 26 . 1 % ) on both sets .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer ( e . g . , > 90 % ) , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . the same tendency is observed for acoustic supervision ( 15 . 6 % ) and multi - task learning ( 14 . 4 % )
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer ( e . g . , > 90 % ) , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . the same tendency is observed for acoustic supervision ( 27 . 7 % ) and multi - task learning ( 26 . 1 % )
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . further improving performance by leveraging source domain features , such as the winocoref embeddings ( upadhyay et al . , 2018 ) on the training data for example . manual features reduce recall , but help the system to improve accuracy and precision ( sometimes considerably ) . finally , manual features reduce performance , improving recall and accuracy ( sometimes significantly ) .
evaluating the approaches laid out in section iv , we consider three real - world datasets described in table ii . originally , all raw messages for the experiments were unlabeled , in that their urgency status was unknown . since this is a small but information - dense dataset , we labeled all messages in macedonia as urgent or non - urgent ( hence , there is not a significant difference in the urgency status ) . since macedonia is a relatively small country , the size of the raw messages is small , but we consider it a significant resource for our experiments . table ii shows that nepal is roughly balanced , while kerala is imbalanced .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . further improving performance by leveraging source domain features , especially for high - supervision settings , can further improve performance . manual features reduce recall , but help improve performance ,
in low - supervision settings . concerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning 0the best f - measure achieved on nepal in table v was more than 69 % , but when using macedonia as source , it was only 62 . 5 % compared to 69 . 5 % ) on bangladesh .
in low - supervision settings . concerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning 0the best f - measure achieved on kerala in table vi was more than 69 % , which shows the diminishing returns from mixing source and target training data .
in low - supervision settings . concerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning 0the best f - measure achieved on nepal in table vii was more than 69 % , but when using kerala as source , it was only 69 . 5 % ∗ 78 . 5 % .
4 shows the synchronic performance of our system in the setup when tn and rn are tested on the locations from the same year ( including peaceful ones ) . in this easier setup , we observe exactly the same trends ( table 4 ) . at the same time , the integral metrics ( f1 ) consistently improves ( t - test , p < 0 . 01 ) .
replication experiment in table 2 , we replicate the experiments from ( kutuzov et al . , 2017 ) on both sets . it follows their evaluation scheme , where only the presence of the correct armed group name in the k nearest neighbours of the ˆi mattered , and only conflict areas were present in the yearly test sets . essentially , it measures the recall @ k , without penalizing the models for yielding incorrect answers along with the correct ones , and never penalizing those that did not belong to the test set .
3 shows the diachronic performance of our system in the setup when tn and rn are tested on the locations from the same year ( including peaceful ones ) . in both cases , the algorithm performs better than the previous stateof - the - art models .
4 : the ablation study on the woz2 . 0 dataset with the joint goal accuracy on the test set . the effectiveness of our hierarchical attention design is proved by an accuracy drop of 1 . 69 % after removing residual connections and the hierarchical stack of our attention modules .
3 : the joint goal accuracy of the dst models on the woz2 . 0 test set and the multiwoz test set . we also include the inference time complexity ( itc ) for each model as a metric for scalability table 3 compares our model with the previous state - of - the - art models on both datasets . for example , our model achieves an overall accuracy of 96 . 83 % on woz 2 . 0 and 94 . 53 % on the multi - woz dataset , with a marginal drop of 0 . 3 % compared with previous work . considering the fact that woz is a relatively small dataset , this small difference does not represent a significant big performance drop . on the muli - domain dataset , we achieve an overall performance improvement of 48 . 53 % , which marginally outperforms the previous work by a margin of 2 . 48 % . on the n - domain datasets , we maintain an overall improvement of 39 . 53 % .
5 : the ablation study on the multiwoz dataset with the joint domain accuracy ( jd acc . ) and joint goal accuracy ( jds acc . ) on the test set . from table 5 , we can further calculate that given the correct slot prediction , comer has 87 . 42 % chance to make the correct value prediction . while comer had done great job on domain prediction ( 95 . 42 % ) and value prediction ( 87 . 42 % ) , the accuracy of the slot prediction was only 58 . 42 % . we can also see that the jds acc . has a significant impact on the model ' s prediction performance .
4 presents the results of domain transfer using 200 training examples . we use the three aspects of the beer review data together as our source tasks while use the four aspects of hotel review data as our target . our model ( ours ) shows marked performance improvement . the error reduction over the best baseline is 15 . 08 % on average . using the three aspect of the data augmentation , we obtain the best results .
3 presents the results of aspect transfer on the beer review dataset . our model ( ours ) obtains substantial gains in accuracy over the baselines across all three target aspects . it closely matches the performance of oracle with only 0 . 40 % absolute difference . specifically , all rationale - augmented methods ( ra - svm , ra - trans and ours ) outperform their rationale - free counterparts on average . this confirms the value of human rationales in the low - resource settings . we observe that the transfer baseline that directly uses rationale as augmented supervision ( rra ) underperforms ours by a large margin .
5 presents the results of an ablation study of our model in the setting of domain transfer . as this table indicates , both the language modeling objective and the wasserstein distance contribute similarly to the task . for example , both ours and l [ italic ] distance have the best performance on the task , while wasserbeck distance has the worst performance .
compare our proposed model against two baseline models - catseq ( yuan et al . , 2018 ) and rl - rl ( chan et al . 2018 ) . the results are summarized in table 5 . for the two datasets , we use inspec [ yuan and ananiadou , 2018 ] which we base our model on . on the other hand , for the three datasets , our model performs slightly better than the other two baselines on three of the four datasets .
also evaluated the models in terms of α - ndcg @ 5 ( clarke et al . , 2018 ) . the results are summarized in table 2 . our model obtains the best performance on three out of the four datasets . the difference is most prevalent in the kp20k dataset , where our gan model ( at 0 . 85 ) is nearly 5 % better than the other baseline models .
simplify our dataset , we have decided to focus our work on job positions – which , we believe , are an interesting window into the nature of gender bias – and were able to obtain a comprehensive list of professional occupations from the bureau of labor statistics ' detailed occupations table , from the united states department of labor . the values inside , however , had to be expanded since each line contained multiple occupations and sometimes very specific ones . fortunately , this table also provided a percentage of women participation in the jobs described as " hostess occupations " for those that had more than 50 thousand workers . we filtered some of these because they were too generic ( " computer occupations , all other " , and others ) or because they had gender specific words for the profession ( " hostess " , " waiter / waitress " ) . we also filtered some because it was easier for women to interpret .
table 6 , we have found that google translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender - neutral pronouns , in general . furthermore , this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes , such as life and physical sciences , architecture , engineering , computer science and mathematics . table 6 summarizes these data ,
results of experiment 1 are shown in table 1 . we observe substantial racial disparities in the performance of all classifiers . for davidson et al . ( 2017 ) and waseem ( 2016 ) we see that black - aligned tweets are particularly sensitive to the word " b * tch " and are frequently classified as hate speech . for the founta et al . , ( 2018 ) classifier , we see a substantial racial disparity , with blackaligned tweets classified as harassment at 1 . 1 times the rate of white aligned tweets , a higher rate than in experiment 1 . however , this disparity is narrower than for golbeck et al . ( 2017 ) because of the higher correlation between blackaligned and whitealigned tweets , and because the classifier is trained on wasesem and hovy ( 2016 ) .
performance of these models on the 20 % held - out validation data is reported in table 4 . overall we see varying performance across the classifiers , with some performing much better out - of - sample than others . in particular , we see that hate speech and harassment are particularly difficult to detect . since we are primarily interested in within classifier , between corpora performance , any variation between classifiers should not impact our results . we see that for all the tweets that are considered as hate speech , our results are slightly superior .
results of experiment 1 are shown in table 1 . we observe substantial racial disparities in the performance of all classifiers . for davidson et al . ( 2017 ) and waseem ( 2016 ) both see that black - aligned tweets are particularly sensitive to the word " b * tch " and are frequently labeled as hate speech . for the founta et al . , 2018 ) classifier , we see that tweets in the blackaligned corpus are classified as harassment at a higher rate for both groups than those in the whitealigned corpus , although the disparity is less pronounced for waseeme and hovy ( 2016 ) . in the wasedem ( and hovy ) classifiers , the rates are reported as 0 . 005 and 0 . 012 respectively compared to the previous stateof - the - art experiment , which labels blackaligned tweets as abusive . we see similar results for golbeck and schmidhuber ( 2018 ) with regards to the golbeck et al . ( 2018 ) experiment , where blackaligned tweet is classified as a hate speech class at 1 . 7 times the rate of whitealigned tweets , which is slightly higher than the previous experiment , although still comparable .
results of experiment 1 are shown in table 1 . we observe substantial racial disparities in the performance of all classifiers . for davidson et al . ( 2017 ) and waseem ( 2016 ) we see that black - aligned tweets are particularly sensitive to the word " b * tch " and are frequently labeled as hate speech . for the founta et al . , 2018 ) classifier , we see a substantial racial disparity , with blackaligned tweets classified as harassment at a higher rate for both groups than in experiment 1 . however , for waseeme and hovy ( 2016 ) , their classifier is 1 . 479 times more likely to classify them as abusive , compared to the previous state - of - the - art model .
can be seen in table 31 , sparsemax and tvmax achieve better results overall when compared with softmax , indicating that the use of selective attention leads to better captions . moreover , for tvmax , automatic metrics results are slightly worse than sparsemax but still superior to softmax on mscoco and similar on flickr30k . selective attention mechanisms like sparsemax , especially when compared to tvmax is used in combination with selective attention .
performing slightly worse than sparsemax under automatic metrics , tvmax outperforms softmax and softmax in the caption human evaluation and the attention relevance human evaluation , reported in table 2 . the superior score on attention relevance shows that tvmax is better at selecting the relevant features and its output is more interpretable . additionally , the better caption evaluation results demonstrate that the ability to select compact regions induces the generation of better captions . with these scores , we computed the mean of the captions evaluation scores and the mean computed by tvmax . the results are reported in tables 2 .
can be seen in table 3 the models using sparsemax and tvmax achieve similar results in the output attention . however , for tvmax , the results are slightly worse than when using softmax , indicating that the features captured by the sparsemax transformation are inferior to those captured by softmax . moreover , when using bounding box features , sparsemax outperforms softmax in both instances . we can also see that combining the features of contiguous regions of the image leads to a better overall performance . additionally , we can see that using tvmax in the final attention layer improves the accuracy . this corroborates our intuition that selecting only the bounding boxes of the relevant objects may help the model to better interpret the image . thus , having sparse attention mechanisms , the model can achieve a superior performance overall when using the features obtained by softmax . moreover it can also be seen that combining contiguous regions with contiguous features further improves the results . finally , it can be observed that combining features obtained using the tvmax transformation improves accuracy .
iv presents the system ' s performance on each error generation algorithm . we included only p @ 1 and p @ 10 to show trend on all languages . " random character " and " character bigrams " includes data for edit distance 1 and 2 whereas " characters swap " consists of data from edit distance 2 .
considered four approaches — trie data structure , burkhard - keller tree ( bk tree ) , directed acyclic word graphs ( dawgs ) and symmetric delete algorithm ( sda ) 6 . in table i , we represent the performance of algorithms for edit distance 2 without adding results for bk trees because its performance was in range of couple of seconds .
best performances for each language is reported in table ii . we present precision @ k9 for k ∈ 1 , 3 , 5 , 10 and mean reciprocal rank ( mrr ) . the system performs well on synthetic dataset , with a minimum of 80 % p @ 1 and 98 % mrr .
system is able to do each sub - step in real - time . all the sentences used for this analysis had exactly one error according to our system . detection time was the average time weighted over number of tokens , suggestion time was weighted over misspelling character length and ranking time weighted against character length weighted over suggestions length .
system performance on these two benchmarks is presented in table vi . in general terms , both systems perform better than each other on both benchmarks .
shown in table vii , most of the words for each language were detected as detected as known but still there was a minor percentage of words which had to be detected as errors .
, table 6 presents the results of models trained on tweets from one domain to another . we observe that predictive performance is relatively consistent across all domains with two exceptions ( ' food & beverage ' consistently shows lower performance , while ' other ' achieves higher performance ) when using all the data available from the other domains .
total , 1 , 232 tweets ( 62 . 4 % ) are complaints and 739 are not complaints ( 37 . 4 % ) . the statistics for each category is in table 3 .
unigrams and part - of - speech features specific of complaints and non - complaints are presented in table 4 . all correlations shown in these tables are statistically significant ( p < . 01 ) with respect to the presence of words in the unigram representation . negations are uncovered through word clusters ( e . g . , my account , my account ) several words are distinctive of complaints , as these are used to describe actions completed in the past ( e . , g . , a complaint , a working account ) or a complaint ( a complaint ) . a distinctive feature of complaints is that it contains word clusters that are specific of the complaint or the task being addressed to the responsible party ( the responsible party ) . on the other hand , other words such as my account ( been , my order ) and my order ( thank you ) are particularly distinctive for this task . in addition , word clusters such as ellipsis ( which refer to items of services possessed by the complainer ) and word2vec ( which refers to services used to obtain documents ) . in general terms , all words that signify that a complaint is a response are considered as part of a complaint process . mentions of time are one of the most distinctive features of complaints . when interacting with documents , it is easier to detect patterns of time than to classify words .
top features for the liwc category and word2vec topics . general topics typical of complaints include requiring assistance or customer support . complaints tend to not contain personal pronouns ( he , she , him , you , shehe , male , female ) , as the focus on expressing the complaint is on the self and the party the complaint are addressed to and not other third parties . several groups of words are used to express complaints : about orders or deliveries ( in the retail domain ) , about access ( in complaints to service providers ) or about parts of tech products ( in tech ) . a notable group of words that are not part of complaints is negate ( negate ) , which refers to items that the complainer may not use .
are presented in table 6 . most sentiment analysis models show accuracy above chance in predicting complaints . the best results are obtained by the volkova & bachrach model ( sentiment – v & b ) which achieves 60 . 2 f1 . from this group of models , the most predictive groups are intensifiers and downgraders . complaint specific features are predictive of complaints , but to a smaller extent than sentiment , reaching an overall 55 . 2 auc . from the second group , we see that sentiment analysis is a strong baseline for predictive accuracy . syntactic part - ofspeech features such as the liwc dictionaries ( which combine syntactic and semantic information ) and word2vec topics perform similarly to the best performing feature - based models . however , the best performance is obtained by combining the best features of all three models .
results using the original data set and distantly supervised data set are shown in table 7 . the dist . supervision model achieves the best results with a f1 of 79 . 2 and a auc of 0 . 885 . difficultadapt also boosts predictive performance by 1 . 8 points , which shows the diminishing returns from pooling data .
5 shows the model performance in macro - averaged f1 using the best performing feature set . results show that in all but one case , adding out - of - domain data helps predictive performance . the apparel domain is qualitatively very different from the others as a large number of complaints are about returns or the company not stocking items , hence leading to different features being important for prediction . domain adaptation is beneficial , lowering performance on a single domain compared to data pooling .
ert achieved a final accuracy of 91 . 20 % , now marginally comparable to ulmfit ' s full performance . gpt - 2 , on the other hand , finetuned to a full 77 . 42 % , now slightly comparable to the performance of ulmfit . rersults for this experiment are outlined in table 4 .
table 5 , it can be seen that generative pretraining via language modeling does account for a considerable amount of performance , constituting 44 . 32 % of the overall performance ( a boost of 42 . 67 % in accuracy ) in the multitasking setup .
shown in table 6 , reducing the number of attention heads severely decreases multitasking performance . using only one attention head , thereby attending to only one context position at once , degrades performance to less than the performance of 10 heads using the standard finetuning scheme .
6 shows the ablation study results on paragraph selection loss lpara and entity prediction loss lentity . as shown in the second table , using paragraph selection can further improve the joint f1 by 0 . 31 points ,
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distractor and fullwiki setting respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the distractor and fullwiki setting respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
3 shows the performance of paragraph selection on the dev set of hotpotqa . in dfgn , paragraphs are selected based on a threshold to maintain high recall ( 98 . 28 % ) , leading to a low precision ( 71 . 28 % ) . compared to both threshold - based and pure topn - based paragraph selection , our two - step paragraph selection process is more accurate , achieving 94 . 28 % precision and 94 . 53 % recall .
shown in table 5 , the use of ps graph improves the joint f1 score over the plain roberta model by 0 . 18 points .
shown in table 7 , both hgn variants outperform dfgn and eps , indicating that the performance gain comes from better model design .
4 shows the bleu scores of our dual2seq model taking gold or automatic amrs as inputs . the improvement from automatic amr to gold amr ( + 0 . 7bleu ) is significant , which shows that the translation quality of our model can be improved with an increase of amr parsing accuracy .
3 shows the test bleu , ter and meteor scores of all systems trained on the nc - v11 subset or the full meteor subset . dual2seq is consistently better than both opennmt - tf and transformer - tf under all three metrics , as shown in the second group of table 3 , the gap between seq2seq baseline and meteor baseline is much larger under the small - scale setting of our model ( 25 . 5 bleus vs . 25 . 1 ) than under the large - scale set of features ( 29 . 5 / 25 . 1 ) . the difference between our model and our model is narrower than that under our model , but still comparable to our model . we also observe that our model performs better than our model under all the three metrics . the gap is narrower under our approach , however , still under - performs our model on both metrics .
most representative models are elmo , gpt , bert and its variants , and xlnet . next , we give a brief overview of these models and summarize their performance on various benchmarks . smaller tweaks to various aspects of the model have resulted in hundreds of entries on leaderboards ( e . g . , those linked to in section 4 . 3 . 3 and table 4 ) leading only to marginal improvements . table 4 quantitatively compares these models across various benchmarks
2 shows that coreference propagation ( corefprop ) improves named entity recognition performance across all three domains . the largest gains are on the computer science research abstracts of scierc ,
3 shows the performance of our framework on the three high - level tasks . our framework establishes a new state - of - the - art on all three of these tasks , and on all subtasks except event argument identification . relative error reductions range from 0 . 5 - 27 . 9 % over previous state of the art models .
fprop also improves relation extraction performance on scierc . relation propagation ( relprop ) improves relations extraction performance over pretrained bert .
7 compares the results of bert and scibert with the best - performing model configurations . scibert significantly improves performance for scientific datasets including scierc , genia and scierc .
6 shows that variations of our bert model benefit from wider context windows . our model achieves the best performance with a 3sentence window across all relation and event extraction tasks .
table 1 we report the best and average precision @ 1 scores and the average number of iterations among 10 experiments , for different language translations . our proposed model outperforms the previous stateof - the - art models in both languages . in addition , the noise - aware model is more stable and therefore requires fewer iterations to converge . the accuracy improvements are small but consistent , and we note that we consider them as a lower - bound on the actual improvements as the current test set comes from the same distribution of the training set , and also contains similarly noisy pairs .
further analyze our findings with respect to baselines and existing discourse parsers . the first set of results in table 3 shows that the hierarchical right / left branching baselines dominate the completely right branching ones . while the avg attention - aggregation function has the best performance on both evaluation corpora , it has the worst performance on the intra - and rst - dt datasets . finally , the second set in the table shows the performance of the baselines based on the three baselines : the hierarchical right / left branching ones and the hierarchical instr - dt ones .
2 shows the results of different approaches for each of the comparison tasks . our first approach obtains the best results on cwc dataset . it obtains a 4 . 17 % improvement over the average number of turns taken by the previous approach . it also achieves a 5 . 42 % increase over the former state - of - the - art approach .
3 shows the turn - level evaluation results . our approach dkrn outperforms all state - of - the - art methods in terms of all metrics on both datasets .
5 shows the evaluation results . our dkrn agent outperforms all the other agents with a large margin .
6 shows the results of the second study . our agent outperforms the comparison agents with a large margin .
istic input a cnn augmented with self - attentionwe show encouraging relative improvements for future research in this direction . we showed encouraging relative improvement in the vqa task by adding self - aware modules in the baseline resnet - 34 . it is possible to improve the feature extraction procedure with a small improvement in vqa task . anderson et al . ( 2018 ) showed promising results in the second stage
, we managed to show improvements with the β modulation with a resnet - 152 . though the improvement is slim , it is encouraging to continue researching into visual modulation
experimental results of all models are shown in table 1 . first , han models appear to be more appealing than svm because there is less variation in program implementation , hence less effort is required to reproduce the results . with respect to rouge scores , we suspect that there are not enough data to pretrain the models and that there is no need to re - train the models . we observe that the redundancy removal step is crucial for the model to perform well in terms of redundancy removal . it is clear from table 1 that redundancy removal is important for redundancy removal , but it is less effective for svm than it is in svm .
3 : joint goal accuracy on dstc2 and woz 2 . 0 test set vs . various stateof - the - art models . we also compare our model with the delexicalisation - based model mrkˇsi ´ c et al . ( 2017 ) and rastogi et al . , 2018 ) on the test set of hotpotqa . in both cases , the model achieves the best joint goal accuracy and the best multi - domain goal accuracy . in the first case , both the model and the model perform well on both test sets . in the second case , we achieve the best performance with an absolute improvement of 3 . 0 % .
2 : joint goal accuracy on dstc2 and woz 2 . 0 of statenet psi using different pre - trained models based on different single slot . the fact that the food initialization has the best performance verifies our selection of the slot with the worst performance for pre - training . we also pricerange our food initialization as well as the worst performing slot for pretraining .
5 shows the performance on multi30k dataset in asymmetric mode . ame outperforms the fme model , confirming the importance of word embeddings adaptation .
show the results for english and german captions . for english captions , we see 21 . 28 % improvement on average compared to ( gella et al . , 2017 ) in symmetric mode . ame also achieves competitive results with fme model in both languages .
show the results for english and german captions . for english captions , we see 21 . 28 % improvement on average compared to ( gella et al . , 2017 ) in symmetric mode . ame also achieves competitive results with fme model in both languages .
show the results for english captions . ame performs better than fme model in both symmetric and asymmetric modes , which shows the advantage of finetuning word embeddings during training .
show the results for english captions . ame reaches 6 . 25 % and 6 . 66 % better results on average compared to fme model in symmetric and asymmetric modes , respectively , compared to monolingual modes .
4 shows the results for italian and german , compared to english , both for the original and the debiased embeddings . as expected , in both languages , the difference between the average of the two sets is much lower . in italian , we get a reduction of 91 . 67 % of the gap with respect to english .
2 shows the results for italian and german , compared to english . as expected , the average ranking of samegender pairs is significantly lower than that of different - gender pairs , both for german and italian , while the difference between the sets in english is much smaller . table 2 shows how the results translate into german for both languages . for italian , we have decided to use a different ranking for the two languages .
6 shows the results for italian and german for both datasets . in both cases , the new embeddings perform better than the original ones .
results reported in table 7 show that precision on bdi indeed increases as a result of the reduced effect of grammatical gender on the embeddings .
semantic threshold for od - d2v and od - w2v is set at 0 . 6 . we also evaluate our methods with respect to baselines such as ari , silhouette coefficient , bert and doc2vec . these methods generally perform well compared to the competition on both datasets . in particular , od performs particularly well against the following baselines : wmd ( which relies on word2vec embeddings ) and tf - idf . the results of these methods are shown in table 6 . opinion distance methods generally outperform both baselines in terms of ari and average precision ( paired t - test ) on all datasets except for those in the distractor and fullwiki setting .
opinions : we see that od significantly outperforms the baseline methods and od achieves high ari and sil scores for the " seanad abolition " and " pornography " datasets . od also outperforms text - similarity based baselines like tf - idf and wmd , od significantly improves the classification performance of the baselines : on the " video games " dataset , od achieves a ari / sil score of close to zero ( paired t - test ) and a ranking of 0 . 01 . from this group of data , we observe that od achieves the best performance with a weighted average of 5 . 0 % compared to the baseline baseline of 6 . 0 % . od further improves classification performance by high margins :
completeness , here we also compare against unigram or n - gram based classifiers the classification performance of the baselines is reported in table 4 . svm with only od features outperforms many baselines : we see that on " video games " and " pornography " datasets , od is significantly better than any combination of features excluding od . for the " seanad abolition " dataset , we see that od + lsa improves classification performance by a noticeable margin : on the " video games " dataset we see od outperforms od and lsa by a margin of 2 - 2 . 5 % .
ributhe results of different variants are shown in table 6 . od significantly outperforms od - parse : on the three datasets , od achieves a weighted average weighted f1 score of 0 . 54 , 0 . 56 and 0 . 41 respectively compared to the score of od using jensen - shannon divergence . on the other hand , od is significantly better than jensen - sannon divergence on all three datasets . sentiment polarity shifters are extremely useful for classification of opinion distance : we observe that the difference between od and od is relatively small ( i . e . , - 0 . 01 ) compared to od , which is primarily used to classify tweets from " video games " and " pornography " domains . this suggests that od is more effective in classification of opinions .
performance of our model on the testing and development set is shown in table 3 . the difference in accuracy is minimal , however we see significant difference in macro - f score due to different class balance in these sets . we observe that the branch - lstm model predicts commenting , the majority class well , however it is unable to pick out denying , the mostchallenging under - represented class .
denying instances get misclassified as commenting ( see table 6 ) . the statistics for denying instances are presented in table 6 .
1 shows the absa datasets from the restaurants domain for english , spanish , french , dutch , russian , turkish and turkish . from left to right each row displays the number of tokens , number of targets and number of multiword targets for each training and test set .
2 provides detailed results on the opinion target extraction ( ote ) task for english . we show in bold our best models ( all ) chosen via 5 - fold cv on the training data . moreover , we also show the results of the best models using only one type of clustering feature , namely , the best brown , clark , word2vec and word3vec models .
6 shows that our system outperforms the best previous approaches across the five languages .
errors in our system are caused by false negatives [ fn ] , as can be seen in table 7 .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
greek analogy test set contains 39 , 174 questions divided into semantic and syntactic analogy questions . semantic questions are divided into 15 categories and include 13 , 650 questions in total . syntactic questions include 25 , 524 questions we show the full greek word analogy dataset in table 1 . we compare our proposed corpus with the original ones from our first study .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
compared to gr def model in terms of pearson correlation . gr def model had the highest correlation with human ratings ,
epm " generalizes best , and in out - ofdomain evaluations , it considerably outperforms the model of e2e - coref ,
pos and named entity tags have the least and the pairwise features have the most significant effect .
performance of the " + epm " model compared to recent state - of - the - art coreference models on the conll test set is presented in table 4 . epm feature - values result in significantly better performance than those of jim while the performance of " + g & l " is considerably less than jim .
observe that incorporating all the linguistic features bridges the gap between the performance of " top - pairs " and " ranking " . it also improves the generalization ability of the model .
observe that incorporating all the linguistic features bridges the gap between the performance of " top - pairs " and " ranking " . it also improves the generalization ability of the model ,
performance of the " + epm " model compared to recent state - of - the - art coreference models on the conll test set is presented in table 4 . epm feature - values result in significantly better performance than those of jim while the performance of " + g & l " is considerably less than jim .
4 shows the coverage of vsms with the highest coverage . for brevity , we only report coverage on w2 contexts lemmatization allows more targets to exceed the sgns frequency threshold , which results in significantly better coverage . pos - disambiguation , in turn , fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for target targets . wn - n shows low coverage with almost all targets .
2 provides detailed descriptions of the vsms in question . lemmatized targets generally perform better on simverb , simlex and simlex . the type - based vsms perform similarly on simlex , with the difference being less pronounced for simlex compared to lemmatized ones . morph - based targets perform well on both datasets we provide the morph - based vsms with the advantage of high performance on the vsm ( vsm ) stage . syntactic targets perform similarly to the effect of the pos - based ones on the simlex / simlex contexts . this approach uses word type specializedization ( pos ) which allows the derivationalization of the word forms to better interpret the semantic information . this approach results in a uniform performance improvement on simmlex and simlex , lemma variants perform similarly , however the results are slightly less uniform . this is evident from the results of phase 1 of morph - a ( morph - a ) in the comparison of the two datasets .
6 provides exact scores for reference . note that the shared vocabulary setup puts the type and type targets at advantage since it eliminates the effect of low coverage . for reference , type targets ( p ≤ . 005 ) perform best on vn with the effect being less pronounced on the type targets . for wn - n , the effect is minimal but still significant for lemmatized targets . when dependency - based contexts are used , lemma - based targets significantly outperform type targets in terms of f scores on both vn and p scores . lemma based targets generally perform better on both variants of the same word . for vn , it is better to focus on type targets than on targets with a low f score .
embeddings derived from wiki - pubmed - pmc outperform glove ( moen and ananiadou , 2013a ) in the extraction of most relation types ( table 1 ) . the feed - forward ann displays significant over - fitting across all relation types , with the exception of boc , which obtains the most significant overfitting .
- sentential cooccurrence baseline outperforms other approaches that allow boundary expansion . as the results of cross - sentence relation extraction ( ρ = 0 ) shows , the semantic relations in this data are strongly concentrated within a sentence boundary , with the boundary expansion being the most important part of the relation extraction process .
3 compares multi - news to other news datasets used in experiments below . we choose to compare it with duc data from 2003 and 2004 and tac 2011 data , as these are typically used in multi - document settings . the number of examples in this set is in table 3 . the total number of words in the concatenated inputs is shorter than other mds news datasets , as the size of the input documents is smaller than mds data . our summaries are notably longer than those in other works , as those consist of 10 inputs .
report the percentage of n - grams in the gold summaries which do not appear in the input documents as a measure of how abstractive our summaries are in table 4 . as the table shows , the smaller mds datasets tend to be more abstractive , but multi - news is comparable and similar to the abstractiveness of sds datasets . grusky et al . ( 2018 ) additionally define three measures of the extractive nature of a dataset , which we use here for a comparison . the summaries used here are comparable but smaller than the ones used in the original documents .
model outperforms pg - mmr when trained and tested on the multi - news dataset . the transformer performs best in terms of r - 1 while hi - map outperforms it on r - 2 and r - su . also , we notice a drop in performance between pg - original ( which is pre - trained and applies mmr on top of the model ) , finally , we see an increase in performance of the transformer over the previous state of the art models .
shown in table 5 , as the required derivation step increases , the prkgc + ns model suffers from predicting answer entities and generating correct nlds . this indicates that the challenge of rc - qede is in how to extract relevant information from supporting documents and synthesize these relevant information .
evaluation results are shown in table 2 . it is clear from table 2 that the annotated nlds are of high quality ( reachability ) , and each nld is properly derived from supporting documents ( derivability ) . crowdworkers found that 45 . 3 % of 294 ( out of 900 ) 3 - step nlds has missing steps to derive a statement .
shown in table 4 , the prkgc models learned to reason over more than simple shortest paths . supervising path attentions ( prkgc + ns ) are particularly effective for improving the generalization ability of question answering . as shown in the second table , when using more than 1 , 021 maximum answerable paths , rg - l model learned to distinguish between human - generated nlds and non - nlds . when using more - than - 1 , 021 max - pooled paths , rc - qede performs best , but still suffers from the deficiency of interpretability due to the high correlation between pg - original genome and f1 ( see table 4 ) . finally , when interacting with more than 980 maximum answers , it produces markedly better results .
shown in table 7 , the prkgc models achieve a comparable performance to other sophisticated neural models .
results are presented in table 3 . perhaps the most striking thing about the ablation results is that the ' traditional ' lstm layout outsperformed the ' alternating ' one we chose for our submission . apart of the flipped results , we also observed very similar results for the single model as well . finally , we observed a slight improvement in cv score for the two models that we chose .
system ' s official score was 60 . 9 % ( micro - f1 ) af therefore , we report both the official score and the result of re - scoring our second submission after replacing these 10 files with the ones from our first submission . the results are presented in tables 1 and 2 .
, we report both the official score ( from our second submission ) and the result of re - scoring these files after replacing them with the ones from our first submission . the results are presented in tables 1 and 2 .
table 4 , we compare relis with non - rl - based summarisation systems like tcsum and srsum . relis outperforms all rl - based systems except for the one that icsum uses , priorsum . relis performs on par with the duc datasets , but it has the advantage of training on a larger corpus .
2 compares the quality of our ^ ( cid : 28 ) u x with other widely used rewards for input - specific rl ( see x4 ) . ^ ( cid : 27 ) ux significantly increases the correlation with the ground - truth ranking of our proposed l2r method over all other approaches except for asrl ( which obtains the best interpretability ) and j2r .
furthermore trained models on additional polarity features for task b as mentioned before . the results displayed in table 6 are presented in bold .
task a , all models trained on the stacked learner beat the baseline substantially except for those using sif embeddings .
task b , all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings .
performed an ablation study on a single model having obtained 69 . 23 % accuracy on the validation set . results are summarized in table 7 . we can observe that the elmo layer had a significant impact on the model ' s performance , it also contributed significantly to its δ % increase in performance as compared to using glove pre - trained word embeddings . using the concatenation of the max - pooled and last hidden states of the bilstm hidden layer with the greatest performance was beneficial for our model . regarding optimization strategies , we also tried using sgd with different learning rates and a stepwise learning rate schedule . we found that using the sgd had a beneficial effect , however , it did not improve performance as much as we had hoped . finally , we tried using concatenated parts of the data to calculate the final cost of the model ,
the corresponding classification report . in general , we confirm what klinger et al . ( 2018 ) report : anger was the most difficult class to predict , followed by surprise , whereas joy , fear , and disgust are the better performing ones .
4 shows the overall effect of hashtags and emoji on classification performance . tweets containing emoji seem to be easier for the model to classify than those without . hashtags also have a positive effect , however it is less significant .
5 shows the effect specific emoji have on classification performance . it is clear some emoji strongly contribute to improving prediction quality . some of the most interesting ones are mask , rage , and cry , which significantly increase prediction accuracy . further , contrary to intuition , the sob emoji contributes less than cry , despite representing a stronger emotion . finally , not all emoji are beneficial for this task . when removing sweat smile and confused accuracy increased ,
results on winograd and winocoref datasets are shown in table 7 . the best performing system is knowcomb . it improves by over 20 % over the state - of - art general coreference system . it also outperforms rahman and ng ( 2012 ) by a margin of 3 . 5 % .
results are shown in table 7 . our knowcomb system achieves the same level of performance as does the state - of - art general coreference system we base it on . however , it performs slightly worse than our knowcomb systems on standard coreference problems .
ailed analysis to study the coverage of our predicate schemas knowledge , we label the instances in winograd ( which also applies to winocoref ) with type 1 and type 2 knowledge . the distribution of the instances is shown in table 9 .
also provide an ablation study of the knowledge schemas on winocoref . these results use the best performing knowcomb system . they show that both type 1 and type 2 schema knowledge have the highest precision on category 1 and category 2 datainstances , respectively , compared to those on full data . type 1 also shows the performance improvement over the former state - of - the - art knowcomb systems . type 2 also exhibits the best performance .
4 shows the numerical results obtained during the experiments for the four combinations tested . in general terms , the results displayed in table 4 show that the rejection method can reduce the error of the output predictions when applying a pre - trained black - box classification system to a new domain . table 4 : accuracy obtained by training an standalone classifier , applying the api and the proposed wrapper for each domain the rejection method proved beneficial for both datasets .
