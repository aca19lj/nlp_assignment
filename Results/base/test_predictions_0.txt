performance of bert and roberta on the easy subset is reported in table vii . the performance on the hard subset is relatively high , indicating that training on b - copa encourages bert to rely less on superficial cues . moreover , training on copa exposes the model to a variety of adversarial and non - adaptive cues , such as the word " pool " , " memory " , " body " and " body " . these cues are notably gender - neutral , improving the performance of the bert - large - ft model over the previous state of the art model .
shown in table 1 , bert and roberta achieve considerable improvements on copa ( see table 1 ) . compared to prior work , berta achieves the best performance on both copa and supervision .
2 shows the five tokens with highest coverage . for example , a is the token with the highest coverage and appears in in either a correct alternative or wrong alternative 21 . 2 % of the time . its productivity of 57 . 5 % expresses that it appears in correct alternatives 7 . 9 % more often than expected by random chance . this suggests that a model could rely on such unbalanced distributions of tokens to predict answers based only on alternatives without understanding the task .
human evaluation shows that our mirrored instances are comparable in difficulty to the original ones ( see table 3 ) .
4 shows the performance of bert and roberta on the easy and hard subsets . compared to previous models , bert performs much better on the hard subset , indicating that bert relies less on superficial cues . on the easy subset , the improvements are slim , but significant ( p - value < 0 . 001 ) compared to sasaki et al . ( 2017 ) , indicating that the superficial cues are important for bert to improve performance .
relatively high accuracies of bert - large and roberta - large show that these pretrained models are already well - equipped to perform this task " out - of - the - box " . moreover , training on b - copa encourages bert to rely less on superficial cues , improving performance on easy and hard subsets , respectively , when trained with only superficial cues .
observe that bert trained on balanced copa is less sensitive to a few highly productive superficial cues than bert - trained on b - copa . these cues are shown in table 7 . however , for cues with lower productivity , the picture is less clear , in case of roberta , there are no noticeable trends .
the proposed cnnlstmour - neg - ant improves upon the state - of - the - art lstm - w / o neg . baseline with f1 scores improving from 0 . 72 to 0 . 78 on the positive sentiment test and from 1 . 83 to 2 . 86 on the negative sentiment test .
performance of bilstm with gold negation cues for scope prediction is shown in table 7 . the performance gap between in - scope and out - ofscope tokens is minimal , however it is significant ( p < 0 . 001 ) .
statistics are shown in table 3 . average number of tokens per tweet is 22 . 3 , per sentence is 13 . 6 and average scope length is 2 . 9 .
4 shows the f - score of false negation from a 0 . 61 baseline to 0 . 68 on a test set containing 47 false and 557 actual negation cues .
results are shown in table 5 . with data augmentation , our damd model obtains the average value of diversity and appropriateness , and is slightly better than hdsa in both languages . however , the improvement is still significant , with an absolute improvement of 2 . 5 % compared to hdsa .
results are shown in table 1 . after applying our data augmentation , both the action and slot diversity are improved consistently , hdsa has the worse performance and benefits less from data auguration compared to our proposed domain - aware multi - decoder network ,
3 shows that after applying our domain - adaptive delexcalization and domain - aware belief span modeling , the task completion ability of seq2seq models becomes better . the damd model outperforms all the base lines with respect to inform and success rates and bleu score is 15 . 7 % better than the previous best state - of - the - art model ( mehri et al . 2008 ) . moreover , if a model has access to ground truth system action forms , the model further improves its task performance and achieves higher task success rates .
3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and qa - srl . the models are evaluated using exact match ( em ) and f1 measures , as the results show , incorporating coverage improves the model ' s performance in the in - domain evaluation as well as the out - ofdomain evaluation in qasrl .
2 shows the performance for both systems for in - domain ( the multinli development set ) as well as out - of - domain evaluations on snli , glockner , and sick datasets . the results show that coverage information considerably improves the generalization of these models across multiple nli datasets , as measured by the cross - dataset evaluation results .
4 shows that gdpl has the smallest kl - divergence to the human on the number of dialog turns over the baselines , where πturns denotes the discrete distribution of the kldivergence that the policy has to make with the dialog turns in the agenda - based user simulator .
performance of each approach that interacts with the agenda - based user simulator is shown in table 3 . gdpl achieves extremely high performance in the task success on account of the substantial improvement in inform f1 and match rate over the baselines , gdpl even outperforms human in completing the task , and its average dialog turns are close to those of humans , though gdpl is inferior in terms of match rate . acer and ppo are comparable in performance , but their agenda turns are lower than gdpl - sess , aldm is comparable in task success and is more comparable in match rate , it is clear from table 3 that gdpl has better performance on the agenda success and that it is better to match dialog turns with gdpl than to match them . finally , gdpl ' s performance is comparable to the human - generated dialog turns ,
agent the performance of different agents on the neural user simulator is shown in table 5 . aldm obtains the best performance when interacting with vhus . gdpl even achieves higher success rates than acer and ppo . however , gdpl is still inferior in match rate and achieves lower success rates .
6 presents the results of human evaluation . gdpl outperforms aldm and ppo significantly in all aspects ( sign test , p - value < 0 . 001 ) except for the quality compared to acer . among all the baselines , gdpl obtains the most preference against ppo .
7 shows the distribution of the return r = t γtrt according to each metric . it can be observed that the learned reward function has good interpretability in that the reward function is positive when the dialog gets a full score on each metric , and negative otherwise .
present precision scores for the word analogy tests in table vii . our proposed method outperforms the original glove embeddings in semantic and syntactic analogy tests . however , it has the advantage of training in syntactic and semantic analogy tests , which results in a better interpretability performance . in particular , it achieves higher precision scores than proposed by spine and word2vec .
apply the test on five participants . results tabulated at table v shows that our proposed method significantly improves the interpretability by increasing the average true answer percentage from ∼ 28 % for baseline to ∼ 71 % for our method .
performing slightly worse than the original embeddings , word2sense outperforms all the alternatives except word2vec baseline in terms of oiwe - ipg and sov scores . on the other hand , it performs slightly better on the sov and spine tests , indicating that the semantic information injected into the algorithm by the additional cost term is significant enough to result in a measurable improvement . this corroborates the hypothesis that the algorithm injected by roget and glove are a better complement to the original algorithm .
investigate the effect of the additional cost term on the performance in the semantic analogy test , in table viii . we find that for all the questions that are considered , the precision scores drop significantly when we add the cost term from the original embeddings to the proposed ones . this result , in turn , hurts the performance of the test when we include all the concept words in the equation , as shown in fig . viii .
accuracies are presented in table ix . the proposed method outperforms the original embeddings and performs on par with the sov , spine and word2sense datasets . however , it has the advantage of training on a larger corpus , which results in significantly better interpretability . this result along with the intrinsic evaluations show that the proposed imparting method can significantly improve interpretability without a drop in performance .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results on event coreference . our joint model outperforms all the base lines with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points .
2 presents the performance of our method with respect to entity coreference . our joint model improves upon the strong lemma baseline by 3 . 8 points in conll f1 score .
3 presents the results on event coreference . our joint model outperforms all the base lines with a gap of 10 . 5 conll f1 points from the last published results ( kcp ) , while surpassing our strong lemma baseline by 3 points . the results reconfirm that pre - clustering of documents to topics is beneficial , improving upon the kcp performance by 4 . 6 points ,
show the precision numbers for some particular recalls as well as the auc in table 1 , where our model generally leads to better precision .
show the precision numbers for some particular recalls as well as the auc in table 2 , where pcnn + att ( 1 ) refers to train sentences with two entities and one relation label , pcnn - att ( m ) . these train sentences have higher auc and rank + exatt scores , indicating that train sentences are more effective in generation of answers .
experimental results on wikidata dataset are summarized in table 3 . the results of " - word - att " row refers to the results without word - level attention . according to the table , the drop of precision demonstrates that there is a need to design more sophisticated capsule nets to obtain better interpretability . these capsule net features have a high precision on word level attention .
the table 4 depicts , the training time increases with the growth of d .
show the comparison of 1 - 5 iterations . we find that the performance reach the best when iteration is set to 3 .
3 presents the rouge metrics of our system ( neuraltd + learnedrewards ) and multiple stateof - the - art systems . the summaries generated by our system are optimised towards high correlation with human judgement , and are 18 . 5 % better than any prior system we have tried ( upadhyay et al . , 2017b , c ; zhou et al . 2018b ; kedzie et al , 2018a ; xu et al . ( 2018b ) and zhou et al . , 2018a ) summarize the summaries extracted by the system ' s neuraltd counterparts , but are only comparable to human judgement ( 18 . 5 % ) in terms of rouges f - score .
table 1 , we find that all metrics we consider have low correlation with the human judgement . more importantly , their g - pre and g - rec scores are all below . 50 , which means that more than half of the good summaries identified by the metrics are actually not good , and more than 50 %
3 shows the quality of different reward learning models . as a baseline , we also consider the feature - rich reward learning method proposed by peyrard and gurevych ( see § 2 ) . mlp with bert as en ( 2018 ) coder has the best overall performance . specifically , bert + mlp + pref significantly outperforms ( p < 0 . 001 ) all the other reward learning methods that do not use bert * mlp ,
human evaluation is shown in table 4 . summaries generated by neuraltd receives significantly higher human evaluation scores than those by refresh ( p = 0 . 0088 , double - tailed ttest ) and extabsrl ( p ( cid : 28 ) 0 . 01 ) . also , the average human rating for refresh is significantly higher ( p : 27 )
5 compares the rouge scores of using different rewards to train the extractor in extabsrl ( the abstractor is pre - trained , and is applied to rephrase the extracted sentences ) . again , using the learned reward boosts rouges scores significantly ( p = 0 . 0057 ) .
results on the proprietary help desk dataset are shown in table 9 . table 9 shows the results of an ablation study we performed to identify the most important components of our model architecture and training regime . we found that the hierarchical model performs significantly worse than the two layers of our lstm , indicating that there is a need to design more sophisticated models to better interpret the data .
model makes use of a fast recurrent network implementation ( lei et al . , 2016 ) and multiheaded attention ( lin et al . 2017 ) and achieves over a 4 . 1x inference speedup over traditional encoders such as lstm ( hochreiter and schmidhuber , 1997 ) . sru also exhibits a significant speedup in inference time compared to an lsm encoder ( by a factor of 4 . 4x in our experiments ) , retrieving the best candidate once the context is encoded takes a negligible amount of time compared with the time to encode the context . table 8 also highlights the scalability of using a dual encoder architecture : the sru is more than 4x faster at inference time than an sru encoder ,
performance of our model according to these auc metrics can be seen in table 3 . the high auc indicates that our model can easily distinguish between the true response and negative responses . furthermore , the auc @ p numbers show that the model can distinguish between negative responses and positive responses .
4 shows rn @ k on the test set for different values of n and k when using a random table 4 shows that recall drops significantly as n grows , meaning that the r10 @ k evaluation performed by prior work may significantly overstate model performance in a production setting .
results in table 5 show that the clustering and frequency whitelists perform comparably to the frequency and clustering ones , respectively . however , recall is still relatively high , with the exception of random 10k , which shows the diminishing returns from clustering removal . the recall results are slightly worse than those in the frequency / clustering whitelist ,
6 shows r @ 1 and coverage for the frequency and clustering whitelists . while recall @ 1 shows higher recall , coverage shows lower coverage .
results of the human evaluation are in table 7 . our proposed system works well , selecting acceptable ( i . e . good or great ) responses about 80 % of the time , and selecting great responses more than 80 % . interestingly , the size and type of whitelist seem to have little effect on performance , indicating that the whitelist contains responses appropriate to a variety of conversational contexts .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , the cues are markedly gender - neutral , improving the bias metric by 9 % in the standard task formulation and to parity in the gold - two - mention case .
note particularly the large difference in performance between genders , both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . in fact , lee et al . ( 2017 ) and parallelism produce remarkably similar output : of the 2000 example pairs in development set , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples .
cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 6 ) . in fact , the cues are markedly gender - neutral , improving the bias metric by 9 % in the standard task formulation and to parity in the gold - two - mention case .
istent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific heads and that these heads are in the deep layers of the network ( e . g . l3h7 ) .
find that the instances of coreference that transformersingle can handle is substantially
investigate the effects of the multi - factor count ( m ) in our final model on the test datasets in table 3 . we observe that for the nyt10 dataset , m = { 1 , 2 , 3 } gives good performance with m = 1 achieving the highest f1 score . on the nyt11 dataset , the performance is slightly worse than m = 4 achieving the best performance . these experiments show that the number of factors giving the highest performance may vary depending on the underlying dataset .
present the results of different models on the relation extraction task on the two datasets in table 2 . our model outperforms the previous stateof - the - art models on both datasets in terms of f1 score . on the nyt10 dataset , it achieves 3 . 8 % higher f1 scores compared to the previous best state of the art model ea . similarly , the same performance is obtained on the nyt11 dataset , which shows the precision scores obtained by pcnn in the previous experiments .
experimental results on the nyt11 dataset are shown in table 4 . when we add multi - factor attention to the baseline bilstm - cnn model without the dependency distance - based weight factor in the attention mechanism , we get 0 . 5 % f1 score improvement ( a2 − a3 ) . adding the dependency weight factor with the window size of 5 improves the f1 scores by 3 . 2 % ( a3 − a4 ) . replacing the attention normalizing function with softmax operation reduces the rec . score marginally ( a4 − a6 ) . to mimic the effect of concatenation of attention , we concatenate the attention scores of the two attention mechanisms . we apply max - pooling operation across the multiple attention scores to compute the final attention scores . this reduces the noise and allows the model to regain the recall scores .
flickr30k we have zsgnet showing much better category - wise performance than other models that use object detectors pretrained on pascal - voc ( " animals " , " people " and " vehicles " ) . on the other hand , it has slightly better performance on classes that are common to both flickr29k ( animals and " bodyparts " are the most difficult classes to classify ) , and on the classes like " clothing , bodyparts and instruments " our model exhibits much better performance .
2 compares zsgnet with prior works on flickr30k entities and referit . we denote these models using " det " and " cls " to denote models using pascal voc detection weights and imagenet [ 10 , 41 ] classification weights . networks marked with " * " fine - tune their object detector pretrained on pascalvoc on the fixed entities ( voc , scrc , referit ) and other fine - tuned object detectors ( zsgnet , imagenet , zsgnet ) . referit networks ( cls ) is able to distinguish between pretrained and unlabelled objects .
4 shows the performance of our zsgnet model compared to qrg on the unseen splits in flickr - split - 0 and flickr - split - 0 . as shown in table 4 , the accuracy obtained on flickr - split - 0 is higher than qrg even though the latter has seen more data we observe that the accuracy remains the same across all unseen splits ,
show the performance of our model with different loss functions using the base model of zsgnet on the validation set of referit in table 6 . note that using softmax loss by itself places us higher than the previous methods . further using binary cross entropy loss and focal loss give a significant performance boost which is expected in a low - supervision setting .
the en - de news / ted task ( table 4 ) , fine - tuning rapidly forgets previous tasks . ewc outperforms no - reg , l2 and l2 on all three tasks except it . the it task is very small : training on it data alone results in over - fitting , with a 17 . 5 % improvement over the baseline news model . it data is very noisy , with two it tasks leading to overfitting and overfitting . ewc reduces forgetting on two previous tasks while further improving on the target domain .
es - en , the health and bio tasks overlap , but forgetting still occurs under bio ( table 3 ) . regularization reduces forgetting and allows further improvements on bio over noreg fine - tuning . ewc outperforms the l2 approach
5 shows improvements on data with uniform ensembling over oracle without domain labelling using our adaptive decoding schemes . identity - bi strongly improves over uniform , especially for es - en bio , and bi with is as in eq . 10 . the improvement over uniform is less striking than for oracle - bi , but still significant . bi and is both individually outperform the oracle for most domains , with adaptive decoding improving over both uniform and unadapted models .
table 6 shows the results for models fine - tuned with ewc . uniform ensembling outperforms the oracle model in most cases , while bi + is performs better in all cases .
no - reg ensembling outperforms the approach described in freitag and al - onaizan ( 2016 ) in terms of bleu for test data concatenated across domains . bi + is with ewc - adapted models gives a 3 . 8 % improvement over the strong uniform ensemble chosen with no test domain labeling . bi + is gives a 2 . 4 % boost over the oracle model chosen with unadapted uniform ensembles .
results in table 8 show that when only using original utterances with ellipsis , precision is relatively high while recall is low .
can be seen in table 6 that empirically adding logits from two models after classifiers performs the best .
table 6 shows the performance of different approaches on map metric when trained on the cnn - zhao dataset . the best performance is achieved by our approach , which verifies the effectiveness of our model .
table 3 , we can see that competitive advantage is brought about by the strong baselines on eur - lex , and competitive results on rcv1 . these results appear to indicate that our approach has superior generalization ability on datasets with fewer training examples , i . e . , rcv2 has 729 . 67 examples per label while 3152 . 53 examples have 15 . 59 examples . table 3 shows the performance of our approach compared to previous approaches .
evident from table 1 , there is a significant imbalance in the distribution of training instances that are suggestions and non - suggestions .
3 shows the performances of all the models that we trained on the provided training dataset . the ulmfit model achieved the best results with a f1 - score of 0 . 861 on the training dataset and the test dataset , which shows the performance of the lstm model in sub task a .
4 shows the performance of the top 5 models for sub task a of semeval 2019 task 9 . our team ranked 10th out of 34 participants .
iii shows the wers on the simulated and real test sets when aas is trained with different training data . with the simulated dataset as the training data , fsegan shows lower wer than aas ( 25 . 9 % ) on the real test set . when aas was trained with the real dataset , it achieves the best wer ( 24 . 7 % ) and the best aas score ( 29 . 6 % ) . however , aas performs worse than simulated in terms of wer because the simulated test set is more realistic .
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . the same tendency is observed for acoustic supervision ( 15 . 6 % ) and multi - task learning ( 14 . 4 % )
and ii show the wer and dce ( normalized by the number of frames ) on the test set of librispeech + demand , and chime - 4 . the wiener filtering method shows lower dce , but higher wer than no enhancement . we conjecture that wiener filter remove some fraction of noise , however , remaining speech is distorted as well . the adversarial supervision ( i . e . , wac = 0 , wad > 0 ) consistently shows very high wer , because the enhanced sample tends to have less correlation with noisy speech , as shown in fig . 3 . the same tendency is observed for acoustic supervision ( 27 . 7 % ) and multi - task learning ( 29 . 1 % )
cerning transfer learning experiments ( rq1 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . our approach shows a slight improvement over the performance of local - manual model ( 60 . 5 % ) on f - measure ( 71 . 42 % ) on the nepal and kerala datasets , but still outperforms the local baseline by a margin of 2 . 48 % in f - measure ( 66 . 42 % ) . further improving performance by high margins
evaluating the approaches laid out in section iv , we consider three real - world datasets . originally , all the raw messages for the datasets described in table ii were unlabeled , in that their urgency status was unknown . since the macedonia dataset only contains 205 messages , and is a small but information - dense dataset , we labeled all messages in macedonia as urgent or non - urgent ( hence , there is no urgent message in macedonia per table ii ) . since macedonia is a relatively small country , it is easier to classify messages as urgent than urgent ones . table ii shows that nepal is roughly balanced , while kerala is imbalanced .
iv illustrate the results for rq1 on the nepal and kerala datasets . our approach shows that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) . further improving performance by mixing source and target labeled training data
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v and vii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v and vii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) .
cerning transfer learning experiments ( rq2 ) , we note that source domain embedding model can improve the performance for target model , and upsampling has a generally positive effect ( tables v - viii ) . as expected , transfer learning table vii , a result not found to be significant even at the 90 % level ) .
4 shows the synchronic performance of our system when tn and rn are tested on the locations from the same year ( including peaceful ones ) . at the same time , the system performs synchronically , confirming the importance of word embeddings adaptation .
replication experiment in table 2 , we replicate the experiments from ( kutuzov et al . , 2017 ) on both sets . it follows their evaluation scheme , where only the presence of the correct armed group name in the k nearest neighbours of the ˆi mattered , and only conflict areas were present in the yearly test sets . essentially , it measures the recall @ k , without penalizing the models for yielding incorrect answers , and never asking questions having no correct answer at all ( e . g . , peaceful locations ) . the performance is very similar for both sets ,
3 shows the diachronic performance of our system in the setup when the matrix tn and rn are applied to the year n + 1 . for both gigaword and now datasets ( and the corresponding embeddings ) , using the cosinebased threshold decreases recall and increases precision ( differences are statistically significant with t - test , p < 0 . 001 ) . at the same time , the integral metrics of f1 consistently improves ( p ≤ 0 . 01 ) .
4 : the ablation study on the woz2 . 0 dataset with the joint goal accuracy on the test set . the effectiveness of our hierarchical attention design is proved by an accuracy drop of 1 . 69 % after removing residual connections and the hierarchical stack of our attention modules .
3 shows the joint goal accuracy of the dst models on the woz2 . 0 test set and the multiwoz test set . we also include the inference time complexity ( itc ) for each model as a metric for scalability table 3 compares the performance of the models with the previous state - of - the - art models . for woz 2 . 0 , we maintain the performance at the level of the state - ofthe - art , with a marginal drop of 0 . 3 % compared with previous work . considering the fact that woz is a relatively small dataset , this small difference does not represent a significant big performance drop . on the muli - domain test set , our model achieves the best performance with an o ( n ) of 48 . 79 % , which marginally outperforms the previous best model . for the multi - woz dataset , we include the feature - rich dst model , which performs on par with the statenet psi dataset .
5 : the ablation study on the multiwoz dataset with the joint domain accuracy ( jd acc . ) and joint goal accuracy ( jg acc . ) on the test set . from table 5 , we can further calculate that given the correct slot prediction , comer has 87 . 42 % chance to make the correct value prediction . however , the accuracy of slot prediction is only 58 . 42 % , which shows the jds acc . and jg acc . are only comparable to that of comer . we can also see that the value prediction based on the slot prediction has an absolute boost of 2 . 48 % when we switch from the slot representation to the nested tuple representation
4 presents the results of domain transfer using the three aspects of the beer review data . our model ( ours ) shows marked performance improvement over the baselines across all three target domains .
3 shows the performance of aspect transfer on the beer review dataset . our model ( ours ) outperforms all the other methods with a large margin . it achieves the best performance on average across all three target aspects .
5 presents the results of an ablation study of our model in the setting of domain transfer . as this table indicates , both the language modeling objective and the wasserstein distance contribute similarly to the task . in both cases , the wasserstein distance leads to better task performance .
compare our proposed model against the pre - trained catseq models in terms of inspec ( krapivin , kp20k , gan ) , rl ( yuan et al . , 2018 ) and catseq ( chan et al . 2019 ) . the results are broken down in table 1 . for the krapivina dataset , we see that our proposed algorithm performs better than the previous best state - of - the - art model on three out of the four datasets . on the other hand , the performance of our model is slightly worse than the best on the other four datasets suggesting that gan models are more effective in generation of keyword clusters .
also evaluated the models in terms of α - ndcg @ 5 ( clarke et al . , 2017 ) . the results are summarized in table 2 . our model obtains the best performance on three out of the four datasets . the difference is most prevalent in kp20k , where gan ( at 0 . 85 ) is nearly 5 % better than the others .
simplify our dataset , we have decided to focus our work on job positions – which , we believe , are an interesting window into the nature of gender bias – and were able to obtain a comprehensive list of professional occupations from the bureau of labor statistics ' detailed occupations table , from the united states department of labor . the values inside , however , had to be expanded since each line contained multiple occupations and sometimes very specific ones . fortunately , this table also provided a percentage of women participation in the jobs described in table 7 , so we filtered those that had more than 50 thousand workers . we filtered those because they were too generic ( " computer occupations , all other " , and others ) or because they had gender specific words for the profession ( " host / hostess " , " waiter / waitress " ) . table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation .
we have found is that google translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender - neutral pronouns , in general . furthermore , this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes , such as life and physical sciences , architecture , engineering , computer science , mathematics and computer science . table 9 summarizes these data ,
results in table 2 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 15 times as frequently as whites . the classifier trained on golbeck et al . ( 2017 ) is particularly sensitive to the word ' b * tch ' with 96 % of blackaligned tweets predicted to belong to this class . for davidson et al . , ( 2018 ) we see that there is a substantial racial disparity in the classifier ' s performance , with blackaligned and whitealigned tweets classified as hate speech at 2 . 7 times the rate of those in the white - aligned corpus . in davidson and al . ( ' 2018 ' ) we see a significant racial disparity , with around 9 % of tweets flagged as offensive compared to 2 . 5 % of those by whites . in the united states , we see an almost uniform racial imbalance , with the blackaligned corpus classified as a hate speech group at 1 . 9 % and 2 . 1 % of the whitealigned corpus , respectively . this suggests that there are some significant racial disparities in the performance of these tweets , which are classified as harassment at a higher rate than in the previous experiment , although we see no significant racial imbalance .
performance of these models on the 20 % held - out validation data is reported in table 1 . we see that hate speech and harassment are particularly difficult to detect . in fact , the most prevalent class of hate speech is offensive , followed by harassment at the level of 0 . 72 , 0 . 86 and 0 . 87 respectively , compared to the previous state - of - the - art , which labels black - aligned tweets as hate speech .
results in table 2 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 5 times as frequently as white aligned tweets . the classifier trained on davidson et al . ( 2017 ) is particularly sensitive to blackaligned tweets . it is clear from the results that there is a significant racial disparity in the classifier ' s performance . the most prevalent classifier is sexism , which is classified as harassment at 1 . 7 times the rate of those in the white - aligned corpus . for waseam ( 2016 ) , we see a substantial racial disparity , classified as hate speech at 2 . 9 % and 1 . 15 % respectively compared to the previous state of the art , respectively . for golbeck et al . , ( 2018 ) we see that there are no significant trends in the blackaligned corpus , although there are still instances of racism and sexism in the corpus that are classified as offensive .
results in table 1 show that for tweets containing the word " n * gga " , classifiers trained on waseem and hovy ( 2016 ) are both predict black - aligned tweets to be instances of sexism approximately 1 . 5 times as frequently as whites . the classifier trained on davidson et al . ( 2017 ) is particularly sensitive to the word ' b * tch ' with 96 % of blackaligned tweets predicted to belong to this class . the founta et al . , 2018 classifier labels black aligned tweets as harassment at a higher rate for both groups than in the previous experiment , although it is more likely to classify them as offensive . for golbeck et al , 2018 , we see that there is a significant racial disparity in the classifier ' s performance , with black - aligned tweets classified as hate speech at 1 . 1 times the rate of white aligned tweets . for davidson and al . , we see a substantial racial disparity , with the blackaligned tweet classified as offensive at 2 . 5 % of the time and 1 . 7 % of those in the white - aligned corpus . golbeck and al . ( 2018 ) classifies black aligned tweet harassment at an even higher rate , with 18 % classified as abusive compared to 6 % and 8 % respectively . for waseeme and landon ( 2018 ) we see an extremely low rate of hate speech , at 0 . 8 % compared to 1 . 3 % and 6 % respectively , compared to the 18 % and 9 % rates of sexism in the original experiment , respectively .
can be seen in table 4 the performance of sparsemax compared to softmax on mscoco and flickr30k . selective attention mechanisms like sparsemax and tvmax reduce repetition , however , the results are still slightly worse than softmax , indicating that sparsemax is better at selecting the right captions and the repetition is less important for selecting the captions .
performing slightly worse than sparsemax under automatic metrics , tvmax outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation , reported in table 2 .
can be seen in table 4 the results obtained when sparsemax and tvmax are combined in the self - attention layers of the decoder and in the output attention layers . moreover , when combining these features , the results are superior than those obtained using softmax . thus , the accuracy obtained by combining sparsemax with bounding box features is superior than that obtained by softmax ( i . e . , when combining only the features of contiguous regions of the image , i . e . the yes / no number and the attention layer ) . this corroborates our intuition that sparsemax is better than softmax in that the features obtained by selecting bounding boxes are easier for the model to obtain . this validates our hypothesis that selecting only contiguous regions leads to a better answering capability .
iv presents the system ' s performance on each error generation algorithm . we included only p @ 1 and p @ 10 to show trend on all languages . " random character " and " character bigrams " includes data for edit distance 1 and 2 whereas " characters swap " consists of data from edit distance 2 .
considered three approaches — trie data structure , burkhard - keller tree ( bk tree ) , directed acyclic word graphs ( dawgs ) and symmetric delete algorithm ( sda ) 6 . in table i , we represent the performance of algorithms for edit distance 2 without adding results for bk tree because its performance was in range of couple of seconds .
best performances for each language is reported in table iii . we present precision @ k9 for k ∈ 1 , 3 , 5 , 10 and mean reciprocal rank ( mrr ) . the system performs well on synthetic dataset with p @ 1 and p @ 10 indicating that the system is well - equipped to handle multiple languages .
system is able to do each sub - step in real - time . all the sentences used for this analysis had exactly one error according to our system . detection time is the average time weighted over number of tokens in query sentence , ranking time is weighted over misspelling character length . ranking time is calculated over iterations of suggestions generated by the system , and length weighted over ranking length is computed over iterations generated by query sentence length .
performance of these systems on the test set is shown in table vi . since these tools only work on word - error level , we used only unigram probabilities for ranking . our system outperforms both the systems .
shown in table vii , most of the words for each language were detected as known but still there was a minor percentage of words that were detected for error .
table 3 presents the results of models trained on tweets from one domain to the other . the results are presented in tables 1 and 2 . table 3 shows that predictive performance is relatively consistent across all domains with two exceptions ( ' food & beverage ' consistently shows lower performance , while ' other ' achieves higher performance ) with respect to predictive accuracy .
total , 1 , 232 tweets ( 62 . 4 % ) are complaints and 739 are not complaints ( 37 . 6 % ) .
unigrams and part - of - speech features specific of complaints and non - complaints are presented in table 4 . all correlations shown in these tables are statistically significant at p < . 01 , with simes correction for multiple comparisons . negations are uncovered through word clusters ( not , no , won ' t ) a significant number of words ( been , still , working , great , posemo , assent ) are missing from these tables , indicating that the complaint is not contained in a single sentence . complaints are not accompanied by exclamation marks . on the other hand , the presence of words that show positive sentiment ( thank , love , understanding ) are among the most distinctive features for a tweet not being labeled as a complaint . in addition , other words that express positive sentiment are also distinctive of complaints , such as gratitude ( good , great ) or laughter ( lol ) ( very bad , great ) . a large part of complaints are formulated as questions about the task being completed ( e . g . , why is this not working ? , when will get my response ? ) . question marks are uncovered around pronoun usage , as it can be seen in the unigrams section of this table . several words that describe positive sentiment or emotions are present in the complaint , as shown in the table 4 . these are used to describe actions completed by the complainer ( e
top features for the liwc categories and word2vec topics ( negate ) . the top liwc category ( issues ) contain words referring to issues or errors . complaints tend to not contain personal pronouns ( he , she , him , you , shehe , male , female ) , as the focus on expressing the complaint is on the self and the party the complaints are addressed to and not other third parties . general topics typical of complaint tweets include requiring assistance or customer support . several groups of words are frequently used to express complaints : about orders or deliveries ( in the retail domain ) , about access ( in complaints to service providers ) and about parts of tech products ( in tech ) . affect is the most distinctive group of words , the most frequently used in complaints ( in retail domain ) .
are shown in table 1 . most sentiment analysis models show accuracy above chance in predicting complaints . the best results are obtained by the volkova & bachrach model ( sentiment – v & b ) which achieves 60 f1 and 50 auc auc respectively . complaint specific features are predictive of complaints , but to a smaller extent than complaint specific features . syntactic features such as the liwc dictionaries ( which combine syntactic and semantic information ) and word2vec topics perform best , but predictive accuracy is low . sentiment – mpqa ) achieves a better performance than any sentiment or complaint feature group , showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task . we also see a slight improvement in f1 score over the baseline model , showing that syntactic features are more effective in boosting predictive accuracy . further improving predictive accuracy by training more sophisticated features , such as predictive accuracy and recall scores , to mimic complaints more effectively .
presented in table 7 show that the domain adaptation approach further boosts f1 by 1 point to 79 ( t - test , p < 0 . 5 ) and roc auc by 0 . 012 .
8 shows the performance of our model in macro - averaged f1 using the best performing feature set . overall , the performance is comparable across all domains , with the exception of transport . the apparel domain is qualitatively very different from the others as a large number of complaints are about returns or the company not stocking items , hence leading to different features being important for prediction . domain adaptation is beneficial , lowering performance on a single domain compared to predictive performance in the others . this highlights the differences in feature set between feature set and domain adaptation . online features such as domain adaptation ( e . g . , domain adaptation ) and multi - domain learning ( pooling ) are beneficial , improving prediction performance on multiple domains .
ert achieved a final accuracy of 91 . 20 % , now marginally comparable to ulmfit ' s full performance . gpt - 2 , on the other hand , finetuned to an accuracy of 96 . 28 % , a full 4 . 42 % improvement over the performance of ulm fit .
table 5 , it can be seen that generative pretraining via language modeling does account for a considerable amount of performance , constituting 44 . 32 % of the overall performance ( a boost of 42 . 67 % ) in the multitasking setup , and constituting 43 . 93 % of that overall performance , a boost of 39 . 97 % in the standard finetuning setup .
shown in table 6 , reducing the number of attention heads severely decreases multitasking performance . using only one attention head , thereby attending to only one context position at once , degrades the performance to less than the performance of 10 heads using the standard finetuning scheme . this shows that more attention heads , while attending to multiple different context contexts , is important to boosting performance to state - of - the - art performance .
6 shows the ablation study results on paragraph selection loss lpara and entity prediction loss lentity . as shown in the table , using paragraph selection can further improve the joint f1 by 0 . 31 points ,
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 86 / 59 . 86 respectively on the distractor and fullwiki set , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
1 and table 2 summarize our results on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves a joint em / f1 score of 43 . 57 / 71 . 03 and 35 . 86 / 59 . 86 on the distractractor and fullwiki setting respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous state of the art .
3 shows the performance of paragraph selection on the dev set of hotpotqa . in dfgn , paragraphs are selected based on a threshold to maintain high recall ( 98 . 28 % ) , leading to a low precision ( 60 . 53 % ) . compared to both threshold - based paragraph selection and paragraph selection , our joint f1 score is 69 . 53 % better than that of ps graph ( 71 . 53 % ) and 73 . 45 % more accurate .
shown in table 5 , the pre - trained hgn models outperform the best state - of - the - art models in the distractor setting . in fact , the hgn model achieves a joint f1 score of 69 . 86 / 71 . 03 on the test set , which is slightly better than eps ( 71 . 53 / 87 . 86 ) .
results of hgn for different reasoning types are shown in table 7 . for comp - yn , the performance drop is significantly worse than that of bridge , indicating that the performance gain comes from a better model design .
4 shows the bleu scores of dual2seq when gold or automatic amrs are available . the improvement from automatic amr to gold amr is significant , which shows that the translation quality of our model can be improved with an increase of amr parsing accuracy .
3 shows the test bleu , ter and meteor scores of all systems trained on the smallscale news commentary v11 subset or the largescale full set . dual2seq is consistently better than the other systems under all three metrics , dual 2seq is better than both opennmt - tf and transformer - tf in both settings . the gap between seq2seq - v11 ( which shows the advantage of pre - training the model vs . the current state - of - the - art ) is larger than that under the large - scale setting ( e . g . , the ncv11 subset ) or the fullset subset ( i . e . the negotiation / henderson subset ) by 2 points , which shows the diminishing returns from training the model when trained on small - scale datasets . finally , the gap is narrower under the big - scale set ( e : g . , when trained with only one false positive relation , and when trained using only fragments of data from the original dataset ) , indicating that the reliance on syntactic patterns leads to a more accurate model . duel2seq also outperforms the seq - tf baseline by a noticeable margin ( 17 . 8 vs 19 . 2 ) in the nc - v11 subset and in the full set ( 25 . 0 vs 25 . 0 ) .
most representative models are elmo , gpt , bert , mt - dnn and xlnet . next , we give a brief overview of these models and summarize their performance on the selected benchmark tasks . table 2 quantitatively compares these models against various benchmarks : [ bert ] gpt is the most representative of its kind , followed by xlnet and bert - keller .
2 shows that coreference propagation ( corefprop ) improves named entity recognition performance across all three domains . the largest gains are on the computer science research abstracts of scierc ,
1 shows the performance of all state - of - the - art models when trained on the entity and relation extraction tasks . relative error reductions range from 0 . 2 - 27 . 9 % over previous state of the art models .
relation propagation ( relprop ) improves relation extraction performance over pretrained bert , but does not improve fine - tuned bert .
7 compares the results of bert and scibert with the best - performing model configurations : scierc relation and genia . the results show that bert significantly boosts performance for scientific datasets including scierc , and the scierc dataset .
6 shows that both variations of our bert model benefit from wider context windows . our model achieves the best performance with a 3sentence window across all relation and event extraction tasks .
table 1 we report the best and average precision @ 1 scores and the average number of iterations among 10 experiments , for different language translations . our model improves the precision and accuracy in the translation tasks . the accuracy improvements are small but significant , and we note that the noise - aware model is more stable and therefore requires fewer iterations to converge . we also note that we consider the accuracy improvements as a lower - bound on the actual improvements as the current test set comes from the same distribution of the training set , and also contains similarly noisy pairs .
results are shown in table 1 . the first set of results show that the hierarchical right / left branching baselines dominate the completely right branching ones . however , their performance is still significantly worse than any discourse parser ( intra - and interdomain ) . the second set shows the performance of existing discourse parsers when trained on the rst - dt dataset . the sentiment parser by joty et al . ( 2015 ) achieves the highest score with 86 . 88 % on the sentiment parser .
4 shows the results of the second stage of the evaluation . our dkrn agent ( ours ) obtains the highest overall performance on cwc dataset . the average number of turns taken by the agent is 69 . 5 , which indicates that the agent has achieved a high performance on the task .
3 shows the turn - level evaluation results . our approach dkrn outperforms all state - of - the - art methods in terms of all metrics with two tasks : the keyword prediction task ( r20 @ 1 ) and the response retrieval task ( mrr ) . the keyword prediction task is described in table 4 .
5 shows the results of the second study . our dkrn agent outperforms all the other agents with a large margin .
6 shows the results of the second study . our agent outperforms all the comparison agents with a large margin .
istic input a cnn augmented with self - attentionwe show encouraging relative improvements for future research in this direction . we empirically found that self attention was the most efficient in the 3rd stage
experimental results on a resnet - 152 are shown in table 1 . we show that self - attention is crucial to improving the visual performance of the resnet . it is clear from the small improvements in the vqa that the presence of self - aware modules is important to improving visual performance .
experimental results of all models are shown in table 1 . first , han models appear to be more appealing than svm and logreg because there is less variation in program implementation , hence less effort is required to reproduce rouge scores . they yield higher precision scores than traditional models . they have higher recall scores , indicating that there is a need to learn the task at hand . the redundancy removal step is crucial for rouges scores to improve predictive performance . it is crucial to design effective redundancy removal strategies that are able to handle multiple tasks at once . we showed here that redundancy removal was the most efficient way to reduce redundancy removal . it reduces recall scores and precision scores . it improves the recall scores by 2 . 8 % compared to logreg ,
3 : joint goal accuracy on dstc2 and woz 2 . 0 test set vs . various approaches proposed by mrkˇsi ´ c and ruder ( 2017 ) . statenet ps outperforms statenet and statenet on all metrics , with the exception of belief tracking , which performs best in the multi - domain setting ( + 3 . 5 % joint goal accuracy vs . 3 . 0 % on the dstc2 test set ) , reported in table 3 .
2 shows the joint goal accuracy on dstc2 and woz 2 . 0 of statenet psi using different pre - trained models based on different single slot . the fact that the food initialization has the best performance verifies our selection of the slot with the worst performance for pre - training .
5 shows the performance of asymmetric word embeddings in multi30k dataset . ame outperforms fme and fme in both languages ,
achieve 21 . 42 % improvement on aver age compared to kiros et al . ( 2014 ) in the symmetric manner . there is a 1 . 8 % boost on average compared to mono due to more training data and multilingual text encoder . ame performs better than fme model on both symmetric and asymmetric modes .
german descriptions are 11 . 03 % better on average compared to ( gella et al . , 2017 ) in symmetric mode . ame also achieves better results than fme model in german descriptions too , which shows the advantage of finetuning word embeddings .
achieve 21 . 28 % improvement on aver age compared to kiros et al . ( 2014 ) in the symmetric manner . we show that adapting the word embedding for the task at hand , boosts the general performance , since ame model significantly outperforms fme model in both languages .
show the results for english and german captions . ame achieves 3rd and 4th best results on average compared to monolingual model in symmetric and asymmetric modes , respectively .
4 shows the results of the experiments for italian and german , compared to english , both for the original and the debiased embeddings ( for each language there is a corresponding reduction in the average of the two sets ) . in italian , we get a reduction of 91 . 67 % of the gap with respect to english . in german , the difference between the averages is 100 % .
2 shows the results for italian and german , compared to english , both for the original and the debiased embeddings . as expected , the average ranking of samegender pairs is significantly lower than that of different - gender pairs , both in german and italian , while the difference between the sets in english is much smaller .
6 shows the results for italian and german for both datasets , compared to the original embeddings . in both cases , the performance improves significantly over the original ones .
results in table 7 show that precision on the embeddings for german and italian is relatively high , compared to that in german , both for debiased and unigrambled documents ( cf . table 7 ) .
3 shows the ari and silhouette coefficients of od - d2v and od - w2v ( paired t - test ) for baselines conditioned on the rand index ( ari ) and tf - idf index ( rai ) . opinion distance od significantly outperforms the competition in both categories , with the exception of ari being slightly better for od - crowded and doc2vec - based datasets . the opinion distance od is particularly sensitive to the sentiment of the opposing party , with an ari of 0 . 15 and 0 . 07 compared to the 0 . 01 baseline . in the exceptional case of " hydroelectric dams " dataset , we see that the opinion distance between od and od is more important than the sentiment distance itself . this is evident from the significant drop in ari from od to od since the clustering distance is narrower .
3 shows the ari and sil scores of od methods compared to od - parse on the " video games " and " pornography " datasets . od significantly outperforms the baseline methods in terms of ari , silhouette coefficient , wmd coefficient and sent2vec coefficient . the classification system od achieves the best performance on both datasets : the " seanad abolition " dataset ( barely providing a performance improvement over random clustering , i . e . , a zero ari score ) and the " seanadr abolition " . the od variant performs slightly better than wmd and doc2vec , indicating that the clustering technique is more effective in reducing repetition . we observe that the od variant is more accurate in predicting instances of harassment , although the disparity is narrower .
performance of the classifiers on the " video games " and " pornography " datasets is reported in table 4 . svm with only od features outperforms all the other baselines except for the one that includes svm . on the " seanad abolition " dataset , the classification performance based on od is significantly better than those using svm , lsa and wmd . the classification performance of od is markedly better than od , although od is slightly worse than od due to the larger number of trained classifiers . the classification performances of od are summarized in table 6 .
ributhe results of different variants are shown in table 5 . we observe that compared to od - parse , od is much more accurate . on the three datasets , od achieves an average weighted f1 score of 0 . 54 , 0 . 56 and 0 . 41 respectively compared to the scores of - 0 . 01 and zero . 01 by od . table 5 shows that od is significantly better than jensen - shannon divergence on all three datasets . sentiment polarity shifters have a generally positive effect on clustering performance , however it is less beneficial for od to converge to the opinion representation representation score .
performance on the testing and development sets is shown in table 3 . the branch - lstm model predicts commenting , the majority class well , however it is unable to pick out any denying , the mostchallenging underrepresented class .
denying instances get misclassified as commenting ( see table 5 ) ,
1 shows the absa datasets for each language . from left to right each row displays the number of tokens , number of targets and the type of multiword targets for each training and test set . for brevity we only include tokens for training and the test set , as shown in table 2 . for the english language , there are no tokens for train and test sets .
3 provides detailed results on the opinion target extraction ( ote ) task for english . we show in bold our best model ( all ) chosen via 3 - fold cv on the training data . moreover , we also include the best models using only one type of clustering feature , namely , the best brown , clark and word2vec models .
6 shows that our system outperforms the best previous approaches in terms of f1 score .
errors in our system are caused by false negatives [ fn ] , as it can be seen in table 7 .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
greek analogy test set contains 39 , 174 questions divided into semantic and syntactic analogy questions . semantic questions are divided into 9 categories and include 13 , 650 questions in total . they are mostly language specific , syntactic questions include 25 , 524 questions in the semantic category and 15 , 524 in the syntactic category . they include 39 , 096 questions for semantic analogy test .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
noticed that the sub - category in which most models had the worst performance was currency country category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
shown in table 4 , gr def had the highest correlation with human ratings of similarity , at 2 . 3 % compared to that of gr def .
epm " generalizes best , and in out - ofdomain evaluations , it considerably outperforms ensemble e2e - coref ,
pos and named entity tags have the least and pairwise features have the most significant effect . the results of " - pairwise " compared to " + pairwise " .
orporating epm feature - values improves three points over the strong lemma baseline on conll test set .
observe that incorporating all the linguistic features bridges the gap between the performance of " top - pairs " and " ranking " .
observe that the impact on generalization is also not notable , i . e . the conll score improves only by 0 . 5pp over " ranking " .
orporating epm feature - values improves three points over the strong lemma baseline on conll test set .
first analyze the coverage of the vsms in question with respect to the lexica at hand , see table 4 . for brevity we only report coverage on w2 contexts lemmatization allows more targets to exceed the sgns frequency threshold , which results in consistently better coverage . pos - disambiguation , in turn , fragments the vocabulary and fragments the target ' s vocabulary , leading to a reduction in the coverage .
1 shows the performance of the vsms in question on similarity benchmarks . lemmatized targets generally perform better , with the boost being more pronounced on simverb . morph - fitting consists of two types : type - based vsms that are more similar in vocabulary and type - aware . syntactic targets show a significant drop in performance when using pos pos on simlex , simlex and simlex verbs , however , the performance increase when using dep - w contexts is greater than that when using lemmatized targets . this is evident from the large difference in performance between the types of vsms under the vsm ( " mfit - a " and " - ar " ) . lemma - based targets are comparable in performance , but are slightly less appealing : in simlex - verb , there is a drop of performance from the type to morph - ar ,
5 provides exact scores for reference . lemma vsms generally perform better than type - based vsms , however it is slightly worse . for example , lemma vsms perform better on vn when dependency - based pos is used , but on lemmatized vsms the performance is significantly worse on wn - v ( p ≤ . 005 ) with the exception of those using pos - based w2 contexts ( p ≈ . 005 ) .
embeddings derived from wiki - pubmed - pmc outperform glove - based models in the extraction of most relation types . for example , the model using boc outperforms the models using bow as well as asm features . the feed - forward ann displays significant over - fitting across all relation types , with the exception of the relation type of boc feature , which is more related to sentence prediction . note that the feedforward ann performs best when trained with boc features instead of sentence prediction features .
3 shows the results of applying the cooccurrence baseline on the relation extraction tasks . the machine learning approaches based on boc lexical features outperform the traditional approaches such as recurlink ( tp , td / tr ) and nextreview ( tn , tr ) in terms of lexical co - occurrence . the boc model outperforms all the other methods that do not use lexical cues .
3 compares multi - news to other news datasets used in experiments below . the total number of words in the concatenated inputs is two orders of magnitude larger than other mds datasets , which are typically used in multi - document settings . additionally , the number of examples in the summaries is shorter than in other works , indicating that there is a need to design more interpretable summaries . the number of example sentences in the multi - decatenation inputs is one of the largest in the mds dataset ( with a total of 260 examples ) and is slightly larger than in the sds dataset , 260 examples in total . the concatenation size of the input documents is also smaller than in mds ( 260 examples ) but still comparable to sds ( 300 examples ) .
table 4 shows the percentage of n - grams in the gold summaries which do not appear in the input documents as a measure of how abstractive our summaries are in table 4 . multi - news is comparable to the abstractiveness of sds datasets , but smaller than sds . it is comparable with the sds dataset , and similar to the duc dataset .
model outperforms pg - original and pg - mmr when trained and tested on the multi - news dataset . the transformer performs best in terms of r - 1 while hi - map outperforms it on r - 2 and r - su .
shown in table 5 , as the required derivation step increases , the prkgc + ns model suffers from predicting answer entities and generating correct nlds . this indicates that the challenge of rc - qede is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer .
evaluation results shown in table 2 show that the annotated nlds are of high quality ( reachability ) , and each nld is properly derived from supporting documents ( derivability ) . on the other hand , the evaluation results show that only 3 . 8 % of the 294 ( out of 900 ) 3 - step nlds are actually of quality , which indicates that there is a need to improve interpretability .
shown in table 4 , the prkgc models learned to reason over more than simple shortest paths . as shown in the second table , the shortest path is the mostchallenging one , and the longer ones are the most difficult to solve . yet , for those that do not have access to annotated nlds , paths with the maximum score get the best score . supervising path attentions are beneficial for improving the human interpretability of question answering .
shown in table 7 , the prkgc models achieve a comparable performance to other sophisticated neural models .
results are presented in table 4 . the ablation results show that the ' traditional ' lstm layout performs better than the ' alternating ' one we chose for our submission . apart of the flipped results of the lstm - 800 and the lsm - 400 , smaller discrepancies in cv score are sometimes associated with larger discrepancies in test set performance . these are mostly due to small size of the data set ( low some ablated models that perform poorly in the singlemodel scenario ( e . g . no - overlaps , l - translations ) are able to regain a lot of accuracy when ensembled .
system ' s official score was 60 . 9 % ( micro - f1 ) af therefore , we report both the official score ( from our second submission ) and the result of re - scoring each submission after replacing these 10 files with the ones from our first submission .
istent with our hypothesis , we have decided to focus our attention on the five scenarios that we consider in table vii . the results are summarized in tables vii and viii . they show that , for each of these scenarios , the loss of one life is statistically significant , with an absolute improvement of 2 . 36 % in f1 score .
table 2 , we compare the performance of icsi and relis with previous state - of - the - art systems . for non - rl - based systems , we see that relis performs better than priorsum , tcsum and srsum , and deeptd ,
2 compares the quality of our ^ ( cid : 27 ) u x with other widely used rewards for input - specific rl ( see reaper ) , confirming that our proposed l2r method yields a superior reward oracle .
task b , all models trained on polarity features outperform the baseline except for those trained on sif embeddings ( table6 ) .
task a , all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings .
task b , all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings .
performed an ablation study on a single model having obtained 69 . 23 % accuracy on the validation set . results are summarized in table 1 . we can observe that emoji also contributed significantly to the model ' s performance , with an absolute improvement of 3 . 71 % when using the multi - factor model .
the corresponding classification report . in general , we confirm what klinger et al . ( 2018 ) report : anger was the most difficult class to predict , followed by surprise , whereas joy , fear , and disgust are the better performing ones .
4 shows the overall effect of hashtags and emoji on classification performance . tweets containing emoji seem to be easier for the model to classify than those without . hashtags also have a positive effect , however it is less significant .
5 shows the effect specific emoji have on classification performance . it is clear some emoji strongly contribute to improving prediction quality . the most interesting ones are mask , rage , and cry , which significantly increase accuracy . further , contrary to intuition , the sob emoji contributes less than cry , despite representing a stronger emotion .
performance on winograd and winocoref datasets is shown in table 7 . the best performing system is knowcomb . it improves by over 20 % over a state - of - the - art general coreference system . it also outperforms rahman and ng ( 2012 ) by a margin of 2 % .
results on standard ace and ontonotes datasets are shown in table 8 . our knowcomb system achieves the same level of performance as does the state - of - art general coreference system we base it on . as hard coreference problems are rare in standard coreference datasets , we do not have significant performance improvement .
ailed analysis to study the coverage of predicate schemas knowledge , we label the instances in winograd ( which can be seen in table 9 ) . the distribution of the instances is shown in the table 9 . the percentage of instances in the category that require predicate schema knowledge is significantly higher than the size of the corresponding category .
also provide an ablation study on the winocoref dataset in table 10 . these results use the best performing knowcomb system . they show that both type 1 and type 2 schema knowledge havehigher precision on category 1 and category 2 datainstances , respectively , compared to that on full data . also , the performance drop between cat1 / cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement . this is evident from table 10 , which also shows that the use of predicate schemas leads to a better knowledge acquisition .
1 shows the numerical results obtained during the experiments for the four combinations tested . in general terms , the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre - trained black - box classification system to a new domain . table 1 : accuracy obtained by training an standalone classifier , applying the api and the wrapper for each domain
